<?xml version="1.0" encoding="iso-8859-1"?>
<!--
     The FreeBSD Documentation Project
     $FreeBSD$
-->

<chapter id="zfs">
  <chapterinfo>
    <authorgroup>
      <author>
	<firstname>Tom</firstname>
	<surname>Rhodes</surname>
	<contrib>Written by </contrib>
      </author>
      <author>
	<firstname>Allan</firstname>
	<surname>Jude</surname>
	<contrib>Written by </contrib>
      </author>
      <author>
	<firstname>Benedict</firstname>
	<surname>Reuschling</surname>
	<contrib>Written by </contrib>
      </author>
      <author>
	<firstname>Warren</firstname>
	<surname>Block</surname>
	<contrib>Written by </contrib>
      </author>
    </authorgroup>
  </chapterinfo>

  <title>The Z File System (<acronym>ZFS</acronym>)</title>

  <para>The <emphasis>Z File System</emphasis>, or
    <acronym>ZFS</acronym>, is an advanced file system designed to
    overcome many of the major problems found in previous
    designs.</para>

  <para>Originally developed at &sun;, ongoing <acronym>ZFS</acronym>
    development has moved to the
    <ulink url="http://open-zfs.org">OpenZFS Project</ulink>.
    <xref linkend="zfs-history"/> describes the development history in
    more detail.</para>

  <para><acronym>ZFS</acronym> has three major design goals:</para>

  <itemizedlist>
    <listitem>
      <para>Data integrity: All data
	includes a <link
	linkend="zfs-term-checksum">checksum</link> of the data.  When
	data is written, the checksum is calculated and written along
	with it.  When that data is later read back, the
	checksum is calculated again.  If the checksums do not match, a
	data error has been detected.  <acronym>ZFS</acronym> will attempt to
	automatically correct errors when data
	redundancy is available.</para>
    </listitem>

    <listitem>
      <para>Pooled storage: physical storage devices are added to a
	pool, and storage space is allocated from that shared pool.
	Space is available to all file systems, and can be increased
	by adding new storage devices to the pool.</para>
    </listitem>

    <listitem>
      <para>Performance: multiple
	caching mechanisms provide increased performance.
	<link linkend="zfs-term-arc">ARC</link> is an advanced
	memory-based read cache.  A second level of
	disk-based read cache can be added with
	<link linkend="zfs-term-l2arc">L2ARC</link>, and disk-based synchronous
	write cache is available with
	<link linkend="zfs-term-zil">ZIL</link>.</para>
    </listitem>
  </itemizedlist>

  <para>A complete list of <acronym>ZFS</acronym> features and
    terminology is shown in <xref linkend="zfs-term"/>.</para>

  <sect1 id="zfs-differences">
    <title>What Makes <acronym>ZFS</acronym> Different</title>

    <para><acronym>ZFS</acronym> is significantly different from any
      previous file system because it is more than just
      a file system.  Combining the
      traditionally separate roles of volume manager and file system
      provides <acronym>ZFS</acronym> with unique advantages.  The file system is now
      aware of the underlying structure of the disks.  Traditional
      file systems could only be created on a single disk at a time.
      If there were two disks then two separate file systems would
      have to be created.  In a traditional hardware
      <acronym>RAID</acronym> configuration, this problem was worked
      around by presenting the operating system with a single logical
      disk made up of the space provided by a number of disks, on top
      of which the operating system placed its file system.  Even in
      the case of software <acronym>RAID</acronym> solutions like
      <acronym>GEOM</acronym>, the <acronym>UFS</acronym> file system
      living on top of the <acronym>RAID</acronym> transform believed
      that it was dealing with a single device.
      <acronym>ZFS</acronym>'s combination of the volume manager and
      the file system solves this and allows the creation of many file
      systems all sharing a pool of available storage.  One of the
      biggest advantages to <acronym>ZFS</acronym>'s awareness of the
      physical layout of the disks is that <acronym>ZFS</acronym> can
      grow the existing file systems automatically when additional
      disks are added to the pool.  This new space is then made
      available to all of the file systems.  <acronym>ZFS</acronym>
      also has a number of different properties that can be applied to
      each file system, creating many advantages to creating a number
      of different filesystems and datasets rather than a single
      monolithic filesystem.</para>
  </sect1>

  <sect1 id="zfs-quickstart">
    <title><acronym>ZFS</acronym> Quick Start Guide</title>

    <para>There is a start up mechanism that allows &os; to mount
      <acronym>ZFS</acronym> pools during system initialization.  To
      enable it, add this line to
      <filename>/etc/rc.conf</filename>:</para>

    <programlisting>zfs_enable="YES"</programlisting>

    <para>Then start the service:</para>

    <screen>&prompt.root; <userinput>service zfs start</userinput></screen>

    <para>The examples in this section assume three
      <acronym>SCSI</acronym> disks with the device names
      <devicename><replaceable>da0</replaceable></devicename>,
      <devicename><replaceable>da1</replaceable></devicename>, and
      <devicename><replaceable>da2</replaceable></devicename>.  Users
      of <acronym>SATA</acronym> hardware should instead use
      <devicename><replaceable>ada</replaceable></devicename> device
      names.</para>

    <sect2>
      <title>Single Disk Pool</title>

      <para>To create a simple, non-redundant <acronym>ZFS</acronym>
	pool using a single disk device, use
	<command>zpool</command>:</para>

      <screen>&prompt.root; <userinput>zpool create <replaceable>example</replaceable> <replaceable>/dev/da0</replaceable></userinput></screen>

      <para>To view the new pool, review the output of
	<command>df</command>:</para>

      <screen>&prompt.root; <userinput>df</userinput>
Filesystem  1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a   2026030  235230  1628718    13%    /
devfs               1       1        0   100%    /dev
/dev/ad0s1d  54098308 1032846 48737598     2%    /usr
example      17547136       0 17547136     0%    /example</screen>

      <para>This output shows that the <literal>example</literal> pool
	has been created and <emphasis>mounted</emphasis>.  It is now
	accessible as a file system.  Files may be created on it and
	users can browse it, as seen in the following example:</para>

      <screen>&prompt.root; <userinput>cd /example</userinput>
&prompt.root; <userinput>ls</userinput>
&prompt.root; <userinput>touch testfile</userinput>
&prompt.root; <userinput>ls -al</userinput>
total 4
drwxr-xr-x   2 root  wheel    3 Aug 29 23:15 .
drwxr-xr-x  21 root  wheel  512 Aug 29 23:12 ..
-rw-r--r--   1 root  wheel    0 Aug 29 23:15 testfile</screen>

      <para>However, this pool is not taking advantage of any
	<acronym>ZFS</acronym> features.  To create a dataset on this
	pool with compression enabled:</para>

      <screen>&prompt.root; <userinput>zfs create example/compressed</userinput>
&prompt.root; <userinput>zfs set compression=gzip example/compressed</userinput></screen>

      <para>The <literal>example/compressed</literal> dataset is now a
	<acronym>ZFS</acronym> compressed file system.  Try copying
	some large files to <filename
	  class="directory">/example/compressed</filename>.</para>

      <para>Compression can be disabled with:</para>

      <screen>&prompt.root; <userinput>zfs set compression=off example/compressed</userinput></screen>

      <para>To unmount a file system, use
	<command>zfs umount</command> and then verify by using
	<command>df</command>:</para>

      <screen>&prompt.root; <userinput>zfs umount example/compressed</userinput>
&prompt.root; <userinput>df</userinput>
Filesystem  1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a   2026030  235232  1628716    13%    /
devfs               1       1        0   100%    /dev
/dev/ad0s1d  54098308 1032864 48737580     2%    /usr
example      17547008       0 17547008     0%    /example</screen>

      <para>To re-mount the file system to make it accessible again,
	use <command>zfs mount</command> and verify with
	<command>df</command>:</para>

      <screen>&prompt.root; <userinput>zfs mount example/compressed</userinput>
&prompt.root; <userinput>df</userinput>
Filesystem         1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a          2026030  235234  1628714    13%    /
devfs                      1       1        0   100%    /dev
/dev/ad0s1d         54098308 1032864 48737580     2%    /usr
example             17547008       0 17547008     0%    /example
example/compressed  17547008       0 17547008     0%    /example/compressed</screen>

      <para>The pool and file system may also be observed by viewing
	the output from <command>mount</command>:</para>

      <screen>&prompt.root; <userinput>mount</userinput>
/dev/ad0s1a on / (ufs, local)
devfs on /dev (devfs, local)
/dev/ad0s1d on /usr (ufs, local, soft-updates)
example on /example (zfs, local)
example/data on /example/data (zfs, local)
example/compressed on /example/compressed (zfs, local)</screen>

      <para><acronym>ZFS</acronym> datasets, after creation, may be
	used like any file systems.  However, many other features are
	available which can be set on a per-dataset basis.  In the
	following example, a new file system, <literal>data</literal>
	is created.  Important files will be stored here, the file
	system is set to keep two copies of each data block:</para>

      <screen>&prompt.root; <userinput>zfs create example/data</userinput>
&prompt.root; <userinput>zfs set copies=2 example/data</userinput></screen>

      <para>It is now possible to see the data and space utilization
	by issuing <command>df</command>:</para>

      <screen>&prompt.root; <userinput>df</userinput>
Filesystem         1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a          2026030  235234  1628714    13%    /
devfs                      1       1        0   100%    /dev
/dev/ad0s1d         54098308 1032864 48737580     2%    /usr
example             17547008       0 17547008     0%    /example
example/compressed  17547008       0 17547008     0%    /example/compressed
example/data        17547008       0 17547008     0%    /example/data</screen>

      <para>Notice that each file system on the pool has the same
	amount of available space.  This is the reason for using
	<command>df</command> in these examples, to show that the file
	systems use only the amount of space they need and all draw
	from the same pool.  The <acronym>ZFS</acronym> file system
	does away with concepts such as volumes and partitions, and
	allows for several file systems to occupy the same
	pool.</para>

      <para>To destroy the file systems and then destroy the pool as
	they are no longer needed:</para>

      <screen>&prompt.root; <userinput>zfs destroy example/compressed</userinput>
&prompt.root; <userinput>zfs destroy example/data</userinput>
&prompt.root; <userinput>zpool destroy example</userinput></screen>
    </sect2>

    <sect2>
      <title><acronym>ZFS</acronym> RAID-Z</title>

      <para>Disks fail.  One
	method of avoiding data loss from disk failure is to
	implement <acronym>RAID</acronym>.  <acronym>ZFS</acronym>
	supports this feature in its pool design.
	<acronym>RAID-Z</acronym> pools require three or more disks
	but yield more usable space than mirrored pools.</para>

      <para>To create a <acronym>RAID-Z</acronym> pool, use this
	command, specifying the disks to add to the
	pool:</para>

      <screen>&prompt.root; <userinput>zpool create storage raidz da0 da1 da2</userinput></screen>

      <note>
	<para>&sun; recommends that the number of devices used in a
	  <acronym>RAID</acronym>-Z configuration is between three and
	  nine.  For environments requiring a single pool consisting
	  of 10 disks or more, consider breaking it up into smaller
	  <acronym>RAID-Z</acronym> groups.  If only two disks are
	  available and redundancy is a requirement, consider using a
	  <acronym>ZFS</acronym> mirror.  Refer to &man.zpool.8; for
	  more details.</para>
      </note>

      <para>This command creates the <literal>storage</literal> zpool.
	This may be verified using &man.mount.8; and &man.df.1;.  This
	command makes a new file system in the pool called
	<literal>home</literal>:</para>

      <screen>&prompt.root; <userinput>zfs create storage/home</userinput></screen>

      <para>Now compression and keeping extra
	copies of directories and files can be enabled with these
	commands:</para>

      <screen>&prompt.root; <userinput>zfs set copies=2 storage/home</userinput>
&prompt.root; <userinput>zfs set compression=gzip storage/home</userinput></screen>

      <para>To make this the new home directory for users, copy the
	user data to this directory, and create the appropriate
	symbolic links:</para>

      <screen>&prompt.root; <userinput>cp -rp /home/* /storage/home</userinput>
&prompt.root; <userinput>rm -rf /home /usr/home</userinput>
&prompt.root; <userinput>ln -s /storage/home /home</userinput>
&prompt.root; <userinput>ln -s /storage/home /usr/home</userinput></screen>

      <para>Users now have their data stored on the freshly
	created <filename class="directory">/storage/home</filename>.
	Test by adding a new user and logging in as that user.</para>

      <para>Try creating a snapshot which can be rolled back
	later:</para>

      <screen>&prompt.root; <userinput>zfs snapshot storage/home@08-30-08</userinput></screen>

      <para>Note that the snapshot option will only capture a real
	file system, not a home directory or a file.  The
	<literal>@</literal> character is a delimiter used between the
	file system name or the volume name.  When a user's home
	directory is accidentally deleted, restore it with:</para>

      <screen>&prompt.root; <userinput>zfs rollback storage/home@08-30-08</userinput></screen>

      <para>To list all available snapshots, run
	<command>ls</command> in the file system's
	<filename class="directory">.zfs/snapshot</filename>
	directory.  For example, to see the previously taken
	snapshot:</para>

      <screen>&prompt.root; <userinput>ls /storage/home/.zfs/snapshot</userinput></screen>

      <para>It is possible to write a script to perform regular
	snapshots on user data.  However, over time, snapshots can
	consume a great deal of disk space.  The previous snapshot can
	be removed using the following command:</para>

      <screen>&prompt.root; <userinput>zfs destroy storage/home@08-30-08</userinput></screen>

      <para>After testing,
	<filename class="directory">/storage/home</filename> can be
	made the real <filename class="directory">/home</filename>
	using this command:</para>

      <screen>&prompt.root; <userinput>zfs set mountpoint=/home storage/home</userinput></screen>

      <para>Run <command>df</command> and <command>mount</command> to
	confirm that the system now treats the file system as the real
	<filename class="directory">/home</filename>:</para>

      <screen>&prompt.root; <userinput>mount</userinput>
/dev/ad0s1a on / (ufs, local)
devfs on /dev (devfs, local)
/dev/ad0s1d on /usr (ufs, local, soft-updates)
storage on /storage (zfs, local)
storage/home on /home (zfs, local)
&prompt.root; <userinput>df</userinput>
Filesystem   1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a    2026030  235240  1628708    13%    /
devfs                1       1        0   100%    /dev
/dev/ad0s1d   54098308 1032826 48737618     2%    /usr
storage       26320512       0 26320512     0%    /storage
storage/home  26320512       0 26320512     0%    /home</screen>

      <para>This completes the <acronym>RAID-Z</acronym>
	configuration.  Daily status updates about the file systems
	created can be generated as part of the nightly
	&man.periodic.8; runs:</para>

      <screen>&prompt.root; <userinput>echo 'daily_status_zfs_enable="YES"' &gt;&gt; /etc/periodic.conf</userinput></screen>
    </sect2>

    <sect2>
      <title>Recovering <acronym>RAID</acronym>-Z</title>

      <para>Every software <acronym>RAID</acronym> has a method of
	monitoring its <literal>state</literal>.  The status of
	<acronym>RAID-Z</acronym> devices may be viewed with this
	command:</para>

      <screen>&prompt.root; <userinput>zpool status -x</userinput></screen>

      <para>If all pools are <link
	  linkend="zfs-term-online">Online</link> and everything is
	normal, the message indicates that:</para>

      <screen>all pools are healthy</screen>

      <para>If there is an issue, perhaps a disk is in the <link
	  linkend="zfs-term-offline">Offline</link> state, the pool
	state will look similar to:</para>

      <screen>  pool: storage
 state: DEGRADED
status: One or more devices has been taken offline by the administrator.
	Sufficient replicas exist for the pool to continue functioning in a
	degraded state.
action: Online the device using 'zpool online' or replace the device with
	'zpool replace'.
 scrub: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	storage     DEGRADED     0     0     0
	  raidz1    DEGRADED     0     0     0
	    da0     ONLINE       0     0     0
	    da1     OFFLINE      0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</screen>

      <para>This indicates that the device was previously taken
	offline by the administrator with this
	command:</para>

      <screen>&prompt.root; <userinput>zpool offline storage da1</userinput></screen>

      <para>Now the system can be powered down to replace
	<devicename>da1</devicename>.  When the system is back online,
	the failed disk can replaced in the pool:</para>

      <screen>&prompt.root; <userinput>zpool replace storage da1</userinput></screen>

      <para>From here, the status may be checked again, this time
	without <option>-x</option> so that all pools
	are shown:</para>

      <screen>&prompt.root; <userinput>zpool status storage</userinput>
 pool: storage
 state: ONLINE
 scrub: resilver completed with 0 errors on Sat Aug 30 19:44:11 2008
config:

	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    da0     ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</screen>

      <para>In this example, everything is normal.</para>
    </sect2>

    <sect2>
      <title>Data Verification</title>

      <para><acronym>ZFS</acronym> uses checksums to verify the
	integrity of stored data.  These are enabled automatically
	upon creation of file systems and may be disabled using the
	following command:</para>

      <screen>&prompt.root; <userinput>zfs set checksum=off storage/home</userinput></screen>

      <warning>
	<para>Doing so is <emphasis>not</emphasis> recommended!
	  Checksums take very little storage space and provide data
	  integrity.  Many ZFS features will not work properly with
	  checksums disabled.  There is also no noticeable performance
	  gain from disabling these checksums.</para>
      </warning>
	
      <para>Checksum verification is known as <quote>scrubbing</quote>.
	Verify the data integrity of the <literal>storage</literal>
	pool, with this command:</para>

      <screen>&prompt.root; <userinput>zpool scrub storage</userinput></screen>

      <para>The duration of a scrub depends on the amount of data
	stored.  Large amounts of data can take a considerable amount
	of time to verify.  It is also very <acronym>I/O</acronym>
	intensive, so much so that only one scrub> may be run at any
	given time.  After the scrub has completed, the status is
	updated and may be viewed with a status request:</para>

      <screen>&prompt.root; <userinput>zpool status storage</userinput>
 pool: storage
 state: ONLINE
 scrub: scrub completed with 0 errors on Sat Jan 26 19:57:37 2013
config:

	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    da0     ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</screen>

      <para>The completion time is displayed and helps to ensure data
	integrity over a long period of time.</para>
	<!-- WB: what does that mean? -->

      <para>Refer to &man.zfs.8; and &man.zpool.8; for other
	<acronym>ZFS</acronym> options.</para>
    </sect2>
  </sect1>

  <sect1 id="zfs-zpool">
    <title><command>zpool</command> Administration</title>

    <para>The administration of ZFS is divided between two main
      utilities.  The <command>zpool</command> utility which controls
      the operation of the pool and deals with adding, removing,
      replacing and managing disks, and the <link
      linkend="zfs-zfs"><command>zfs</command></link> utility, which
      deals with creating, destroying and managing datasets (both
      <link linkend="zfs-term-filesystem">filesystems</link> and <link
	linkend="zfs-term-volume">volumes</link>).</para>

    <sect2 id="zfs-zpool-create">
      <title>Creating &amp; Destroying Storage Pools</title>

      <para>Creating a ZFS Storage Pool (<acronym>zpool</acronym>)
	involves making a number of decisions that are relatively
	permanent because the structure of the pool cannot be
	changed after the pool has been created.  The most important
	decision is what types of vdevs to group the physical disks
	into.  See the list of <link
	linkend="zfs-term-vdev">vdev types</link> for details about
	the possible options.  After the pool has been created, most
	vdev types do not allow additional disks to be added to the
	vdev.  The exceptions are mirrors, which allow additional
	disks to be added to the vdev, and stripes, which can be
	upgraded to mirrors by attaching an additional disk to the
	vdev.  Although additional vdevs can be added to a pool, the
	layout of the pool cannot be changed once the pool has been
	created, instead the data must be backed up and the pool
	recreated.</para>

      <para>A ZFS pool that is no longer needed can be destroyed so
	that the disks making up the pool can be reused in another
	pool or for other purposes.  Destroying a pool involves
	unmouting all of the datasets in that pool.  If the datasets
	are in use, the unmount operation will fail and the pool will
	not be destroyed.  The destruction of the pool can be forced
	with the <option>-f</option> parameter, however this can cause
	undefined behavior in the applications which had open files on
	those datasets.</para>
    </sect2>

    <sect2 id="zfs-zpool-attach">
      <title>Adding &amp; Removing Devices</title>

      <para>Adding disks to a zpool can be broken down into two
	separate cases: attaching a disk to an existing vdev with
	<command>zpool attach</command>, or adding vdevs to the pool
	with <command>zpool add</command>.  Only some <link
	linkend="zfs-term-vdev">vdev types</link> allow disks to be
	added to the vdev after creation.</para>

      <para>When adding disks to the existing vdev is not
	an option, as in the case of RAID-Z, the other option is
	to add a vdev to the pool.  It is possible, but
	discouraged, to mix vdev types.  ZFS stripes data across each
	of the vdevs.  For example, if there are two mirror vdevs,
	then this is effectively a RAID 10, striping the writes across
	the two sets of mirrors.  Because of the way that space is
	allocated in ZFS to attempt to have each vdev reach
	100% full at the same time, there is a performance penalty if
	the vdevs have different amounts of free space.</para>

      <para>Currently, vdevs cannot be removed from a zpool, and disks
	can only be removed from a mirror if there is enough remaining
	redundancy.</para>

    </sect2>

    <sect2 id="zfs-zpool-replace">
      <title>Replacing a Functioning Device</title>

      <para>There are a number of situations in which it may be
	desirable to replace a disk with a different disk.  This
	process requires connecting the new disk at the same time as
	the disk to be replaced.  <command>zpool replace</command>
	will copy all of the data from the old disk to the new one.
	After this operation completes, the old disk is disconnected
	from the vdev.  If the new disk is larger than the old disk,
	it may be possible to grow the zpool, using the new space.
	See <link linkend="zfs-zpool-online">Growing a
	  Pool</link>.</para>
    </sect2>

    <sect2 id="zfs-zpool-resilver">
      <title>Dealing with Failed Devices</title>

      <para>When a disk in a ZFS pool fails, the vdev that the disk
	belongs to will enter the <link
	linkend="zfs-term-degraded">Degraded</link> state.  In this
	state, all of the data stored on the vdev is still available,
	but performance may be impacted because missing data will need
	to be calculated from the available redundancy.  To restore
	the vdev to a fully functional state the failed physical
	device will need to be replace replaced, and ZFS must be
	instructed to begin the <link
	linkend="zfs-term-resilver">resilver</link> operation, where
	data that was on the failed device will be recalculated
	from the available redundancy and written to the replacement
	device.  Once this process has completed the vdev will return
	to <link linkend="zfs-term-online">Online</link> status.  If
	the vdev does not have any redundancy, or if multiple devices
	have failed and there is insufficient redundancy to
	compensate, the pool will enter the <link
	linkend="zfs-term-faulted">Faulted</link> state.  If a
	sufficient number of devices cannot be reconnected to the pool
	then the pool will be inoperative, and data will need to be
	restored from backups.</para>
    </sect2>

    <sect2 id="zfs-zpool-online">
      <title>Growing a Pool</title>

      <para>The usable size of a redundant ZFS pool is limited by the
	size of the smallest device in the vdev.  If each device in
	the vdev is replaced sequentially, after the smallest device
	has completed the <link
	linkend="zfs-zpool-replace">replace</link> or <link
	linkend="zfs-term-resilver">resilver</link> operation, the
	pool can grow based on the size of the new smallest device.
	This expansion can be triggered by using <command>zpool
	online</command> with the <option>-e</option> parameter on
	each device.  After the expansion of each device, the
	additional space will become available in the pool.</para>
    </sect2>

    <sect2 id="zfs-zpool-import">
      <title>Importing &amp; Exporting Pools</title>

      <para>Pools can be exported in preparation for moving them to
	another system.  All datasets are unmounted, and each device
	is marked as exported but still locked so it cannot be used
	by other disk subsystems.  This allows pools to be imported on
	other machines, other operating systems that support ZFS, and
	even different hardware architectures (with some caveats, see
	&man.zpool.8;).  When a dataset has open files,
	<option>-f</option> can be used to force the export
	of a pool.  <option>-f</option> causes the datasets to be
	forcibly unmounted, which can cause undefined behavior in the
	applications which had open files on those datasets.</para>

      <para>Importing a pool automatically mounts the datasets.  This
	may not be the desired behavior, and can be prevented with
	<option>-N</option>.  <option>-o</option> sets temporary
	properties for this import only.  <option>altroot=</option>
	allows importing a zpool with a base mount point instead of
	the root of the file system.  If the pool was last used on a
	different system and was not properly exported, an import
	might have to be forced with <option>-f</option>.
	<option>-a</option> imports all pools that do not appear to be
	in use by another system.</para>
    </sect2>

    <sect2 id="zfs-zpool-upgrade">
      <title>Upgrading a Storage Pool</title>

      <para>After upgrading &os;, or if a pool has been imported from
	a system using an older version of ZFS, the pool can be
	manually upgraded to the latest version of ZFS.  Consider
	whether the pool may ever need to be imported on an older
	system before upgrading.  The upgrade process is unreversible
	and cannot be undone.</para>

      <para>The newer features of ZFS will not be available until
	<command>zpool upgrade</command> has completed.
	<option>-v</option> can be used to see what new features will
	be provided by upgrading, as well as which features are
	already supported by the existing version.</para>
    </sect2>

    <sect2 id="zfs-zpool-status">
      <title>Checking the Status of a Pool</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-zpool-iostat">
      <title>Performance Monitoring</title>

      <para>ZFS has a built-in monitoring system that can display
	statistics about I/O happening on the pool in real-time.
	It shows the amount of free and used space on the pool, how
	many read and write operations are being performed per second,
	and how much I/O bandwidth is currently being utilized for
	read and write operations.  By default, all pools in the
	system will be monitored and displayed.  A pool name can be
	provided as part of the command to monitor just that specific
	pool.  A basic example:</para>

      <screen>&prompt.root; <userinput>zpool iostat</userinput>
               capacity     operations    bandwidth
pool        alloc   free   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
data         288G  1.53T      2     11  11.3K  57.1K</screen>

      <para>To continuously monitor I/O activity on the pool, a
	number can be specified as the last parameter, indicating
	the frequency in seconds to wait between updates.  ZFS will
	print the next statistic line after each interval.  Press
	<keycombo
	action="simul"><keycap>Ctrl</keycap><keycap>C</keycap></keycombo>
	to stop this continuous monitoring.  Alternatively, give a
	second number on the command line after the interval to
	specify the total number of statistics to display.</para>

      <para>Even more detailed pool I/O statistics can be
	displayed with <option>-v</option>.  In this case 
	each storage device in the pool will be shown with a
	corresponding statistics line.  This is helpful to
	determine how many read and write operations are being
	performed on each device, and can help determine if any
	specific device is slowing down I/O on the entire pool.  The
	following example shows a mirrored pool consisting of two
	devices.  For each of these, a separate line is shown with
	the current I/O activity.</para>

      <screen>&prompt.root; <userinput>zpool iostat -v </userinput>
                            capacity     operations    bandwidth
pool                     alloc   free   read  write   read  write
-----------------------  -----  -----  -----  -----  -----  -----
data                      288G  1.53T      2     12  9.23K  61.5K
  mirror                  288G  1.53T      2     12  9.23K  61.5K
    ada1                     -      -      0      4  5.61K  61.7K
    ada2                     -      -      1      4  5.04K  61.7K
-----------------------  -----  -----  -----  -----  -----  -----</screen>
    </sect2>

    <sect2 id="zfs-zpool-split">
      <title>Splitting a Storage Pool</title>

      <para>A ZFS pool consisting of one or more mirror vdevs can be
	split into a second pool.  The last member of each mirror
	(unless otherwise specified) is detached and used to create a
	new pool containing the same data.  It is recommended that
	the operation first be attempted with the <option>-n</option>
	parameter.  This will print out the details of the proposed
	operation without actually performancing it.  This helps
	ensure the operation will happen as expected.</para>
    </sect2>
  </sect1>

  <sect1 id="zfs-zfs">
    <title><command>zfs</command> Administration</title>

    <para>The <command>zfs</command> utility is responsible for
      creating, destroying, and managing all <acronym>ZFS</acronym>
      datasets that exist within a pool.  The pool is managed using
      the <link linkend="zfs-zpool"><command>zpool</command></link>
      command.</para>

    <sect2 id="zfs-zfs-create">
      <title>Creating &amp; Destroying Datasets</title>

      <para>Unlike with traditional disks and volume managers, space
	in <acronym>ZFS</acronym> is not preallocated, allowing
	additional file systems to be created at any time.  With
	traditional file systems, once all of the space was
	partitioned and assigned to a file system, there was no way to
	add an additional file system without adding a new disk.
	<acronym>ZFS</acronym> also allows you to set a number of
	properties on each <link
	linkend="zfs-term-dataset">dataset</link>.  These properties
	include features like compression, deduplication, caching and
	quoteas, as well as other useful properties like readonly,
	case sensitivity, network file sharing and mount point.  Each
	separate dataset can be administered, <link
	linkend="zfs-zfs-allow">delegated</link>, <link
	linkend="zfs-zfs-send">replicated</link>, <link
	linkend="zfs-zfs-snapshot">snapshoted</link>, <link
	linkend="zfs-zfs-jail">jailed</link>, and destroyed as a unit.
	This offers many advantages to creating a separate dataset for
	each different type or set of files.  The only drawback to
	having an extremely large number of datasets, is that some
	commands like <command>zfs list</command> will be slower,
	and the mounting of an extremely large number of datasets
	(100s or 1000s) can make the &os; boot process take
	longer.</para>

      <para>Destroying a dataset is much quicker than deleting all
	of the files that reside on the dataset, as it does not
	invole scanning all of the files and updating all of the
	corresponding metadata.  In modern versions of
	<acronym>ZFS</acronym> the <command>zfs destroy</command>
	operation is asynchronous, the free space may take several
	minutes to appear in the pool.  The <literal>freeing</literal>
	property, accessible with <command>zpool get freeing
	<replaceable>poolname</replaceable></command> indicates how
	many datasets are having their blocks freed in the background.
	If there are child datasets, such as <link
	linkend="zfs-term-snapshot">snapshots</link> or other
	datasets, then the parent cannot be destroyed.  To destroy a
	dataset and all of its children, use the <option>-r</option>
	parameter to recursively destroy the dataset and all of its
	children.  The <option>-n -v</option> parameters can be used
	to not actually perform the destruction, but instead list
	which datasets and snapshots would be destroyed and in the
	case of snapshots, how much space would be reclaimed by
	proceeding with the destruction.</para>
    </sect2>

    <sect2 id="zfs-zfs-volume">
      <title>Creating &amp; Destroying Volumes</title>

      <para>A volume is special type of <acronym>ZFS</acronym>
	dataset.  Rather than being mounted as a file system, it is
	exposed as a block device under
	<devicename>/dev/zvol/<replaceable>poolname</replaceable>/<replaceable>dataset</replaceable></devicename>.
	This allows the volume to be used for other file systems, to
	back the disks of a virtual machine, or to be exported using
	protocols like iSCSI or HAST.</para>

      <para>A volume can be formatted with any filesystem on top of
	it.  This will appear to the user as if they are working with
	a regular disk using that specific filesystem and not ZFS.
	In this way, non-ZFS file systems can be augmented with
	ZFS features that they would not normally have.  For example,
	combining the ZFS compression property together with a
	250&nbsp;MB volume allows creation of a compressed FAT
	filesystem.</para>

      <screen>&prompt.root; <userinput>zfs create -V 250m -o compression=on tank/fat32</userinput>
&prompt.root; <userinput>zfs list tank</userinput>
NAME USED AVAIL REFER MOUNTPOINT
tank 258M  670M   31K /tank
&prompt.root; <userinput>newfs_msdos -F32 /dev/zvol/tank/fat32</userinput>
&prompt.root; <userinput>mount -t msdosfs /dev/zvol/tank/fat32 /mnt</userinput>
&prompt.root; <userinput>df -h /mnt | grep fat32</userinput>
Filesystem           Size Used Avail Capacity Mounted on
/dev/zvol/tank/fat32 249M  24k  249M     0%   /mnt
&prompt.root; <userinput>mount | grep fat32</userinput>
/dev/zvol/tank/fat32 on /mnt (msdosfs, local)</screen>

      <para>Destroying a volume is much the same as destroying a
	regular filesystem dataset.  The operation is nearly
	instantaneous, but it make take several minutes for the free
	space to be reclaimed in the background.</para>

    </sect2>

    <sect2 id="zfs-zfs-rename">
      <title>Renaming a Dataset</title>

      <para>The name of a dataset can be changed using <command>zfs
	  rename</command>.  The rename command can also be used to
	change the parent of a dataset.  Renaming a dataset to be
	under a different parent dataset will change the value of
	those properties that are inherited by the child dataset.
	When a dataset is renamed, it is unmounted and then remounted
	in the new location (inherited from the parent dataset).  This
	behavior can be prevented using the <option>-u</option>
	parameter.  Due to the nature of snapshots, they cannot be
	renamed outside of the parent dataset.  To rename a recursive
	snapshot, specify the <option>-r</option> parameter, and all
	snapshots with the same specified snapshot will be
	renamed.</para>
    </sect2>

    <sect2 id="zfs-zfs-set">
      <title>Setting Dataset Properties</title>

      <para>Each <acronym>ZFS</acronym> dataset has a number of
	properties to control its behavior.  Most properties are
	automatically inherited from the parent dataset, but can be
	overridden locally.  Set a property on a dataset with
	<command>zfs set
	<replaceable>property</replaceable>=<replaceable>value</replaceable>
	<replaceable>dataset</replaceable></command>.  Most properties
	have a limited set of valid values, <command>zfs get</command>
	will display each possible property and its valid values.
	Most properties can be reverted to their inherited values
	using <command>zfs inherit</command>.</para>

      <para>It is possible to set user-defined properties in ZFS.
	They become part of the dataset configuration and can be used
	to provide additional information about the dataset or its
	contents.  To distinguish these custom properties from the
	ones supplied as part of ZFS, a colon (<literal>:</literal>)
	is used to create a custom namespace for the property.</para>

      <screen>&prompt.root; <userinput>zfs set <replaceable>custom</replaceable>:<replaceable>costcenter</replaceable>=<replaceable>1234</replaceable> <replaceable>tank</replaceable></userinput>
&prompt.root; <userinput>zfs get <replaceable>custom</replaceable>:<replaceable>costcenter</replaceable> <replaceable>tank</replaceable></userinput>
NAME PROPERTY           VALUE SOURCE
tank custom:costcenter  1234  local</screen>

      <para>To remove such a custom property again, use the
	<command>zfs inherit</command> command with the
	<option>-r</option> option.  If the custom property is not
	defined in any of the parent datasets, it will be removed
	completely (although the changes are still recorded in the
	pool's history).</para>

      <screen>&prompt.root; <userinput>zfs inherit -r <replaceable>custom</replaceable>:<replaceable>costcenter</replaceable> <replaceable>tank</replaceable></userinput>
&prompt.root; <userinput>zfs get <replaceable>custom</replaceable>:<replaceable>costcenter</replaceable> <replaceable>tank</replaceable></userinput>
NAME    PROPERTY           VALUE              SOURCE
tank    custom:costcenter  -                  -
&prompt.root; <userinput>zfs get all <replaceable>tank</replaceable> | grep <replaceable>custom</replaceable>:<replaceable>costcenter</replaceable></userinput>
&prompt.root;</screen>
    </sect2>

    <sect2 id="zfs-zfs-snapshot">
      <title>Managing Snapshots</title>

      <para><link linkend="zfs-term-snapshot">Snapshots</link> are one
	of the most powerful features of <acronym>ZFS</acronym>.  A
	snapshot provides a point-in-time copy of the dataset that the
	parent dataset can be rolled back to if required.  Create a
	snapshot with <command>zfs snapshot
	<replaceable>dataset</replaceable>@<replaceable>snapshotname</replaceable></command>.
	Specifying the <option>-r</option> parameter will recursively
	create a snapshot with the same name on all child
	datasets.</para>

      <para>By default, snapshots are mounted in a hidden directory
	under the parent dataset: <filename
	class="directory">.zfs/snapshots/<replaceable>snapshotname</replaceable></filename>.
	Individual files can easily be restored to a previous state by
	copying them from the snapshot back to the parent dataset.  It
	is also possible to revert the entire dataset back to the
	point-in-time of the snapshot using <command>zfs
	  rollback</command>.</para>

      <para>Snapshots consume space based on how much the parent file
	system has changed since the time of the snapshot.  The
	<literal>written</literal> property of a snapshot tracks how
	much space is being used by a snapshot.</para>

      <para>To destroy a snapshot and recover the space consumed by
	the overwritten or deleted files, run <command>zfs destroy
	<replaceable>dataset</replaceable>@<replaceable>snapshot</replaceable></command>.
	The <option>-r</option> parameter will recursively remove all
	snapshots with the same name under the parent dataset.  Adding
	the <option>-n -v</option> parameters to the destroy command
	will display a list of the snapshots that would be deleted and
	an estimate of how much space would be reclaimed by proceeding
	with the destroy operation.</para>
    </sect2>

    <sect2 id="zfs-zfs-clones">
      <title>Managing Clones</title>

      <para>A clone is a copy of a snapshot that is treated more like
	a regular dataset.  Unlike a snapshot, a clone is not read
	only, is mounted, and can have its own properties.  Once a
	clone has been created, the snapshot it was created from
	cannot be destroyed.  The child/parent relationship between
	the clone and the snapshot can be reversed using <command>zfs
	promote</command>.  After a clone has been promoted, the
	snapshot becomes a child of the clone, rather than of the
	original parent dataset.  This will change how the space is
	accounted, but not actually change the amount of space
	consumed.</para>
    </sect2>

    <sect2 id="zfs-zfs-send">
      <title>ZFS Replication</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-zfs-quota">
      <title>Dataset, User and Group Quotas</title>

      <para><link linkend="zfs-term-quota">Dataset
	  quotas</link> can be used to restrict the amount of space
	that can be consumed by a peticular dataset.  <link
	linkend="zfs-term-refquota">Reference Quotas</link> work in
	very much the same way, except they only count the space used
	by the dataset it self, excluding snapshots and child
	datasets.  Similarly <link
	linkend="zfs-term-userquota">user</link> and <link
	linkend="zfs-term-groupquota">group</link> quotas can be used
	to prevent users or groups from consuming all of the available
	space in the pool or dataset.</para>

      <para>To enforce a dataset quota of 10&nbsp;GB for
	<filename>storage/home/bob</filename>, use the
	following:</para>

      <screen>&prompt.root; <userinput>zfs set quota=10G storage/home/bob</userinput></screen>

      <para>To enforce a reference quota of 10&nbsp;GB for
	<filename>storage/home/bob</filename>, use the
	following:</para>

      <screen>&prompt.root; <userinput>zfs set refquota=10G storage/home/bob</userinput></screen>

      <para>The general format is
	<literal>userquota@<replaceable>user</replaceable>=<replaceable>size</replaceable></literal>,
	and the user's name must be in one of the following
	formats:</para>

      <itemizedlist>
	<listitem>
	  <para><acronym>POSIX</acronym> compatible name such as
	    <replaceable>joe</replaceable>.</para>
	</listitem>

	<listitem>
	  <para><acronym>POSIX</acronym> numeric ID such as
	    <replaceable>789</replaceable>.</para>
	</listitem>

	<listitem>
	  <para><acronym>SID</acronym> name
	    such as
	    <replaceable>joe.bloggs@example.com</replaceable>.</para>
	</listitem>

	<listitem>
	  <para><acronym>SID</acronym>
	    numeric ID such as
	    <replaceable>S-1-123-456-789</replaceable>.</para>
	</listitem>
      </itemizedlist>

      <para>For example, to enforce a user quota of 50&nbsp;GB for a
	user named <replaceable>joe</replaceable>, use the
	following:</para>

      <screen>&prompt.root; <userinput>zfs set userquota@joe=50G</userinput></screen>

      <para>To remove the quota or make sure that one is not set,
	instead use:</para>

      <screen>&prompt.root; <userinput>zfs set userquota@joe=none</userinput></screen>

      <note>
	<para>User quota properties are not displayed by
	  <command>zfs get all</command>.
	  Non-<username>root</username> users can only see their own
	  quotas unless they have been granted the
	  <literal>userquota</literal> privilege.  Users with this
	  privilege are able to view and set everyone's quota.</para>
      </note>

      <para>The general format for setting a group quota is:
	<literal>groupquota@<replaceable>group</replaceable>=<replaceable>size</replaceable></literal>.</para>

      <para>To set the quota for the group
	<replaceable>firstgroup</replaceable> to 50&nbsp;GB,
	use:</para>

      <screen>&prompt.root; <userinput>zfs set groupquota@firstgroup=50G</userinput></screen>

      <para>To remove the quota for the group
	<replaceable>firstgroup</replaceable>, or to make sure that
	one is not set, instead use:</para>

      <screen>&prompt.root; <userinput>zfs set groupquota@firstgroup=none</userinput></screen>

      <para>As with the user quota property,
	non-<username>root</username> users can only see the quotas
	associated with the groups that they belong to.  However,
	<username>root</username> or a user with the
	<literal>groupquota</literal> privilege can view and set all
	quotas for all groups.</para>

      <para>To display the amount of space consumed by each user on
	the specified filesystem or snapshot, along with any specified
	quotas, use <command>zfs userspace</command>.  For group
	information, use <command>zfs groupspace</command>.  For more
	information about supported options or how to display only
	specific options, refer to &man.zfs.1;.</para>

      <para>Users with sufficient privileges and
	<username>root</username> can list the quota for
	<filename>storage/home/bob</filename> using:</para>

      <screen>&prompt.root; <userinput>zfs get quota storage/home/bob</userinput></screen>
    </sect2>

    <sect2 id="zfs-zfs-reservation">
      <title>Reservations</title>

      <para><link linkend="zfs-term-reservation">Reservations</link>
	guarantee a minimum amount of space will always be available
	to a dataset.  The reserved space will not
	be available to any other dataset.  This feature can be
	especially useful to ensure that users cannot comsume all of
	the free space, leaving none for an important dataset or log
	files.</para>

      <para>The general format of the <literal>reservation</literal>
	property is
	<literal>reservation=<replaceable>size</replaceable></literal>,
	so to set a reservation of 10&nbsp;GB on
	<filename>storage/home/bob</filename>, use:</para>

      <screen>&prompt.root; <userinput>zfs set reservation=10G storage/home/bob</userinput></screen>

      <para>To make sure that no reservation is set, or to remove a
	reservation, use:</para>

      <screen>&prompt.root; <userinput>zfs set reservation=none storage/home/bob</userinput></screen>

      <para>The same principle can be applied to the
	<literal>refreservation</literal> property for setting a
	<link linkend="zfs-term-refreservation">Reference
	Reservation</link>, with the general format
	<literal>refreservation=<replaceable>size</replaceable></literal>.</para>

      <para>To check if any reservations or refreservations exist on
	<filename>storage/home/bob</filename>, execute one of the
	following commands:</para>

      <screen>&prompt.root; <userinput>zfs get reservation storage/home/bob</userinput>
&prompt.root; <userinput>zfs get refreservation storage/home/bob</userinput></screen>
    </sect2>

    <sect2 id="zfs-zfs-compression">
      <title>Compression</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-zfs-deduplication">
      <title>Deduplication</title>

      <para>When enabled, <link
	  linkend="zfs-term-deduplication">Deduplication</link> uses
	the checksum of each block to detect duplicate blocks.  When a
	new block is about to be written and it is determined to be a
	duplicate of an existing block, rather than writing the same
	data again, <acronym>ZFS</acronym> just references the
	existing data on disk an additional time.  This can offer
	tremendous space savings if your data contains many discreet
	copies of the file information.  Deduplication requires an
	extremely large amount of memory, and most of the space
	savings can be had without the extra cost by enabling
	compression instead.</para>

      <para>To activate deduplication, you simply need to set the
	following property on the target pool.</para>

      <screen>&prompt.root; <userinput>zfs set dedup=on <replaceable>pool</replaceable></userinput></screen>

      <para>it is important to mention that only new data being
	written to the pool will be deduplicated.  Data that is
	already residing on the pool will not be deduplicated by
	activating this option.  As such, a pool with a freshly
	activated deduplication property will look something like this
	example.</para>

      <screen>&prompt.root; <userinput>zpool list</userinput>
NAME  SIZE ALLOC  FREE CAP DEDUP HEALTH ALTROOT
pool 2.84G 2.19M 2.83G  0% 1.00x ONLINE -</screen>

      <para>The <literal>DEDUP</literal> column shows the actual rate
	of deduplication for that pool.  A value of
	<literal>1.00x</literal> that no data has been deduplicated
	due to insufficient duplicate data.  In the following example,
	the ports tree is copied three times into different
	directories on the deduplicated pool above to provide
	redundancy.</para>

      <screen>&prompt.root; <userinput>zpool list</userinput>
for d in dir1 dir2 dir3; do
for> mkdir $d &amp;&amp; cp -R /usr/ports $d &amp;
for> done</screen>

      <para>Now that redundant data has been created, ZFS detects that
	and makes sure that the data is not taking up additional
	space.</para>

      <screen>&prompt.root; <userinput>zpool list</userinput>
NAME SIZE  ALLOC FREE CAP DEDUP HEALTH ALTROOT
pool 2.84G 20.9M 2.82G 0% 3.00x ONLINE -</screen>

      <para>The <literal>DEDUP</literal> column does now contain the
	value <literal>3.00x</literal>. This indicates that ZFS
	detected the copies of the ports tree data and was able to
	deduplicate it at a ratio of 1/3.  The space savings that this
	yields can be enormous, but only when there is enough memory
	available to keep track of the deduplicated blocks.</para>

      <para>Deduplication is not always beneficial, especially when
	there is not much redundant data on a ZFS pool.  To see how
	much space could be saved by deduplication for a given set of
	data that is already stored in a pool, ZFS can simulate the
	effects that deduplication would have.  To do that, the
	following command can be invoked on the pool.</para>

      <screen>&prompt.root; <userinput>zdb -S <replaceable>pool</replaceable></userinput>
Simulated DDT histogram:

bucket              allocated                       referenced
______   ______________________________   ______________________________
refcnt   blocks   LSIZE   PSIZE   DSIZE   blocks   LSIZE   PSIZE   DSIZE
------   ------   -----   -----   -----   ------   -----   -----   -----
     1    2.58M    289G    264G    264G    2.58M    289G    264G    264G
     2     206K   12.6G   10.4G   10.4G     430K   26.4G   21.6G   21.6G
     4    37.6K    692M    276M    276M     170K   3.04G   1.26G   1.26G
     8    2.18K   45.2M   19.4M   19.4M    20.0K    425M    176M    176M
    16      174   2.83M   1.20M   1.20M    3.33K   48.4M   20.4M   20.4M
    32       40   2.17M    222K    222K    1.70K   97.2M   9.91M   9.91M
    64        9     56K   10.5K   10.5K      865   4.96M    948K    948K
   128        2   9.50K      2K      2K      419   2.11M    438K    438K
   256        5   61.5K     12K     12K    1.90K   23.0M   4.47M   4.47M
    1K        2      1K      1K      1K    2.98K   1.49M   1.49M   1.49M
 Total    2.82M    303G    275G    275G    3.20M    319G    287G    287G

dedup = 1.05, compress = 1.11, copies = 1.00, dedup * compress / copies = 1.16</screen>

      <para>After <command>zdb -S</command> finished analyzing the
	pool, it outputs a summary that shows the ratio that would
	result in activating deduplication.  In this case,
	<literal>1.16</literal> is a very poor rate that is mostly
	influenced by compression.  Activating deduplication on this
	pool would not save any significant amount of space.  Keeping
	the formula <literal>dedup * compress / copies = deduplication
	ratio</literal> in mind, a system administrator can plan the
	storage allocation more towards having multiple copies of data
	or by having a decent compression rate in order to utilize the
	space savings that deduplication provides.  As a rule of
	thumb, compression should be used first before deduplication
	due to the lower memory requirements.</para>
    </sect2>

    <sect2 id="zfs-zfs-jail">
      <title>ZFS and Jails</title>

      <para><command>zfs jail</command> and the corresponding
	<literal>jailed</literal> property are used to delegate a
	<acronym>ZFS</acronym> dataset to a <link
	linkend="jails">Jail</link>.  <command>zfs jail
	<replaceable>jailid</replaceable></command> attaches a dataset
	to the specified jail, and the <command>zfs unjail</command>
	detaches it.  In order for the dataset to be administered from
	within a jail, the <literal>jailed</literal> property must be
	set.  Once a dataset is jailed it can no longer be mounted on
	the host, because the jail administrator may have set
	unacceptable mount points.</para>
    </sect2>
  </sect1>

  <sect1 id="zfs-zfs-allow">
    <title>Delegated Administration</title>

    <para>ZFS features a comprehensive delegation system to assign
      permissions to performs the various ZFS administration functions
      to a regular user.  For example, if each users' home directory
      is a dataset, then each user could be delegated permission to
      create and destroy snapshots of their home directory.  A backup
      user could be assigned the permissions required to make use of
      the ZFS replication features without requiring root access, or
      isolate a usage collection script to run as an unprivledged user
      with access to only the space utilization data of all users.  It
      is even possible to delegate the ability to delegate
      permissions.  It is possible to delegate permissions over each
      ZFS subcommand and most ZFS properties.</para>

    <sect2 id="zfs-zfs-allow-create">
      <title>Delegating Dataset Creation</title>

      <para>Using the <userinput>zfs allow
	<replaceable>someuser</replaceable> create
	<replaceable>mydataset</replaceable></userinput> command will
	give the indicated user the required permissions to create
	child datasets under the selected parent dataset.  There is a
	caveat, creating a new dataset involves mouting it, which
	requires the <literal>vfs.usermount</literal> sysctl be
	enabled in order to allow non-root users to mount a
	filesystem.  There is the further restriction that non-root
	users must own the directory they are mounting the filesystem
	to, in order to prevent abuse.</para>
    </sect2>

    <sect2 id="zfs-zfs-allow-allow">
      <title>Delegating Permission Delegation</title>

      <para>Using the <userinput>zfs allow
	<replaceable>someuser</replaceable> allow
	<replaceable>mydataset</replaceable></userinput> command will
	give the indicated user the ability to assign any permission
	they have on the target dataset (or its children) to other
	users.  If a user has the <literal>snapshot</literal>
	permission and the <literal>allow</literal> permission that
	user can then grant the snapshot permission to some other
	users.</para>
    </sect2>
  </sect1>

  <sect1 id="zfs-advanced">
    <title>ZFS Advanced Topics</title>

    <sect2 id="zfs-advanced-tuning">
      <title>ZFS Tuning</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-advanced-booting">
      <title>Booting Root on ZFS</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-advanced-beadm">
      <title>ZFS Boot Environments</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-advanced-troubleshoot">
      <title>Troubleshooting</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-advanced-i386">
      <title>ZFS on i386</title>

      <para>Some of the features provided by <acronym>ZFS</acronym>
	are RAM-intensive, so some tuning may be required to provide
	maximum efficiency on systems with limited
	<acronym>RAM</acronym>.</para>

      <sect3>
	<title>Memory</title>

	<para>At a bare minimum, the total system memory should be at
	  least one gigabyte.  The amount of recommended
	  <acronym>RAM</acronym> depends upon the size of the pool and
	  the <acronym>ZFS</acronym> features which are used.  A
	  general rule of thumb is 1&nbsp;GB of RAM for every
	  1&nbsp;TB of storage.  If the deduplication feature is used,
	  a general rule of thumb is 5&nbsp;GB of RAM per TB of
	  storage to be deduplicated.  While some users successfully
	  use <acronym>ZFS</acronym> with less <acronym>RAM</acronym>,
	  it is possible that when the system is under heavy load, it
	  may panic due to memory exhaustion.  Further tuning may be
	  required for systems with less than the recommended RAM
	  requirements.</para>
      </sect3>

      <sect3>
	<title>Kernel Configuration</title>

	<para>Due to the <acronym>RAM</acronym> limitations of the
	  &i386; platform, users using <acronym>ZFS</acronym> on the
	  &i386; architecture should add the following option to a
	  custom kernel configuration file, rebuild the kernel, and
	  reboot:</para>

	<programlisting>options        KVA_PAGES=512</programlisting>

	<para>This option expands the kernel address space, allowing
	  the <varname>vm.kvm_size</varname> tunable to be pushed
	  beyond the currently imposed limit of 1&nbsp;GB, or the
	  limit of 2&nbsp;GB for <acronym>PAE</acronym>.  To find the
	  most suitable value for this option, divide the desired
	  address space in megabytes by four (4).  In this example, it
	  is <literal>512</literal> for 2&nbsp;GB.</para>
      </sect3>

      <sect3>
	<title>Loader Tunables</title>

	<para>The <devicename>kmem</devicename> address space can be
	  increased on all &os; architectures.  On a test system with
	  one gigabyte of physical memory, success was achieved with
	  the following options added to
	  <filename>/boot/loader.conf</filename>, and the system
	  restarted:</para>

	<programlisting>vm.kmem_size="330M"
vm.kmem_size_max="330M"
vfs.zfs.arc_max="40M"
vfs.zfs.vdev.cache.size="5M"</programlisting>

	<para>For a more detailed list of recommendations for
	  <acronym>ZFS</acronym>-related tuning, see <ulink
	    url="http://wiki.freebsd.org/ZFSTuningGuide"></ulink>.</para>
      </sect3>
    </sect2>
  </sect1>

  <sect1 id="zfs-links">
    <title>Additional Resources</title>

    <itemizedlist>
      <listitem>
	<para><ulink url="https://wiki.freebsd.org/ZFS">FreeBSD Wiki -
	    ZFS</ulink></para>
      </listitem>

      <listitem>
	<para><ulink
	    url="https://wiki.freebsd.org/ZFSTuningGuide">FreeBSD Wiki
	    - ZFS Tuning</ulink></para>
      </listitem>

      <listitem>
	<para><ulink
	    url="http://wiki.illumos.org/display/illumos/ZFS">Illumos
	    Wiki - ZFS</ulink></para>
      </listitem>

      <listitem>
	<para><ulink
	    url="http://docs.oracle.com/cd/E19253-01/819-5461/index.html">Oracle
	    Solaris ZFS Administration
	    Guide</ulink></para>
      </listitem>

      <listitem>
	<para><ulink
	    url="http://www.solarisinternals.com/wiki/index.php/ZFS_Evil_Tuning_Guide">ZFS
	    Evil Tuning Guide</ulink></para>
      </listitem>

      <listitem>
	<para><ulink
	    url="http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide">ZFS
	    Best Practices Guide</ulink></para>
      </listitem>
    </itemizedlist>

    <sect2 id="zfs-history">
      <title>History of <acronym>ZFS</acronym></title>

      <para></para>
    </sect2>
  </sect1>

  <sect1 id="zfs-term">
    <title><acronym>ZFS</acronym> Features and Terminology</title>

    <para><acronym>ZFS</acronym> is a fundamentally different file
      system because it is more than just a file system.
      <acronym>ZFS</acronym> combines the roles of file system and
      volume manager, enabling additional storage devices to be added
      to a live system and having the new space available on all of
      the existing file systems in that pool immediately.  By
      combining the traditionally separate roles,
      <acronym>ZFS</acronym> is able to overcome previous limitations
      that prevented <acronym>RAID</acronym> groups being able to
      grow.  Each top level device in a zpool is called a vdev, which
      can be a simple disk or a <acronym>RAID</acronym> transformation
      such as a mirror or <acronym>RAID-Z</acronym> array.
      <acronym>ZFS</acronym> file systems (called datasets), each have
      access to the combined free space of the entire pool.  As blocks
      are allocated from the pool, the space available to each file
      system decreases.  This approach avoids the common pitfall with
      extensive partitioning where free space becomes fragmentated
      across the partitions.</para>

    <informaltable pgwide="1">
      <tgroup cols="2">
	<tbody valign="top">
	  <row>
	    <entry id="zfs-term-zpool">zpool</entry>

	    <entry>A storage pool is the most basic building block of
	      <acronym>ZFS</acronym>.  A pool is made up of one or
	      more vdevs, the underlying devices that store the data.
	      A pool is then used to create one or more file systems
	      (datasets) or block devices (volumes).  These datasets
	      and volumes share the pool of remaining free space.
	      Each pool is uniquely identified by a name and a
	      <acronym>GUID</acronym>.  The zpool also controls the
	      version number and therefore the features available for
	      use with <acronym>ZFS</acronym>.

	      <note>
		<para>&os;&nbsp;9.0 and 9.1 include support for
		  <acronym>ZFS</acronym> version 28.  Future versions
		  use <acronym>ZFS</acronym> version 5000 with feature
		  flags.  This allows greater cross-compatibility with
		  other implementations of
		  <acronym>ZFS</acronym>.</para>
	      </note></entry>
	  </row>

	  <row>
	    <entry id="zfs-term-vdev">vdev&nbsp;Types</entry>

	    <entry>A zpool is made up of one or more vdevs, which
	      themselves can be a single disk or a group of disks, in
	      the case of a <acronym>RAID</acronym> transform.  When
	      multiple vdevs are used, <acronym>ZFS</acronym> spreads
	      data across the vdevs to increase performance and
	      maximize usable space.

	      <itemizedlist>
		<listitem>
		  <para id="zfs-term-vdev-disk">
		    <emphasis>Disk</emphasis> - The most basic type
		    of vdev is a standard block device.  This can be
		    an entire disk (such as
		    <devicename><replaceable>/dev/ada0</replaceable></devicename>
		    or
		    <devicename><replaceable>/dev/da0</replaceable></devicename>)
		    or a partition
		    (<devicename><replaceable>/dev/ada0p3</replaceable></devicename>).
		    Contrary to the Solaris documentation, on &os;
		    there is no performance penalty for using a
		    partition rather than an entire disk.</para>
		</listitem>

		<listitem>
		  <para id="zfs-term-vdev-file">
		    <emphasis>File</emphasis> - In addition to disks,
		    <acronym>ZFS</acronym> pools can be backed by
		    regular files, this is especially useful for
		    testing and experimentation.  Use the full path to
		    the file as the device path in the zpool create
		    command.  All vdevs must be atleast 128&nbsp;MB in
		    size.</para>
		</listitem>

		<listitem>
		  <para id="zfs-term-vdev-mirror">
		    <emphasis>Mirror</emphasis> - When creating a
		    mirror, specify the <literal>mirror</literal>
		    keyword followed by the list of member devices
		    for the mirror.  A mirror consists of two or
		    more devices, all data will be written to all
		    member devices.  A mirror vdev will only hold as
		    much data as its smallest member.  A mirror vdev
		    can withstand the failure of all but one of its
		    members without losing any data.</para>

		  <note>
		    <para>regular single disk vdev can be upgraded to
		      a mirror vdev at any time using the
		      <command>zpool</command> <link
			linkend="zfs-zpool-attach">attach</link>
		      command.</para>
		  </note>
		</listitem>

		<listitem>
		  <para id="zfs-term-vdev-raidz">
		    <emphasis><acronym>RAID-Z</acronym></emphasis> -
		    <acronym>ZFS</acronym> implements
		    <acronym>RAID-Z</acronym>, a variation on standard
		    <acronym>RAID-5</acronym> that offers better
		    distribution of parity and eliminates the
		    "<acronym>RAID-5</acronym> write hole" in which
		    the data and parity information become
		    inconsistent after an unexpected restart.
		    <acronym>ZFS</acronym> supports 3 levels of
		    <acronym>RAID-Z</acronym> which provide varying
		    levels of redundancy in exchange for decreasing
		    levels of usable storage.  The types are named
		    <acronym>RAID-Z1</acronym> through
		    <acronym>RAID-Z3</acronym> based on the number of
		    parity devinces in the array and the number of
		    disks that the pool can operate without.</para>

		  <para>In a <acronym>RAID-Z1</acronym> configuration
		    with 4 disks, each 1&nbsp;TB, usable storage will
		    be 3&nbsp;TB and the pool will still be able to
		    operate in degraded mode with one faulted disk.
		    If an additional disk goes offline before the
		    faulted disk is replaced and resilvered, all data
		    in the pool can be lost.</para>

		  <para>In a <acronym>RAID-Z3</acronym> configuration
		    with 8 disks of 1&nbsp;TB, the volume would
		    provide 5&nbsp;TB of usable space and still be
		    able to operate with three faulted disks.  &sun;
		    recommends no more than 9 disks in a single vdev.
		    If the configuration has more disks, it is
		    recommended to divide them into separate vdevs and
		    the pool data will be striped across them.</para>

		  <para>A configuration of 2
		    <acronym>RAID-Z2</acronym> vdevs consisting of 8
		    disks each would create something similar to a
		    <acronym>RAID-60</acronym> array.  A
		    <acronym>RAID-Z</acronym> group's storage capacity
		    is approximately the size of the smallest disk,
		    multiplied by the number of non-parity disks.
		    Four 1&nbsp;TB disks in <acronym>RAID-Z1</acronym>
		    has an effective size of approximately 3&nbsp;TB,
		    and an array of eight 1&nbsp;TB disks in
		    <acronym>RAID-Z3</acronym> will yield 5&nbsp;TB of
		    usable space.</para>
		</listitem>

		<listitem>
		  <para id="zfs-term-vdev-spare">
		    <emphasis>Spare</emphasis> -
		    <acronym>ZFS</acronym> has a special pseudo-vdev
		    type for keeping track of available hot spares.
		    Note that installed hot spares are not deployed
		    automatically; they must manually be configured to
		    replace the failed device using
		    <command>zfs replace</command>.</para>
		</listitem>

		<listitem>
		  <para id="zfs-term-vdev-log">
		    <emphasis>Log</emphasis> - <acronym>ZFS</acronym>
		    Log Devices, also known as ZFS Intent Log
		    (<link
		    linkend="zfs-term-zil"><acronym>ZIL</acronym></link>)
		    move the intent log from the regular pool devices
		    to a dedicated device, typically an
		    <acronym>SSD</acronym>.  Having a dedicated log
		    device can significantly improve the performance
		    of applications with a high volume of synchronous
		    writes, especially databases.  Log devices can be
		    mirrored, but <acronym>RAID-Z</acronym> is not
		    supported.  If multiple log devices are used,
		    writes will be load balanced across them.</para>
		</listitem>

		<listitem>
		  <para id="zfs-term-vdev-cache">
		    <emphasis>Cache</emphasis> - Adding a cache vdev
		    to a zpool will add the storage of the cache to
		    the <link
		    linkend="zfs-term-l2arc"><acronym>L2ARC</acronym></link>.
		    Cache devices cannot be mirrored.  Since a cache
		    device only stores additional copies of existing
		    data, there is no risk of data loss.</para>
		</listitem>
	      </itemizedlist></entry>
	  </row>

	  <row>
	    <entry id="zfs-term-arc">Adaptive Replacement
	      Cache (<acronym>ARC</acronym>)</entry>

	    <entry><acronym>ZFS</acronym> uses an Adaptive Replacement
	      Cache (<acronym>ARC</acronym>), rather than a more
	      traditional Least Recently Used (<acronym>LRU</acronym>)
	      cache.  An <acronym>LRU</acronym> cache is a simple list
	      of items in the cache sorted by when each object was
	      most recently used; new items are added to the top of
	      the list and once the cache is full items from the
	      bottom of the list are evicted to make room for more
	      active objects.  An <acronym>ARC</acronym> consists of
	      four lists; the Most Recently Used
	      (<acronym>MRU</acronym>) and Most Frequently Used
	      (<acronym>MFU</acronym>) objects, plus a ghost list for
	      each.  These ghost lists track recently evicted objects
	      to prevent them from being added back to the cache.
	      This increases the cache hit ratio by avoiding objects
	      that have a history of only being used occasionally.
	      Another advantage of using both an
	      <acronym>MRU</acronym> and <acronym>MFU</acronym> is
	      that scanning an entire filesystem would normally evict
	      all data from an <acronym>MRU</acronym> or
	      <acronym>LRU</acronym> cache in favor of this freshly
	      accessed content.  In the case of
	      <acronym>ZFS</acronym>, since there is also an
	      <acronym>MFU</acronym> that only tracks the most
	      frequently used objects, the cache of the most commonly
	      accessed blocks remains.</entry>
	  </row>

	  <row>
	    <entry
	      id="zfs-term-l2arc"><acronym>L2ARC</acronym></entry>

	    <entry>The <acronym>L2ARC</acronym> is the second level
	      of the <acronym>ZFS</acronym> caching system.  The
	      primary <acronym>ARC</acronym> is stored in
	      <acronym>RAM</acronym>, however since the amount of
	      available <acronym>RAM</acronym> is often limited,
	      <acronym>ZFS</acronym> can also make use of
	      <link linkend="zfs-term-vdev-cache">cache</link>
	      vdevs.  Solid State Disks (<acronym>SSD</acronym>s) are
	      often used as these cache devices due to their higher
	      speed and lower latency compared to traditional spinning
	      disks.  An <acronym>L2ARC</acronym> is entirely
	      optional, but having one will significantly increase
	      read speeds for files that are cached on the
	      <acronym>SSD</acronym> instead of having to be read from
	      the regular spinning disks.  The
	      <acronym>L2ARC</acronym> can also speed up <link
		linkend="zfs-term-deduplication">deduplication</link>
	      since a <acronym>DDT</acronym> that does not fit in
	      <acronym>RAM</acronym> but does fit in the
	      <acronym>L2ARC</acronym> will be much faster than if the
	      <acronym>DDT</acronym> had to be read from disk.  The
	      rate at which data is added to the cache devices is
	      limited to prevent prematurely wearing out the
	      <acronym>SSD</acronym> with too many writes.  Until the
	      cache is full (the first block has been evicted to make
	      room), writing to the <acronym>L2ARC</acronym> is
	      limited to the sum of the write limit and the boost
	      limit, then after that limited to the write limit.  A
	      pair of sysctl values control these rate limits;
	      <literal>vfs.zfs.l2arc_write_max</literal> controls how
	      many bytes are written to the cache per second, while
	      <literal>vfs.zfs.l2arc_write_boost</literal> adds to
	      this limit during the "Turbo Warmup Phase" (Write
	      Boost).</entry>
	  </row>

	  <row>
	    <entry
	      id="zfs-term-zil"><acronym>ZIL</acronym></entry>

	    <entry>The <acronym>ZIL</acronym> accelerates synchronous
	      transactions by using storage devices (such as
	      <acronym>SSD</acronym>s) that are faster than those used
	      for the main storage pool.  When data is being written
	      and the application requests a synchronous write (a
	      guarantee that the data has been safely stored to disk
	      rather than only cached to be written later), the data
	      is written to the faster <acronym>ZIL</acronym> storage,
	      then later flushed out to the regular disks, greatly
	      reducing the latency and increasing performance.
	      Only workloads that are synchronous such as databases
	      will benefit from a <acronym>ZIL</acronym>.  Regular
	      asynchronous writes such as copying files will not use
	      the <acronym>ZIL</acronym> at all.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-cow">Copy-On-Write</entry>

	    <entry>Unlike a traditional file system, when data is
	      overwritten on <acronym>ZFS</acronym> the new data is
	      written to a different block rather than overwriting the
	      old data in place.  Only once this write is complete is
	      the metadata then updated to point to the new location
	      of the data.  This means that in the event of a shorn
	      write (a system crash or power loss in the middle of
	      writing a file), the entire original contents of the
	      file are still available and the incomplete write is
	      discarded.  This also means that <acronym>ZFS</acronym>
	      does not require a &man.fsck.8; after an unexpected
	      shutdown.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-dataset">Dataset</entry>

	    <entry>Dataset is the generic term for a
	      <acronym>ZFS</acronym> file system, volume, snapshot or
	      clone.  Each dataset will have a unique name in the
	      format: <literal>poolname/path@snapshot</literal>.  The
	      root of the pool is technically a dataset as well.
	      Child datasets are named hierarchically like
	      directories; for example,
	      <literal>mypool/home</literal>, the home dataset, is a
	      child of <literal>mypool</literal> and inherits
	      properties from it.  This can be expanded further by
	      creating <literal>mypool/home/user</literal>.  This
	      grandchild dataset will inherity properties from the
	      parent and grandparent.  It is also possible to set
	      properties on a child to override the defaults inherited
	      from the parents and grandparents.
	      <acronym>ZFS</acronym> also allows administration of
	      datasets and their children to be <link
	        linkend="zfs-zfs-allow">delegated</link>.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-filesystem">Filesystem</entry>

	    <entry>A <acronym>ZFS</acronym> dataset is most often used
	      as a file system.  Like most other file systems, a
	      <acronym>ZFS</acronym> file system is mounted somewhere
	      in the systems directory heirarchy and contains files
	      and directories of its own with permissions, flags and
	      other metadata.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-volume">Volume</entry>

	    <entry>In additional to regular file system datasets,
	      <acronym>ZFS</acronym> can also create volumes, which
	      are block devices.  Volumes have many of the same
	      features, including copy-on-write, snapshots, clones and
	      checksumming.  Volumes can be useful for running other
	      file system formats on top of <acronym>ZFS</acronym>,
	      such as <acronym>UFS</acronym> or in the case of
	      Virtualization or exporting <acronym>iSCSI</acronym>
	      extents.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-snapshot">Snapshot</entry>

	    <entry>The <link
		linkend="zfs-term-cow">copy-on-write</link>
		(<acronym>COW</acronym>) design of
	      <acronym>ZFS</acronym> allows for nearly instantaneous
	      consistent snapshots with arbitrary names.  After taking
	      a snapshot of a dataset (or a recursive snapshot of a
	      parent dataset that will include all child datasets),
	      new data is written to new blocks (as described above),
	      however the old blocks are not reclaimed as free space.
	      There are then two versions of the file system, the
	      snapshot (what the file system looked like before) and
	      the live file system; however no additional space is
	      used.  As new data is written to the live file system,
	      new blocks are allocated to store this data.  The
	      apparent size of the snapshot will grow as the blocks
	      are no longer used in the live file system, but only in
	      the snapshot.  These snapshots can be mounted (read
	      only) to allow for the recovery of previous versions of
	      files.  It is also possible to
	      <link linkend="zfs-zfs-snapshot">rollback</link> a live
	      file system to a specific snapshot, undoing any changes
	      that took place after the snapshot was taken.  Each
	      block in the zpool has a reference counter which
	      indicates how many snapshots, clones, datasets or
	      volumes make use of that block.  As files and snapshots
	      are deleted, the reference count is decremented; once a
	      block is no longer referenced, it is reclaimed as free
	      space.  Snapshots can also be marked with a
	      <link linkend="zfs-zfs-snapshot">hold</link>, once a
	      snapshot is held, any attempt to destroy it will return
	      an EBUY error.  Each snapshot can have multiple holds,
	      each with a unique name.  The
	      <link linkend="zfs-zfs-snapshot">release</link> command
	      removes the hold so the snapshot can then be deleted.
	      Snapshots can be taken on volumes, however they can only
	      be cloned or rolled back, not mounted
	      independently.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-clone">Clone</entry>

	    <entry>Snapshots can also be cloned; a clone is a writable
	      version of a snapshot, allowing the file system to be
	      forked as a new dataset.  As with a snapshot, a clone
	      initially consumes no additional space, only as new data
	      is written to a clone and new blocks are allocated does
	      the apparent size of the clone grow.  As blocks are
	      overwritten in the cloned file system or volume, the
	      reference count on the previous block is decremented.
	      The snapshot upon which a clone is based cannot be
	      deleted because the clone is dependeant upon it (the
	      snapshot is the parent, and the clone is the child).
	      Clones can be <literal>promoted</literal>, reversing
	      this dependeancy, making the clone the parent and the
	      previous parent the child.  This operation requires no
	      additional space, however it will change the way the
	      used space is accounted.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-checksum">Checksum</entry>

	    <entry>Every block that is allocated is also checksummed
	      (the algorithm used is a per dataset property, see:
	      <command>zfs set</command>).  <acronym>ZFS</acronym>
	      transparently validates the checksum of each block as it
	      is read, allowing <acronym>ZFS</acronym> to detect
	      silent corruption.  If the data that is read does not
	      match the expected checksum, <acronym>ZFS</acronym> will
	      attempt to recover the data from any available
	      redundancy, like mirrors or <acronym>RAID-Z</acronym>).
	      Validation of all checksums can be triggered with the
	      <link
		linkend="zfs-term-scrub"><command>scrub</command></link>
	      command.  Available checksum algorithms include:

	      <itemizedlist>
		<listitem>
		  <para>fletcher2</para>
		</listitem>

		<listitem>
		  <para>fletcher4</para>
		</listitem>

		<listitem>
		  <para>sha256</para>
		</listitem>
	      </itemizedlist>

	      The fletcher algorithms are faster, but sha256 is a
	      strong cryptographic hash and has a much lower chance of
	      collisions at the cost of some performance.  Checksums
	      can be disabled but it is inadvisable.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-compression">Compression</entry>

	    <entry>Each dataset in <acronym>ZFS</acronym> has a
	      compression property, which defaults to off.  This
	      property can be set to one of a number of compression
	      algorithms, which will cause all new data that is
	      written to this dataset to be compressed as it is
	      written.  In addition to the reduction in disk usage,
	      this can also increase read and write throughput, as
	      only the smaller compressed version of the file needs to
	      be read or written.

	      <note>
		<para><acronym>LZ4</acronym> compression is only
		  available after &os;&nbsp;9.2.</para>
	      </note></entry>
	  </row>

	  <row>
	    <entry id="zfs-term-deduplication">Deduplication</entry>

	    <entry><acronym>ZFS</acronym> has the ability to detect
	      duplicate blocks of data as they are written (thanks to
	      the checksumming feature).  If deduplication is enabled,
	      instead of writing the block a second time, the
	      reference count of the existing block will be increased,
	      saving storage space.  To do this,
	      <acronym>ZFS</acronym> keeps a deduplication table
	      (<acronym>DDT</acronym>) in memory, containing the list
	      of unique checksums, the location of that block and a
	      reference count.  When new data is written, the checksum
	      is calculated and compared to the list.  If a match is
	      found, the data is considered to be a duplicate.  When
	      deduplication is enabled, the checksum algorithm is
	      changed to <acronym>SHA256</acronym> to provide a secure
	      cryptographic hash.  <acronym>ZFS</acronym>
	      deduplication is tunable; if dedup is on, then a
	      matching checksum is assumed to mean that the data is
	      identical.  If dedup is set to verify, then the data in
	      the two blocks will be checked byte-for-byte to ensure
	      it is actually identical and if it is not, the hash
	      collision will be noted by <acronym>ZFS</acronym> and
	      the two blocks will be stored separately.  Due to the
	      nature of the <acronym>DDT</acronym>, having to store
	      the hash of each unique block, it consumes a very large
	      amount of memory (a general rule of thumb is 5-6&nbsp;GB
	      of ram per 1&nbsp;TB of deduplicated data).  In
	      situations where it is not practical to have enough
	      <acronym>RAM</acronym> to keep the entire
	      <acronym>DDT</acronym> in memory, performance will
	      suffer greatly as the <acronym>DDT</acronym> will need
	      to be read from disk before each new block is written.
	      Deduplication can make use of the
	      <acronym>L2ARC</acronym> to store the
	      <acronym>DDT</acronym>, providing a middle ground
	      between fast system memory and slower disks.  Consider
	      using <acronym>ZFS</acronym> compression instead, which
	      often provides nearly as much space savings without the
	      additional memory requirement.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-scrub">Scrub</entry>

	    <entry>In place of a consistency check like &man.fsck.8;,
	      <acronym>ZFS</acronym> has the <literal>scrub</literal>
	      command, which reads all data blocks stored on the pool
	      and verifies their checksums them against the known good
	      checksums stored in the metadata.  This periodic check
	      of all the data stored on the pool ensures the recovery
	      of any corrupted blocks before they are needed.  A scrub
	      is not required after an unclean shutdown, but it is
	      recommended that you run a scrub at least once each
	      quarter.  <acronym>ZFS</acronym> compares the checksum
	      for each block as it is read in the normal course of
	      use, but a scrub operation makes sure even infrequently
	      used blocks are checked for silent corruption.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-quota">Dataset Quota</entry>

	    <entry><acronym>ZFS</acronym> provides very fast and
	      accurate dataset, user and group space accounting in
	      addition to quotas and space reservations.  This gives
	      the administrator fine grained control over how space is
	      allocated and allows critical file systems to reserve
	      space to ensure other file systems do not take all of
	      the free space.

	      <para><acronym>ZFS</acronym> supports different types of
		quotas: the dataset quota, the <link
		  linkend="zfs-term-refquota">reference
		  quota (<acronym>refquota</acronym>)</link>, the
		<link linkend="zfs-term-userquota">user
		  quota</link>, and the
		<link linkend="zfs-term-groupquota">group
		  quota</link>.</para>

	      <para>Quotas limit the amount of space that a dataset
		and all of its descendants (snapshots of the dataset,
		child datasets and the snapshots of those datasets)
		can consume.</para>

	      <note>
		<para>Quotas cannot be set on volumes, as the
		  <literal>volsize</literal> property acts as an
		  implicit quota.</para>
	      </note></entry>
	  </row>

	  <row>
	    <entry id="zfs-term-refquota">Reference
	      Quota</entry>

	    <entry>A reference quota limits the amount of space a
	      dataset can consume by enforcing a hard limit on the
	      space used.  However, this hard limit includes only
	      space that the dataset references and does not include
	      space used by descendants, such as file systems or
	      snapshots.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-userquota">User
	      Quota</entry>

	    <entry>User quotas are useful to limit the amount of space
	      that can be used by the specified user.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-groupquota">Group
	      Quota</entry>

	    <entry>The group quota limits the amount of space that a
	      specified group can consume.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-reservation">Dataset
	      Reservation</entry>

	    <entry>The <literal>reservation</literal> property makes
	      it possible to guaranteed a minimum amount of space for
	      the use of a specific dataset and its descendants.  This
	      means that if a 10&nbsp;GB reservation is set on
	      <filename>storage/home/bob</filename>, if another
	      dataset tries to use all of the free space, at least
	      10&nbsp;GB of space is reserved for this dataset.  If a
	      snapshot is taken of
	      <filename class="directory">storage/home/bob</filename>,
	      the space used by that snapshot is counted against the
	      reservation.  The <link
		linkend="zfs-term-refreservation">refreservation</link>
	      property works in a similar way, except it
	      <emphasis>excludes</emphasis> descendants, such as
	      snapshots.

	      <para>Reservations of any sort are useful in many
		situations, such as planning and testing the
		suitability of disk space allocation in a new system,
		or ensuring that enough space is available on file
		systems for audio logs or system recovery procedures
		and files.</para></entry>
	  </row>

	  <row>
	    <entry id="zfs-term-refreservation">Reference
	      Reservation</entry>

	    <entry>The <literal>refreservation</literal> property
	      makes it possible to guaranteed a minimum amount of
	      space for the use of a specific dataset
	      <emphasis>excluding</emphasis> its descendants.  This
	      means that if a 10&nbsp;GB reservation is set on
	      <filename>storage/home/bob</filename>, if another
	      dataset tries to use all of the free space, at least
	      10&nbsp;GB of space is reserved for this dataset.  In
	      contrast to a regular <link
		linkend="zfs-term-reservation">reservation</link>,
	      space used by snapshots and decendant datasets is not
	      counted against the reservation.  As an example, if a
	      snapshot was taken of
	      <filename>storage/home/bob</filename>, enough disk space
	      would have to exist outside of the
	      <literal>refreservation</literal> amount for the
	      operation to succeed because descendants of the main
	      data set are not counted by the
	      <literal>refreservation</literal> amount and so do not
	      encroach on the space set.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-resilver">Resilver</entry>

	    <entry>When a disk fails and must be replaced, the new
	      disk must be filled with the data that was lost.  This
	      process of calculating and writing the missing data
	      (using the parity information distributed across the
	      remaining drives) to the new drive is called
	      <emphasis>resilvering</emphasis>.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-online">Online</entry>

	    <entry>A ZFS pool or vdev that is in the
	      <literal>Online</literal> state has all of its member
	      devices connected and fully operational.  Individual
	      devices in the <literal>Online</literal> state are
	      functioning normally.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-offline">Offline</entry>

	    <entry>Individual devices can be put in an
	      <literal>Offline</literal> state by the administrator if
	      there is sufficient redundancy to avoid putting the pool
	      or vdev into a <link
	      linkend="zfs-term-faulted">Faulted</link> state.  An
	      administrator may choose to offline a disk in
	      preperation for replacing it, or to make it easier to
	      identify.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-degraded">Degraded</entry>

	    <entry>A ZFS pool or vdev that is in the
	      <literal>Degraded</literal> state has one or more disks
	      that have been disconnected or have failed.  The pool is
	      still usable however if additional devices fail the pool
	      could become unrecoverable.  Reconnecting the missing
	      device(s) or replacing the failed disks will return the
	      pool to a <link
	      linkend="zfs-term-online">Online</link> state after
	      the reconnected or new device has completed the <link
	      linkend="zfs-term-resilver">Resilver</link>
	      process.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-faulted">Faulted</entry>

	    <entry>A ZFS pool or vdev that is in the
	      <literal>Faulted</literal> state is no longer
	      operational and the data residing on it can no longer
	      be accessed.  A pool or vdev enters the
	      <literal>Faulted</literal> state when the number of
	      missing or failed devices exceeds the level of
	      redundancy in the vdev.  If missing devices can be
	      reconnected the pool will return to a <link
	      linkend="zfs-term-online">Online</link> state.  If
	      there is insufficient redundancy to compensate for the
	      number of failed disks, then the contents of the pool
	      are lost and will need to be restored from
	      backups.</entry>
	  </row>
	</tbody>
      </tgroup>
    </informaltable>
  </sect1>
</chapter>
