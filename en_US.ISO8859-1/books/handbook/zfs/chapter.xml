<?xml version="1.0" encoding="iso-8859-1"?>
<!--
     The FreeBSD Documentation Project
     $FreeBSD$
-->

<chapter id="zfs">
  <chapterinfo>
    <authorgroup>
      <author>
	<firstname>Tom</firstname>
	<surname>Rhodes</surname>
	<contrib>Written by </contrib>
      </author>
    </authorgroup>
  </chapterinfo>

  <title>The Z File System (<acronym>ZFS</acronym>)</title>

  <para>The <emphasis>Z File System</emphasis>
    (<acronym>ZFS</acronym>) was developed at &sun; to address many of
    the problems with current file systems.  There were three major
    design goals:</para>

  <itemizedlist>
    <listitem>
      <para>Data integrity: checksums are created when data is written
	and checked when data is read.  If on-disk data corruption is
	detected, the user is notified and recovery methods are
	initiated.</para>
    </listitem>

    <listitem>
      <para>Pooled storage: physical storage devices are added to a
	pool, and storage space is allocated from that shared pool.
	Space is available to all file systems, and can be increased
	by adding new storage devices to the pool.</para>
    </listitem>

    <listitem>
      <para>Performance:</para>
    </listitem>
  </itemizedlist>

  <para>A complete list of <acronym>ZFS</acronym> features and
    terminology is shown in <xref linkend="zfs-term"/>.</para>

  <sect1 id="zfs-differences">
    <title>What Makes <acronym>ZFS</acronym> Different</title>

    <para><acronym>ZFS</acronym> is significantly different from any
      previous file system owing to the fact that it is more than just
      a file system.  <acronym>ZFS</acronym> combines the
      traditionally separate roles of volume manager and file system,
      which provides unique advantages because the file system is now
      aware of the underlying structure of the disks.  Traditional
      file systems could only be created on a single disk at a time,
      if there were two disks then two separate file systems would
      have to be created.  In a traditional hardware
      <acronym>RAID</acronym> configuration, this problem was worked
      around by presenting the operating system with a single logical
      disk made up of the space provided by a number of disks, on top
      of which the operating system placed its file system.  Even in
      the case of software <acronym>RAID</acronym> solutions like
      <acronym>GEOM</acronym>, the <acronym>UFS</acronym> file system
      living on top of the <acronym>RAID</acronym> transform believed
      that it was dealing with a single device.
      <acronym>ZFS</acronym>'s combination of the volume manager and
      the file system solves this and allows the creation of many file
      systems all sharing a pool of available storage.  One of the
      biggest advantages to <acronym>ZFS</acronym>'s awareness of the
      physical layout of the disks is that <acronym>ZFS</acronym> can
      grow the existing file systems automatically when additional
      disks are added to the pool.  This new space is then made
      available to all of the file systems.  <acronym>ZFS</acronym>
      also has a number of different properties that can be applied to
      each file system, creating many advantages to creating a number
      of different filesystems and datasets rather than a single
      monolithic filesystem.</para>
  </sect1>

  <sect1 id="zfs-quickstart">
    <title><acronym>ZFS</acronym> Quick Start Guide</title>

    <para>There is a start up mechanism that allows &os; to mount
      <acronym>ZFS</acronym> pools during system initialization.  To
      enable it, add this line to
      <filename>/etc/rc.conf</filename>:</para>

    <programlisting>zfs_enable="YES"</programlisting>

    <para>Then start the service:</para>

    <screen>&prompt.root; <userinput>service zfs start</userinput></screen>

    <para>The examples in this section assume three
      <acronym>SCSI</acronym> disks with the device names
      <devicename><replaceable>da0</replaceable></devicename>,
      <devicename><replaceable>da1</replaceable></devicename>, and
      <devicename><replaceable>da2</replaceable></devicename>.  Users
      of <acronym>SATA</acronym> hardware should instead use
      <devicename><replaceable>ada</replaceable></devicename> device
      names.</para>

    <sect2>
      <title>Single Disk Pool</title>

      <para>To create a simple, non-redundant <acronym>ZFS</acronym>
	pool using a single disk device, use
	<command>zpool</command>:</para>

      <screen>&prompt.root; <userinput>zpool create <replaceable>example</replaceable> <replaceable>/dev/da0</replaceable></userinput></screen>

      <para>To view the new pool, review the output of
	<command>df</command>:</para>

      <screen>&prompt.root; <userinput>df</userinput>
Filesystem  1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a   2026030  235230  1628718    13%    /
devfs               1       1        0   100%    /dev
/dev/ad0s1d  54098308 1032846 48737598     2%    /usr
example      17547136       0 17547136     0%    /example</screen>

      <para>This output shows that the <literal>example</literal> pool
	has been created and <emphasis>mounted</emphasis>.  It is now
	accessible as a file system.  Files may be created on it and
	users can browse it, as seen in the following example:</para>

      <screen>&prompt.root; <userinput>cd /example</userinput>
&prompt.root; <userinput>ls</userinput>
&prompt.root; <userinput>touch testfile</userinput>
&prompt.root; <userinput>ls -al</userinput>
total 4
drwxr-xr-x   2 root  wheel    3 Aug 29 23:15 .
drwxr-xr-x  21 root  wheel  512 Aug 29 23:12 ..
-rw-r--r--   1 root  wheel    0 Aug 29 23:15 testfile</screen>

      <para>However, this pool is not taking advantage of any
	<acronym>ZFS</acronym> features.  To create a dataset on this
	pool with compression enabled:</para>

      <screen>&prompt.root; <userinput>zfs create example/compressed</userinput>
&prompt.root; <userinput>zfs set compression=gzip example/compressed</userinput></screen>

      <para>The <literal>example/compressed</literal> dataset is now a
	<acronym>ZFS</acronym> compressed file system.  Try copying
	some large files to <filename
	  class="directory">/example/compressed</filename>.</para>

      <para>Compression can be disabled with:</para>

      <screen>&prompt.root; <userinput>zfs set compression=off example/compressed</userinput></screen>

      <para>To unmount a file system, use
	<command>zfs umount</command> and then verify by using
	<command>df</command>:</para>

      <screen>&prompt.root; <userinput>zfs umount example/compressed</userinput>
&prompt.root; <userinput>df</userinput>
Filesystem  1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a   2026030  235232  1628716    13%    /
devfs               1       1        0   100%    /dev
/dev/ad0s1d  54098308 1032864 48737580     2%    /usr
example      17547008       0 17547008     0%    /example</screen>

      <para>To re-mount the file system to make it accessible again,
	use <command>zfs mount</command> and verify with
	<command>df</command>:</para>

      <screen>&prompt.root; <userinput>zfs mount example/compressed</userinput>
&prompt.root; <userinput>df</userinput>
Filesystem         1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a          2026030  235234  1628714    13%    /
devfs                      1       1        0   100%    /dev
/dev/ad0s1d         54098308 1032864 48737580     2%    /usr
example             17547008       0 17547008     0%    /example
example/compressed  17547008       0 17547008     0%    /example/compressed</screen>

      <para>The pool and file system may also be observed by viewing
	the output from <command>mount</command>:</para>

      <screen>&prompt.root; <userinput>mount</userinput>
/dev/ad0s1a on / (ufs, local)
devfs on /dev (devfs, local)
/dev/ad0s1d on /usr (ufs, local, soft-updates)
example on /example (zfs, local)
example/data on /example/data (zfs, local)
example/compressed on /example/compressed (zfs, local)</screen>

      <para><acronym>ZFS</acronym> datasets, after creation, may be
	used like any file systems.  However, many other features are
	available which can be set on a per-dataset basis.  In the
	following example, a new file system, <literal>data</literal>
	is created.  Important files will be stored here, the file
	system is set to keep two copies of each data block:</para>

      <screen>&prompt.root; <userinput>zfs create example/data</userinput>
&prompt.root; <userinput>zfs set copies=2 example/data</userinput></screen>

      <para>It is now possible to see the data and space utilization
	by issuing <command>df</command>:</para>

      <screen>&prompt.root; <userinput>df</userinput>
Filesystem         1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a          2026030  235234  1628714    13%    /
devfs                      1       1        0   100%    /dev
/dev/ad0s1d         54098308 1032864 48737580     2%    /usr
example             17547008       0 17547008     0%    /example
example/compressed  17547008       0 17547008     0%    /example/compressed
example/data        17547008       0 17547008     0%    /example/data</screen>

      <para>Notice that each file system on the pool has the same
	amount of available space.  This is the reason for using
	<command>df</command> in these examples, to show that the file
	systems use only the amount of space they need and all draw
	from the same pool.  The <acronym>ZFS</acronym> file system
	does away with concepts such as volumes and partitions, and
	allows for several file systems to occupy the same
	pool.</para>

      <para>To destroy the file systems and then destroy the pool as
	they are no longer needed:</para>

      <screen>&prompt.root; <userinput>zfs destroy example/compressed</userinput>
&prompt.root; <userinput>zfs destroy example/data</userinput>
&prompt.root; <userinput>zpool destroy example</userinput></screen>
    </sect2>

    <sect2>
      <title><acronym>ZFS</acronym> RAID-Z</title>

      <para>There is no way to prevent a disk from failing.  One
	method of avoiding data loss due to a failed hard disk is to
	implement <acronym>RAID</acronym>.  <acronym>ZFS</acronym>
	supports this feature in its pool design.
	<acronym>RAID-Z</acronym> pools require 3 or more disks but
	yield more usable space than mirrored pools.</para>

      <para>To create a <acronym>RAID-Z</acronym> pool, issue the
	following command and specify the disks to add to the
	pool:</para>

      <screen>&prompt.root; <userinput>zpool create storage raidz da0 da1 da2</userinput></screen>

      <note>
	<para>&sun; recommends that the number of devices used in a
	  <acronym>RAID</acronym>-Z configuration is between three and
	  nine.  For environments requiring a single pool consisting
	  of 10 disks or more, consider breaking it up into smaller
	  <acronym>RAID-Z</acronym> groups.  If only two disks are
	  available and redundancy is a requirement, consider using a
	  <acronym>ZFS</acronym> mirror.  Refer to &man.zpool.8; for
	  more details.</para>
      </note>

      <para>This command creates the <literal>storage</literal> zpool.
	This may be verified using &man.mount.8; and &man.df.1;.  This
	command makes a new file system in the pool called
	<literal>home</literal>:</para>

      <screen>&prompt.root; <userinput>zfs create storage/home</userinput></screen>

      <para>It is now possible to enable compression and keep extra
	copies of directories and files using the following
	commands:</para>

      <screen>&prompt.root; <userinput>zfs set copies=2 storage/home</userinput>
&prompt.root; <userinput>zfs set compression=gzip storage/home</userinput></screen>

      <para>To make this the new home directory for users, copy the
	user data to this directory, and create the appropriate
	symbolic links:</para>

      <screen>&prompt.root; <userinput>cp -rp /home/* /storage/home</userinput>
&prompt.root; <userinput>rm -rf /home /usr/home</userinput>
&prompt.root; <userinput>ln -s /storage/home /home</userinput>
&prompt.root; <userinput>ln -s /storage/home /usr/home</userinput></screen>

      <para>Users should now have their data stored on the freshly
	created <filename class="directory">/storage/home</filename>.
	Test by adding a new user and logging in as that user.</para>

      <para>Try creating a snapshot which may be rolled back
	later:</para>

      <screen>&prompt.root; <userinput>zfs snapshot storage/home@08-30-08</userinput></screen>

      <para>Note that the snapshot option will only capture a real
	file system, not a home directory or a file.  The
	<literal>@</literal> character is a delimiter used between the
	file system name or the volume name.  When a user's home
	directory gets trashed, restore it with:</para>

      <screen>&prompt.root; <userinput>zfs rollback storage/home@08-30-08</userinput></screen>

      <para>To get a list of all available snapshots, run
	<command>ls</command> in the file system's
	<filename class="directory">.zfs/snapshot</filename>
	directory.  For example, to see the previously taken
	snapshot:</para>

      <screen>&prompt.root; <userinput>ls /storage/home/.zfs/snapshot</userinput></screen>

      <para>It is possible to write a script to perform regular
	snapshots on user data.  However, over time, snapshots may
	consume a great deal of disk space.  The previous snapshot may
	be removed using the following command:</para>

      <screen>&prompt.root; <userinput>zfs destroy storage/home@08-30-08</userinput></screen>

      <para>After testing,
	<filename class="directory">/storage/home</filename> can be
	made the real <filename class="directory">/home</filename>
	using this command:</para>

      <screen>&prompt.root; <userinput>zfs set mountpoint=/home storage/home</userinput></screen>

      <para>Run <command>df</command> and <command>mount</command> to
	confirm that the system now treats the file system as the real
	<filename class="directory">/home</filename>:</para>

      <screen>&prompt.root; <userinput>mount</userinput>
/dev/ad0s1a on / (ufs, local)
devfs on /dev (devfs, local)
/dev/ad0s1d on /usr (ufs, local, soft-updates)
storage on /storage (zfs, local)
storage/home on /home (zfs, local)
&prompt.root; <userinput>df</userinput>
Filesystem   1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a    2026030  235240  1628708    13%    /
devfs                1       1        0   100%    /dev
/dev/ad0s1d   54098308 1032826 48737618     2%    /usr
storage       26320512       0 26320512     0%    /storage
storage/home  26320512       0 26320512     0%    /home</screen>

      <para>This completes the <acronym>RAID-Z</acronym>
	configuration.  To get status updates about the file systems
	created during the nightly &man.periodic.8; runs, issue the
	following command:</para>

      <screen>&prompt.root; <userinput>echo 'daily_status_zfs_enable="YES"' &gt;&gt; /etc/periodic.conf</userinput></screen>
    </sect2>

    <sect2>
      <title>Recovering <acronym>RAID</acronym>-Z</title>

      <para>Every software <acronym>RAID</acronym> has a method of
	monitoring its <literal>state</literal>.  The status of
	<acronym>RAID-Z</acronym> devices may be viewed with this
	command:</para>

      <screen>&prompt.root; <userinput>zpool status -x</userinput></screen>

      <para>If all pools are healthy and everything is normal, the
	following message will be returned:</para>

      <screen>all pools are healthy</screen>

      <para>If there is an issue, perhaps a disk has gone offline,
	the pool state will look similar to:</para>

      <screen>  pool: storage
 state: DEGRADED
status: One or more devices has been taken offline by the administrator.
	Sufficient replicas exist for the pool to continue functioning in a
	degraded state.
action: Online the device using 'zpool online' or replace the device with
	'zpool replace'.
 scrub: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	storage     DEGRADED     0     0     0
	  raidz1    DEGRADED     0     0     0
	    da0     ONLINE       0     0     0
	    da1     OFFLINE      0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</screen>

      <para>This indicates that the device was previously taken
	offline by the administrator using the following
	command:</para>

      <screen>&prompt.root; <userinput>zpool offline storage da1</userinput></screen>

      <para>It is now possible to replace <devicename>da1</devicename>
	after the system has been powered down.  When the system is
	back online, the following command may issued to replace the
	disk:</para>

      <screen>&prompt.root; <userinput>zpool replace storage da1</userinput></screen>

      <para>From here, the status may be checked again, this time
	without the <option>-x</option> flag to get state
	information:</para>

      <screen>&prompt.root; <userinput>zpool status storage</userinput>
 pool: storage
 state: ONLINE
 scrub: resilver completed with 0 errors on Sat Aug 30 19:44:11 2008
config:

	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    da0     ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</screen>

      <para>As shown from this example, everything appears to be
	normal.</para>
    </sect2>

    <sect2>
      <title>Data Verification</title>

      <para><acronym>ZFS</acronym> uses checksums to verify the
	integrity of stored data.  These are enabled automatically
	upon creation of file systems and may be disabled using the
	following command:</para>

      <screen>&prompt.root; <userinput>zfs set checksum=off storage/home</userinput></screen>

      <para>Doing so is <emphasis>not</emphasis> recommended as
	checksums take very little storage space and are used to check
	data integrity using checksum verification in a process is
	known as <quote>scrubbing.</quote> To verify the data
	integrity of the <literal>storage</literal> pool, issue this
	command:</para>

      <screen>&prompt.root; <userinput>zpool scrub storage</userinput></screen>

      <para>This process may take considerable time depending on the
	amount of data stored.  It is also very <acronym>I/O</acronym>
	intensive, so much so that only one scrub may be run at any
	given time.  After the scrub has completed, the status is
	updated and may be viewed by issuing a status request:</para>

      <screen>&prompt.root; <userinput>zpool status storage</userinput>
 pool: storage
 state: ONLINE
 scrub: scrub completed with 0 errors on Sat Jan 26 19:57:37 2013
config:

	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    da0     ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</screen>

      <para>The completion time is displayed and helps to ensure data
	integrity over a long period of time.</para>

      <para>Refer to &man.zfs.8; and &man.zpool.8; for other
	<acronym>ZFS</acronym> options.</para>
    </sect2>
  </sect1>

  <sect1 id="zfs-zpool">
    <title><command>zpool</command> Administration</title>

    <para></para>

    <sect2 id="zfs-zpool-create">
      <title>Creating &amp; Destroying Storage Pools</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-zpool-attach">
      <title>Adding &amp; Removing Devices</title>

      <para>Creating a ZFS Storage Pool (<acronym>zpool</acronym>)
	involves making a number of decisions that are relatively
	permanent.  Although additional vdevs can be added to a pool,
	the layout of the pool cannot be changed once the pool has
	been created, instead the data must be backed up and the pool
	recreated.  Currently, devices cannot be removed from a
	zpool.</para>
    </sect2>

    <sect2 id="zfs-zpool-resilver">
      <title>Dealing with Failed Devices</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-zpool-import">
      <title>Importing &amp; Exporting Pools</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-zpool-upgrade">
      <title>Upgrading a Storage Pool</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-zpool-status">
      <title>Checking the Status of a Pool</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-zpool-iostat">
      <title>Performance Monitoring</title>

      <para>ZFS has a built-in monitoring isystem that can display
	statistics about I/O happening on the pool in real-time.
	Additionally, it shows the free and used space on the pool and
	how much I/O bandwidth are currently utilized for read and
	write operations.  By default, all pools in the system will be
	monitored and displayed.  A pool name can be provided to just
	monitor one pool.  A basic example is provided below:</para>

<screen>&prompt.root; <userinput>zpool iostat</userinput>
               capacity     operations    bandwidth
pool        alloc   free   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
data         288G  1.53T      2     11  11.3K  57.1K</screen>

	<para>To monitor I/O activity on the pool continuously, a
	  number indicating the seconds after which to refresh the
	  display can be specified.  ZFS will then print the next
	  statistic line after each intervall has been reached.  Press
	  <keycombo
	  action="simul"><keycap>Ctrl</keycap><keycap>C</keycap></keycombo>
	  to stop this continuous monitoring.  Alternatively, a second
	  whole number can be provided on the command line after the
	  intervall to indicate how many of these statistics should be
	  displayed in total.</para>

	<para>An even more detailed pool I/O statistic can be
	  displayed using the <literal>-v</literal> parameter.  For
	  each storage device that is part of the pool ZFS will
	  provide a separate statistic line.  This is helpful to
	  determine reads and writes on devices that slow down I/O on
	  the whole pool.  In the following example, we have a
	  mirrored pool consisting of two devices.  For each of these,
	  a separate line is shown with the current I/O
	  activity.</para>

<screen>&prompt.root; <userinput>zpool iostat -v </userinput>
                            capacity     operations    bandwidth
pool                     alloc   free   read  write   read  write
-----------------------  -----  -----  -----  -----  -----  -----
data                      288G  1.53T      2     12  9.23K  61.5K
  mirror                  288G  1.53T      2     12  9.23K  61.5K
    ada1                     -      -      0      4  5.61K  61.7K
    ada2                     -      -      1      4  5.04K  61.7K
-----------------------  -----  -----  -----  -----  -----  -----</screen>
</sect2>

    <sect2 id="zfs-zpool-split">
      <title>Splitting a Storage Pool</title>

      <para></para>
    </sect2>
  </sect1>

  <sect1 id="zfs-zfs">
    <title><command>zfs</command> Administration</title>

    <para></para>

    <sect2 id="zfs-zfs-create">
      <title>Creating &amp; Destroying Datasets</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-zfs-volume">
      <title>Creating &amp; Destroying Volumes</title>

      <para></para>

      <para>A volume can be formatted with any filesystem on top of
	it.  This will appear to the user as if they are working with
	a regular disk using that specific filesystem and not ZFS.
	In this way, non-ZFS file systems can be augmented with
	ZFS features that they would not normally have.  For example,
	combining the ZFS compression property together with a
	250&nbsp;MB volume allows to create a compressed FAT
	filesystem.</para>

      <screen>&prompt.root; <userinput>zfs create -V 250m -o compression=on tank/fat32</userinput>
&prompt.root; <userinput>zfs list tank</userinput>
NAME USED AVAIL REFER MOUNTPOINT
tank 258M  670M   31K /tank
&prompt.root; <userinput>newfs_msdos -F32 /dev/zvol/tank/fat32</userinput>
&prompt.root; <userinput>mount -t msdosfs /dev/zvol/tank/fat32 /mnt</userinput>
&prompt.root; <userinput>df -h /mnt | grep fat32</userinput>
Filesystem           Size Used Avail Capacity Mounted on
/dev/zvol/tank/fat32 249M  24k  249M     0%   /mnt
&prompt.root; <userinput>mount | grep fat32</userinput>
/dev/zvol/tank/fat32 on /mnt (msdosfs, local)</screen>
    </sect2>

    <sect2 id="zfs-zfs-rename">
      <title>Renaming a Dataset</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-zfs-set">
      <title>Setting Dataset Properties</title>

      <para></para>

      <para>It is possible to set user-defined properties in ZFS.
	They become part of the dataset configuration and can be used
	to provide additional information about the dataset or its
	contents.  To distinguish these custom properties from the
	ones supplied as part of ZFS, a colon (<literal>:</literal>)
	is used to create a custom namespace for the property.</para>

      <screen>&prompt.root; <userinput>zfs set custom:costcenter=1234</userinput>
&prompt.root; <userinput>zfs get custom:costcenter</userinput>
NAME PROPERTY           VALUE SOURCE
tank custom:costcenter  1234  local</screen>
    </sect2>

    <sect2 id="zfs-zfs-snapshot">
      <title>Managing Snapshots</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-zfs-clones">
      <title>Managing Clones</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-zfs-send">
      <title>ZFS Replication</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-zfs-quota">
      <title>Dataset, User and Group Quotas</title>

      <para>To enforce a dataset quota of 10&nbsp;GB for
	<filename>storage/home/bob</filename>, use the
	following:</para>

      <screen>&prompt.root; <userinput>zfs set quota=10G storage/home/bob</userinput></screen>

      <para>To enforce a reference quota of 10&nbsp;GB for
	<filename>storage/home/bob</filename>, use the
	following:</para>

      <screen>&prompt.root; <userinput>zfs set refquota=10G storage/home/bob</userinput></screen>

      <para>The general format is
	<literal>userquota@<replaceable>user</replaceable>=<replaceable>size</replaceable></literal>,
	and the user's name must be in one of the following
	formats:</para>

      <itemizedlist>
	<listitem>
	  <para><acronym>POSIX</acronym> compatible name such as
	    <replaceable>joe</replaceable>.</para>
	</listitem>

	<listitem>
	  <para><acronym>POSIX</acronym> numeric ID such as
	    <replaceable>789</replaceable>.</para>
	</listitem>

	<listitem>
	  <para><acronym>SID</acronym> name
	    such as
	    <replaceable>joe.bloggs@example.com</replaceable>.</para>
	</listitem>

	<listitem>
	  <para><acronym>SID</acronym>
	    numeric ID such as
	    <replaceable>S-1-123-456-789</replaceable>.</para>
	</listitem>
      </itemizedlist>

      <para>For example, to enforce a user quota of 50&nbsp;GB for a
	user named <replaceable>joe</replaceable>, use the
	following:</para>

      <screen>&prompt.root; <userinput>zfs set userquota@joe=50G</userinput></screen>

      <para>To remove the quota or make sure that one is not set,
	instead use:</para>

      <screen>&prompt.root; <userinput>zfs set userquota@joe=none</userinput></screen>

      <note>
	<para>User quota properties are not displayed by
	  <command>zfs get all</command>.
	  Non-<username>root</username> users can only see their own
	  quotas unless they have been granted the
	  <literal>userquota</literal> privilege.  Users with this
	  privilege are able to view and set everyone's quota.</para>
      </note>

      <para>The general format for setting a group quota is:
	<literal>groupquota@<replaceable>group</replaceable>=<replaceable>size</replaceable></literal>.</para>

      <para>To set the quota for the group
	<replaceable>firstgroup</replaceable> to 50&nbsp;GB,
	use:</para>

      <screen>&prompt.root; <userinput>zfs set groupquota@firstgroup=50G</userinput></screen>

      <para>To remove the quota for the group
	<replaceable>firstgroup</replaceable>, or to make sure that
	one is not set, instead use:</para>

      <screen>&prompt.root; <userinput>zfs set groupquota@firstgroup=none</userinput></screen>

      <para>As with the user quota property,
	non-<username>root</username> users can only see the quotas
	associated with the groups that they belong to.  However,
	<username>root</username> or a user with the
	<literal>groupquota</literal> privilege can view and set all
	quotas for all groups.</para>

      <para>To display the amount of space consumed by each user on
	the specified filesystem or snapshot, along with any specified
	quotas, use <command>zfs userspace</command>.  For group
	information, use <command>zfs groupspace</command>.  For more
	information about supported options or how to display only
	specific options, refer to &man.zfs.1;.</para>

      <para>Users with sufficient privileges and
	<username>root</username> can list the quota for
	<filename>storage/home/bob</filename> using:</para>

      <screen>&prompt.root; <userinput>zfs get quota storage/home/bob</userinput></screen>
    </sect2>

    <sect2 id="zfs-zfs-reservation">
      <title>Reservations</title>

      <para></para>

      <para>The general format of the <literal>reservation</literal>
	property is
	<literal>reservation=<replaceable>size</replaceable></literal>,
	so to set a reservation of 10&nbsp;GB on
	<filename>storage/home/bob</filename>, use:</para>

      <screen>&prompt.root; <userinput>zfs set reservation=10G storage/home/bob</userinput></screen>

      <para>To make sure that no reservation is set, or to remove a
	reservation, use:</para>

      <screen>&prompt.root; <userinput>zfs set reservation=none storage/home/bob</userinput></screen>

      <para>The same principle can be applied to the
	<literal>refreservation</literal> property for setting a
	refreservation, with the general format
	<literal>refreservation=<replaceable>size</replaceable></literal>.</para>

      <para>To check if any reservations or refreservations exist on
	<filename>storage/home/bob</filename>, execute one of the
	following commands:</para>

      <screen>&prompt.root; <userinput>zfs get reservation storage/home/bob</userinput>
&prompt.root; <userinput>zfs get refreservation storage/home/bob</userinput></screen>
    </sect2>

    <sect2 id="zfs-zfs-compression">
      <title>Compression</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-zfs-deduplication">
      <title>Deduplication</title>

      <para></para>

      <para>To activate deduplication, you simply need to set the
	following property on the target pool.</para>

      <screen>&prompt.root; <userinput>zfs set dedup=on <replaceable>pool</replaceable></userinput></screen>

      <para>it is important to mention that only new data being
	written to the pool will be deduplicated.  Data that is
	already residing on the pool will not be deduplicated by
	activating this option.  As such, a pool with a freshly
	activated deduplication property will look something like this
	example.</para>

      <screen>&prompt.root; <userinput>zpool list</userinput>
NAME  SIZE ALLOC  FREE CAP DEDUP HEALTH ALTROOT
pool 2.84G 2.19M 2.83G  0% 1.00x ONLINE -</screen>

      <para>The <literal>DEDUP</literal> column shows the actual rate
	of deduplication for that pool.  A value of
	<literal>1.00x</literal> that no data has been deduplicated
	due to insufficient duplicate data.  In the following example,
	the ports tree is copied three times into different
	directories on the deduplicated pool above to provide
	redundancy.</para>

      <screen>&prompt.root; <userinput>zpool list</userinput>
for d in dir1 dir2 dir3; do
for> mkdir $d &amp;&amp; cp -R /usr/ports $d &amp;
for> done</screen>

      <para>Now that redundant data has been created, ZFS detects that
	and makes sure that the data is not taking up additional
	space.</para>

      <screen>&prompt.root; <userinput>zpool list</userinput>
NAME SIZE  ALLOC FREE CAP DEDUP HEALTH ALTROOT
pool 2.84G 20.9M 2.82G 0% 3.00x ONLINE -</screen>

      <para>The <literal>DEDUP</literal> column does now contain the
	value <literal>3.00x</literal>. This indicates that ZFS
	detected the copies of the ports tree data and was able to
	deduplicate it at a ratio of 1/3.  The space savings that this
	yields can be enormous, but only when there is enough memory
	available to keep track of the deduplicated blocks.</para>

      <para>Deduplication is not always beneficial, especially when
	there is not much redundant data on a ZFS pool.  To see how
	much space could be saved by deduplication for a given set of
	data that is already stored in a pool, ZFS can simulate the
	effects that deduplication would have.  To do that, the
	following command can be invoked on the pool.</para>

      <screen>&prompt.root; <userinput>zdb -S <replaceable>pool</replaceable></userinput>
Simulated DDT histogram:

bucket              allocated                       referenced
______   ______________________________   ______________________________
refcnt   blocks   LSIZE   PSIZE   DSIZE   blocks   LSIZE   PSIZE   DSIZE
------   ------   -----   -----   -----   ------   -----   -----   -----
     1    2.58M    289G    264G    264G    2.58M    289G    264G    264G
     2     206K   12.6G   10.4G   10.4G     430K   26.4G   21.6G   21.6G
     4    37.6K    692M    276M    276M     170K   3.04G   1.26G   1.26G
     8    2.18K   45.2M   19.4M   19.4M    20.0K    425M    176M    176M
    16      174   2.83M   1.20M   1.20M    3.33K   48.4M   20.4M   20.4M
    32       40   2.17M    222K    222K    1.70K   97.2M   9.91M   9.91M
    64        9     56K   10.5K   10.5K      865   4.96M    948K    948K
   128        2   9.50K      2K      2K      419   2.11M    438K    438K
   256        5   61.5K     12K     12K    1.90K   23.0M   4.47M   4.47M
    1K        2      1K      1K      1K    2.98K   1.49M   1.49M   1.49M
 Total    2.82M    303G    275G    275G    3.20M    319G    287G    287G

dedup = 1.05, compress = 1.11, copies = 1.00, dedup * compress / copies = 1.16</screen>

      <para>After <command>zdb -S</command> finished analyzing the
	pool, it outputs a summary that shows the ratio that would
	result in activating deduplication.  In this case,
	<literal>1.16</literal> is a very poor rate that is mostly
	influenced by compression.  Activating deduplication on this
	pool would not save any significant amount of space.  Keeping
	the formula <literal>dedup * compress / copies = deduplication
	ratio</literal> in mind, a system administrator can plan the
	storage allocation more towards having multiple copies of data
	or by having a decent compression rate in order to utilize the
	space savings that deduplication provides.  As a rule of
	thumb, compression should be used first before deduplication
	due to the lower memory requirements.</para>
    </sect2>
  </sect1>

  <sect1 id="zfs-zfs-allow">
    <title>Delegated Administration</title>

    <para>ZFS features a comprehensive delegation system to assign
      permissions to performs the various ZFS administration functions
      to a regular user.  For example, if each users' home directory
      is a dataset, then each user could be delegated permission to
      create and destroy snapshots of their home directory.  A backup
      user could be assigned the permissions required to make use of
      the ZFS replication features without requiring root access, or
      isolate a usage collection script to run as an unprivledged user
      with access to only the space utilization data of all users.  It
      is even possible to delegate the ability to delegate
      permissions.  It is possible to delegate permissions over each
      ZFS subcommand and most ZFS properties.</para>

    <sect2 id="zfs-zfs-allow-create">
      <title>Delegating Dataset Creation</title>

      <para>Using the <userinput>zfs allow
	<replaceable>someuser</replaceable> create
	<replaceable>mydataset</replaceable></userinput> command will
	give the indicated user the required permissions to create
	child datasets under the selected parent dataset.  There is a
	caveat, creating a new dataset involves mouting it, which
	requires the <literal>vfs.usermount</literal> sysctl be
	enabled in order to allow non-root users to mount a
	filesystem.  There is the further restriction that non-root
	users must own the directory they are mounting the filesystem
	to, in order to prevent abuse.</para>
    </sect2>

    <sect2 id="zfs-zfs-allow-allow">
      <title>Delegating Permission Delegation</title>

      <para>Using the <userinput>zfs allow
	<replaceable>someuser</replaceable> allow
	<replaceable>mydataset</replaceable></userinput> command will
	give the indicated user the ability to assign any permission
	they have on the target dataset (or its children) to other
	users.  If a user has the <literal>snapshot</literal>
	permission and the <literal>allow</literal> permission that
	user can then grant the snapshot permission to some other
	users.</para>
    </sect2>
  </sect1>

  <sect1 id="zfs-advanced">
    <title>ZFS Advanced Topics</title>

    <sect2 id="zfs-advanced-tuning">
      <title>ZFS Tuning</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-advanced-booting">
      <title>Booting Root on ZFS</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-advanced-beadm">
      <title>ZFS Boot Environments</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-advanced-troubleshoot">
      <title>Troubleshooting</title>

      <para></para>
    </sect2>

    <sect2 id="zfs-advanced-i386">
      <title>ZFS on i386</title>

      <para>Some of the features provided by <acronym>ZFS</acronym>
	are RAM-intensive, so some tuning may be required to provide
	maximum efficiency on systems with limited
	<acronym>RAM</acronym>.</para>

      <sect3>
	<title>Memory</title>

	<para>At a bare minimum, the total system memory should be at
	  least one gigabyte.  The amount of recommended
	  <acronym>RAM</acronym> depends upon the size of the pool and
	  the <acronym>ZFS</acronym> features which are used.  A
	  general rule of thumb is 1&nbsp;GB of RAM for every
	  1&nbsp;TB of storage.  If the deduplication feature is used,
	  a general rule of thumb is 5&nbsp;GB of RAM per TB of
	  storage to be deduplicated.  While some users successfully
	  use <acronym>ZFS</acronym> with less <acronym>RAM</acronym>,
	  it is possible that when the system is under heavy load, it
	  may panic due to memory exhaustion.  Further tuning may be
	  required for systems with less than the recommended RAM
	  requirements.</para>
      </sect3>

      <sect3>
	<title>Kernel Configuration</title>

	<para>Due to the <acronym>RAM</acronym> limitations of the
	  &i386; platform, users using <acronym>ZFS</acronym> on the
	  &i386; architecture should add the following option to a
	  custom kernel configuration file, rebuild the kernel, and
	  reboot:</para>

	<programlisting>options        KVA_PAGES=512</programlisting>

	<para>This option expands the kernel address space, allowing
	  the <varname>vm.kvm_size</varname> tunable to be pushed
	  beyond the currently imposed limit of 1&nbsp;GB, or the
	  limit of 2&nbsp;GB for <acronym>PAE</acronym>.  To find the
	  most suitable value for this option, divide the desired
	  address space in megabytes by four (4).  In this example, it
	  is <literal>512</literal> for 2&nbsp;GB.</para>
      </sect3>

      <sect3>
	<title>Loader Tunables</title>

	<para>The <devicename>kmem</devicename> address space can be
	  increased on all &os; architectures.  On a test system with
	  one gigabyte of physical memory, success was achieved with
	  the following options added to
	  <filename>/boot/loader.conf</filename>, and the system
	  restarted:</para>

	<programlisting>vm.kmem_size="330M"
vm.kmem_size_max="330M"
vfs.zfs.arc_max="40M"
vfs.zfs.vdev.cache.size="5M"</programlisting>

	<para>For a more detailed list of recommendations for
	  <acronym>ZFS</acronym>-related tuning, see <ulink
	    url="http://wiki.freebsd.org/ZFSTuningGuide"></ulink>.</para>
      </sect3>
    </sect2>
  </sect1>

  <sect1 id="zfs-links">
    <title>Additional Resources</title>

    <itemizedlist>
      <listitem>
	<para><ulink url="https://wiki.freebsd.org/ZFS">FreeBSD Wiki -
	    ZFS</ulink></para>
      </listitem>

      <listitem>
	<para><ulink
	    url="https://wiki.freebsd.org/ZFSTuningGuide">FreeBSD Wiki
	    - ZFS Tuning</ulink></para>
      </listitem>

      <listitem>
	<para><ulink
	    url="http://wiki.illumos.org/display/illumos/ZFS">Illumos
	    Wiki - ZFS</ulink></para>
      </listitem>

      <listitem>
	<para><ulink
	    url="http://docs.oracle.com/cd/E19253-01/819-5461/index.html">Oracle
	    Solaris ZFS Administration
	    Guide</ulink></para>
      </listitem>

      <listitem>
	<para><ulink
	    url="http://www.solarisinternals.com/wiki/index.php/ZFS_Evil_Tuning_Guide">ZFS
	    Evil Tuning Guide</ulink></para>
      </listitem>

      <listitem>
	<para><ulink
	    url="http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide">ZFS
	    Best Practices Guide</ulink></para>
      </listitem>
    </itemizedlist>
  </sect1>

  <sect1 id="zfs-term">
    <title><acronym>ZFS</acronym> Features and Terminology</title>

    <para><acronym>ZFS</acronym> is a fundamentally different file
      system because it is more than just a file system.
      <acronym>ZFS</acronym> combines the roles of file system and
      volume manager, enabling additional storage devices to be added
      to a live system and having the new space available on all of
      the existing file systems in that pool immediately.  By
      combining the traditionally separate roles,
      <acronym>ZFS</acronym> is able to overcome previous limitations
      that prevented <acronym>RAID</acronym> groups being able to
      grow.  Each top level device in a zpool is called a vdev, which
      can be a simple disk or a <acronym>RAID</acronym> transformation
      such as a mirror or <acronym>RAID-Z</acronym> array.
      <acronym>ZFS</acronym> file systems (called datasets), each have
      access to the combined free space of the entire pool.  As blocks
      are allocated from the pool, the space available to each file
      system decreases.  This approach avoids the common pitfall with
      extensive partitioning where free space becomes fragmentated
      across the partitions.</para>

    <informaltable pgwide="1">
      <tgroup cols="2">
	<tbody valign="top">
	  <row>
	    <entry id="zfs-term-zpool">zpool</entry>

	    <entry>A storage pool is the most basic building block of
	      <acronym>ZFS</acronym>.  A pool is made up of one or
	      more vdevs, the underlying devices that store the data.
	      A pool is then used to create one or more file systems
	      (datasets) or block devices (volumes).  These datasets
	      and volumes share the pool of remaining free space.
	      Each pool is uniquely identified by a name and a
	      <acronym>GUID</acronym>.  The zpool also controls the
	      version number and therefore the features available for
	      use with <acronym>ZFS</acronym>.

	      <note>
		<para>&os;&nbsp;9.0 and 9.1 include support for
		  <acronym>ZFS</acronym> version 28.  Future versions
		  use <acronym>ZFS</acronym> version 5000 with feature
		  flags.  This allows greater cross-compatibility with
		  other implementations of
		  <acronym>ZFS</acronym>.</para>
	      </note></entry>
	  </row>

	  <row>
	    <entry id="zfs-term-vdev">vdev&nbsp;Types</entry>

	    <entry>A zpool is made up of one or more vdevs, which
	      themselves can be a single disk or a group of disks, in
	      the case of a <acronym>RAID</acronym> transform.  When
	      multiple vdevs are used, <acronym>ZFS</acronym> spreads
	      data across the vdevs to increase performance and
	      maximize usable space.

	      <itemizedlist>
		<listitem>
		  <para id="zfs-term-vdev-disk">
		    <emphasis>Disk</emphasis> - The most basic type
		    of vdev is a standard block device.  This can be
		    an entire disk (such as
		    <devicename><replaceable>/dev/ada0</replaceable></devicename>
		    or
		    <devicename><replaceable>/dev/da0</replaceable></devicename>)
		    or a partition
		    (<devicename><replaceable>/dev/ada0p3</replaceable></devicename>).
		    Contrary to the Solaris documentation, on &os;
		    there is no performance penalty for using a
		    partition rather than an entire disk.</para>
		</listitem>

		<listitem>
		  <para id="zfs-term-vdev-file">
		    <emphasis>File</emphasis> - In addition to disks,
		    <acronym>ZFS</acronym> pools can be backed by
		    regular files, this is especially useful for
		    testing and experimentation.  Use the full path to
		    the file as the device path in the zpool create
		    command.  All vdevs must be atleast 128&nbsp;MB in
		    size.</para>
		</listitem>

		<listitem>
		  <para id="zfs-term-vdev-mirror">
		    <emphasis>Mirror</emphasis> - When creating a
		    mirror, specify the <literal>mirror</literal>
		    keyword followed by the list of member devices
		    for the mirror.  A mirror consists of two or
		    more devices, all data will be written to all
		    member devices.  A mirror vdev will only hold as
		    much data as its smallest member.  A mirror vdev
		    can withstand the failure of all but one of its
		    members without losing any data.</para>

		  <note>
		    <para>regular single disk vdev can be upgraded to
		      a mirror vdev at any time using the
		      <command>zpool</command> <link
			linkend="zfs-zpool-attach">attach</link>
		      command.</para>
		  </note>
		</listitem>

		<listitem>
		  <para id="zfs-term-vdev-raidz">
		    <emphasis><acronym>RAID-Z</acronym></emphasis> -
		    <acronym>ZFS</acronym> implements
		    <acronym>RAID-Z</acronym>, a variation on standard
		    <acronym>RAID-5</acronym> that offers better
		    distribution of parity and eliminates the
		    "<acronym>RAID-5</acronym> write hole" in which
		    the data and parity information become
		    inconsistent after an unexpected restart.
		    <acronym>ZFS</acronym> supports 3 levels of
		    <acronym>RAID-Z</acronym> which provide varying
		    levels of redundancy in exchange for decreasing
		    levels of usable storage.  The types are named
		    <acronym>RAID-Z1</acronym> through
		    <acronym>RAID-Z3</acronym> based on the number of
		    parity devinces in the array and the number of
		    disks that the pool can operate without.</para>

		  <para>In a <acronym>RAID-Z1</acronym> configuration
		    with 4 disks, each 1&nbsp;TB, usable storage will
		    be 3&nbsp;TB and the pool will still be able to
		    operate in degraded mode with one faulted disk.
		    If an additional disk goes offline before the
		    faulted disk is replaced and resilvered, all data
		    in the pool can be lost.</para>

		  <para>In a <acronym>RAID-Z3</acronym> configuration
		    with 8 disks of 1&nbsp;TB, the volume would
		    provide 5&nbsp;TB of usable space and still be
		    able to operate with three faulted disks.  &sun;
		    recommends no more than 9 disks in a single vdev.
		    If the configuration has more disks, it is
		    recommended to divide them into separate vdevs and
		    the pool data will be striped across them.</para>

		  <para>A configuration of 2
		    <acronym>RAID-Z2</acronym> vdevs consisting of 8
		    disks each would create something similar to a
		    <acronym>RAID-60</acronym> array.  A
		    <acronym>RAID-Z</acronym> group's storage capacity
		    is approximately the size of the smallest disk,
		    multiplied by the number of non-parity disks.
		    Four 1&nbsp;TB disks in <acronym>RAID-Z1</acronym>
		    has an effective size of approximately 3&nbsp;TB,
		    and an array of eight 1&nbsp;TB disks in
		    <acronym>RAID-Z3</acronym> will yield 5&nbsp;TB of
		    usable space.</para>
		</listitem>

		<listitem>
		  <para id="zfs-term-vdev-spare">
		    <emphasis>Spare</emphasis> -
		    <acronym>ZFS</acronym> has a special pseudo-vdev
		    type for keeping track of available hot spares.
		    Note that installed hot spares are not deployed
		    automatically; they must manually be configured to
		    replace the failed device using
		    <command>zfs replace</command>.</para>
		</listitem>

		<listitem>
		  <para id="zfs-term-vdev-log">
		    <emphasis>Log</emphasis> - <acronym>ZFS</acronym>
		    Log Devices, also known as ZFS Intent Log
		    (<acronym>ZIL</acronym>) move the intent log from
		    the regular pool devices to a dedicated device.
		    The <acronym>ZIL</acronym> accelerates synchronous
		    transactions by using storage devices (such as
		    <acronym>SSD</acronym>s) that are faster than
		    those used for the main pool.  When data is being
		    written and the application requests a guarantee
		    that the data has been safely stored, the data is
		    written to the faster <acronym>ZIL</acronym>
		    storage, then later flushed out to the regular
		    disks, greatly reducing the latency of synchronous
		    writes.  Log devices can be mirrored, but
		    <acronym>RAID-Z</acronym> is not supported.  If
		    multiple log devices are used, writes will be load
		    balanced across them.</para>
		</listitem>

		<listitem>
		  <para id="zfs-term-vdev-cache">
		    <emphasis>Cache</emphasis> - Adding a cache vdev
		    to a zpool will add the storage of the cache to
		    the <acronym>L2ARC</acronym>.  Cache devices
		    cannot be mirrored.  Since a cache device only
		    stores additional copies of existing data, there
		    is no risk of data loss.</para>
		</listitem>
	      </itemizedlist></entry>
	  </row>

	  <row>
	    <entry id="zfs-term-arc">Adaptive Replacement
	      Cache (<acronym>ARC</acronym>)</entry>

	    <entry><acronym>ZFS</acronym> uses an Adaptive Replacement
	      Cache (<acronym>ARC</acronym>), rather than a more
	      traditional Least Recently Used (<acronym>LRU</acronym>)
	      cache.  An <acronym>LRU</acronym> cache is a simple list
	      of items in the cache sorted by when each object was
	      most recently used; new items are added to the top of
	      the list and once the cache is full items from the
	      bottom of the list are evicted to make room for more
	      active objects.  An <acronym>ARC</acronym> consists of
	      four lists; the Most Recently Used
	      (<acronym>MRU</acronym>) and Most Frequently Used
	      (<acronym>MFU</acronym>) objects, plus a ghost list for
	      each.  These ghost lists track recently evicted objects
	      to prevent them from being added back to the cache.
	      This increases the cache hit ratio by avoiding objects
	      that have a history of only being used occasionally.
	      Another advantage of using both an
	      <acronym>MRU</acronym> and <acronym>MFU</acronym> is
	      that scanning an entire filesystem would normally evict
	      all data from an <acronym>MRU</acronym> or
	      <acronym>LRU</acronym> cache in favor of this freshly
	      accessed content.  In the case of
	      <acronym>ZFS</acronym>, since there is also an
	      <acronym>MFU</acronym> that only tracks the most
	      frequently used objects, the cache of the most commonly
	      accessed blocks remains.</entry>
	  </row>

	  <row>
	    <entry
	      id="zfs-term-l2arc"><acronym>L2ARC</acronym></entry>

	    <entry>The <acronym>L2ARC</acronym> is the second level
	      of the <acronym>ZFS</acronym> caching system.  The
	      primary <acronym>ARC</acronym> is stored in
	      <acronym>RAM</acronym>, however since the amount of
	      available <acronym>RAM</acronym> is often limited,
	      <acronym>ZFS</acronym> can also make use of
	      <link linkend="zfs-term-vdev-cache">cache</link>
	      vdevs.  Solid State Disks (<acronym>SSD</acronym>s) are
	      often used as these cache devices due to their higher
	      speed and lower latency compared to traditional spinning
	      disks.  An <acronym>L2ARC</acronym> is entirely
	      optional, but having one will significantly increase
	      read speeds for files that are cached on the
	      <acronym>SSD</acronym> instead of having to be read from
	      the regular spinning disks.  The
	      <acronym>L2ARC</acronym> can also speed up <link
		linkend="zfs-term-deduplication">deduplication</link>
	      since a <acronym>DDT</acronym> that does not fit in
	      <acronym>RAM</acronym> but does fit in the
	      <acronym>L2ARC</acronym> will be much faster than if the
	      <acronym>DDT</acronym> had to be read from disk.  The
	      rate at which data is added to the cache devices is
	      limited to prevent prematurely wearing out the
	      <acronym>SSD</acronym> with too many writes.  Until the
	      cache is full (the first block has been evicted to make
	      room), writing to the <acronym>L2ARC</acronym> is
	      limited to the sum of the write limit and the boost
	      limit, then after that limited to the write limit.  A
	      pair of sysctl values control these rate limits;
	      <literal>vfs.zfs.l2arc_write_max</literal> controls how
	      many bytes are written to the cache per second, while
	      <literal>vfs.zfs.l2arc_write_boost</literal> adds to
	      this limit during the "Turbo Warmup Phase" (Write
	      Boost).</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-cow">Copy-On-Write</entry>

	    <entry>Unlike a traditional file system, when data is
	      overwritten on <acronym>ZFS</acronym> the new data is
	      written to a different block rather than overwriting the
	      old data in place.  Only once this write is complete is
	      the metadata then updated to point to the new location
	      of the data.  This means that in the event of a shorn
	      write (a system crash or power loss in the middle of
	      writing a file), the entire original contents of the
	      file are still available and the incomplete write is
	      discarded.  This also means that <acronym>ZFS</acronym>
	      does not require a &man.fsck.8; after an unexpected
	      shutdown.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-dataset">Dataset</entry>

	    <entry>Dataset is the generic term for a
	      <acronym>ZFS</acronym> file system, volume, snapshot or
	      clone.  Each dataset will have a unique name in the
	      format: <literal>poolname/path@snapshot</literal>.  The
	      root of the pool is technically a dataset as well.
	      Child datasets are named hierarchically like
	      directories; for example,
	      <literal>mypool/home</literal>, the home dataset, is a
	      child of <literal>mypool</literal> and inherits
	      properties from it.  This can be expanded further by
	      creating <literal>mypool/home/user</literal>.  This
	      grandchild dataset will inherity properties from the
	      parent and grandparent.  It is also possible to set
	      properties on a child to override the defaults inherited
	      from the parents and grandparents.
	      <acronym>ZFS</acronym> also allows administration of
	      datasets and their children to be delegated.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-volum">Volume</entry>

	    <entry>In additional to regular file system datasets,
	      <acronym>ZFS</acronym> can also create volumes, which
	      are block devices.  Volumes have many of the same
	      features, including copy-on-write, snapshots, clones and
	      checksumming.  Volumes can be useful for running other
	      file system formats on top of <acronym>ZFS</acronym>,
	      such as <acronym>UFS</acronym> or in the case of
	      Virtualization or exporting <acronym>iSCSI</acronym>
	      extents.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-snapshot">Snapshot</entry>

	    <entry>The <link
		linkend="zfs-term-cow">copy-on-write</link> (<acronym>COW</acronym>) design of
	      <acronym>ZFS</acronym> allows for nearly instantaneous
	      consistent snapshots with arbitrary names.  After taking
	      a snapshot of a dataset (or a recursive snapshot of a
	      parent dataset that will include all child datasets),
	      new data is written to new blocks (as described above),
	      however the old blocks are not reclaimed as free space.
	      There are then two versions of the file system, the
	      snapshot (what the file system looked like before) and
	      the live file system; however no additional space is
	      used.  As new data is written to the live file system,
	      new blocks are allocated to store this data.  The
	      apparent size of the snapshot will grow as the blocks
	      are no longer used in the live file system, but only in
	      the snapshot.  These snapshots can be mounted (read
	      only) to allow for the recovery of previous versions of
	      files.  It is also possible to
	      <link linkend="zfs-zfs-snapshot">rollback</link> a live
	      file system to a specific snapshot, undoing any changes
	      that took place after the snapshot was taken.  Each
	      block in the zpool has a reference counter which
	      indicates how many snapshots, clones, datasets or
	      volumes make use of that block.  As files and snapshots
	      are deleted, the reference count is decremented; once a
	      block is no longer referenced, it is reclaimed as free
	      space.  Snapshots can also be marked with a
	      <link linkend="zfs-zfs-snapshot">hold</link>, once a
	      snapshot is held, any attempt to destroy it will return
	      an EBUY error.  Each snapshot can have multiple holds,
	      each with a unique name.  The
	      <link linkend="zfs-zfs-snapshot">release</link> command
	      removes the hold so the snapshot can then be deleted.
	      Snapshots can be taken on volumes, however they can only
	      be cloned or rolled back, not mounted
	      independently.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-clone">Clone</entry>

	    <entry>Snapshots can also be cloned; a clone is a writable
	      version of a snapshot, allowing the file system to be
	      forked as a new dataset.  As with a snapshot, a clone
	      initially consumes no additional space, only as new data
	      is written to a clone and new blocks are allocated does
	      the apparent size of the clone grow.  As blocks are
	      overwritten in the cloned file system or volume, the
	      reference count on the previous block is decremented.
	      The snapshot upon which a clone is based cannot be
	      deleted because the clone is dependeant upon it (the
	      snapshot is the parent, and the clone is the child).
	      Clones can be <literal>promoted</literal>, reversing
	      this dependeancy, making the clone the parent and the
	      previous parent the child.  This operation requires no
	      additional space, however it will change the way the
	      used space is accounted.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-checksum">Checksum</entry>

	    <entry>Every block that is allocated is also checksummed
	      (the algorithm used is a per dataset property, see:
	      <command>zfs set</command>).  <acronym>ZFS</acronym>
	      transparently validates the checksum of each block as it
	      is read, allowing <acronym>ZFS</acronym> to detect
	      silent corruption.  If the data that is read does not
	      match the expected checksum, <acronym>ZFS</acronym> will
	      attempt to recover the data from any available
	      redundancy, like mirrors or <acronym>RAID-Z</acronym>).
	      Validation of all checksums can be triggered with the
	      <link
		linkend="zfs-term-scrub"><command>scrub</command></link>
	      command.  Available checksum algorithms include:

	      <itemizedlist>
		<listitem>
		  <para>fletcher2</para>
		</listitem>

		<listitem>
		  <para>fletcher4</para>
		</listitem>

		<listitem>
		  <para>sha256</para>
		</listitem>
	      </itemizedlist>

	      The fletcher algorithms are faster, but sha256 is a
	      strong cryptographic hash and has a much lower chance of
	      collisions at the cost of some performance.  Checksums
	      can be disabled but it is inadvisable.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-compression">Compression</entry>

	    <entry>Each dataset in <acronym>ZFS</acronym> has a
	      compression property, which defaults to off.  This
	      property can be set to one of a number of compression
	      algorithms, which will cause all new data that is
	      written to this dataset to be compressed as it is
	      written.  In addition to the reduction in disk usage,
	      this can also increase read and write throughput, as
	      only the smaller compressed version of the file needs to
	      be read or written.

	      <note>
		<para><acronym>LZ4</acronym> compression is only
		  available after &os;&nbsp;9.2.</para>
	      </note></entry>
	  </row>

	  <row>
	    <entry id="zfs-term-deduplication">Deduplication</entry>

	    <entry><acronym>ZFS</acronym> has the ability to detect
	      duplicate blocks of data as they are written (thanks to
	      the checksumming feature).  If deduplication is enabled,
	      instead of writing the block a second time, the
	      reference count of the existing block will be increased,
	      saving storage space.  To do this,
	      <acronym>ZFS</acronym> keeps a deduplication table
	      (<acronym>DDT</acronym>) in memory, containing the list
	      of unique checksums, the location of that block and a
	      reference count.  When new data is written, the checksum
	      is calculated and compared to the list.  If a match is
	      found, the data is considered to be a duplicate.  When
	      deduplication is enabled, the checksum algorithm is
	      changed to <acronym>SHA256</acronym> to provide a secure
	      cryptographic hash.  <acronym>ZFS</acronym>
	      deduplication is tunable; if dedup is on, then a
	      matching checksum is assumed to mean that the data is
	      identical.  If dedup is set to verify, then the data in
	      the two blocks will be checked byte-for-byte to ensure
	      it is actually identical and if it is not, the hash
	      collision will be noted by <acronym>ZFS</acronym> and
	      the two blocks will be stored separately.  Due to the
	      nature of the <acronym>DDT</acronym>, having to store
	      the hash of each unique block, it consumes a very large
	      amount of memory (a general rule of thumb is 5-6&nbsp;GB
	      of ram per 1&nbsp;TB of deduplicated data).  In
	      situations where it is not practical to have enough
	      <acronym>RAM</acronym> to keep the entire
	      <acronym>DDT</acronym> in memory, performance will
	      suffer greatly as the <acronym>DDT</acronym> will need
	      to be read from disk before each new block is written.
	      Deduplication can make use of the
	      <acronym>L2ARC</acronym> to store the
	      <acronym>DDT</acronym>, providing a middle ground
	      between fast system memory and slower disks.  Consider
	      using <acronym>ZFS</acronym> compression instead, which
	      often provides nearly as much space savings without the
	      additional memory requirement.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-scrub">Scrub</entry>

	    <entry>In place of a consistency check like &man.fsck.8;,
	      <acronym>ZFS</acronym> has the <literal>scrub</literal>
	      command, which reads all data blocks stored on the pool
	      and verifies their checksums them against the known good
	      checksums stored in the metadata.  This periodic check
	      of all the data stored on the pool ensures the recovery
	      of any corrupted blocks before they are needed.  A scrub
	      is not required after an unclean shutdown, but it is
	      recommended that you run a scrub at least once each
	      quarter.  <acronym>ZFS</acronym> compares the checksum
	      for each block as it is read in the normal course of
	      use, but a scrub operation makes sure even infrequently
	      used blocks are checked for silent corruption.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-quota">Dataset Quota</entry>

	    <entry><acronym>ZFS</acronym> provides very fast and
	      accurate dataset, user and group space accounting in
	      addition to quotas and space reservations.  This gives
	      the administrator fine grained control over how space is
	      allocated and allows critical file systems to reserve
	      space to ensure other file systems do not take all of
	      the free space.

	      <para><acronym>ZFS</acronym> supports different types of
		quotas: the dataset quota, the <link
		  linkend="zfs-term-refquota">reference
		  quota (<acronym>refquota</acronym>)</link>, the
		<link linkend="zfs-term-userquota">user
		  quota</link>, and the
		<link linkend="zfs-term-groupquota">group
		  quota</link>.</para>

	      <para>Quotas limit the amount of space that a dataset
		and all of its descendants (snapshots of the dataset,
		child datasets and the snapshots of those datasets)
		can consume.</para>

	      <note>
		<para>Quotas cannot be set on volumes, as the
		  <literal>volsize</literal> property acts as an
		  implicit quota.</para>
	      </note></entry>
	  </row>

	  <row>
	    <entry id="zfs-term-refquota">Reference
	      Quota</entry>

	    <entry>A reference quota limits the amount of space a
	      dataset can consume by enforcing a hard limit on the
	      space used.  However, this hard limit includes only
	      space that the dataset references and does not include
	      space used by descendants, such as file systems or
	      snapshots.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-userquota">User
	      Quota</entry>

	    <entry>User quotas are useful to limit the amount of space
	      that can be used by the specified user.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-groupquota">Group
	      Quota</entry>

	    <entry>The group quota limits the amount of space that a
	      specified group can consume.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-reservation">Dataset
	      Reservation</entry>

	    <entry>The <literal>reservation</literal> property makes
	      it possible to guaranteed a minimum amount of space for
	      the use of a specific dataset and its descendants.  This
	      means that if a 10&nbsp;GB reservation is set on
	      <filename>storage/home/bob</filename>, if another
	      dataset tries to use all of the free space, at least
	      10&nbsp;GB of space is reserved for this dataset.  If a
	      snapshot is taken of
	      <filename class="directory">storage/home/bob</filename>,
	      the space used by that snapshot is counted against the
	      reservation.  The <link
		linkend="zfs-term-refreservation">refreservation</link>
	      property works in a similar way, except it
	      <emphasis>excludes</emphasis> descendants, such as
	      snapshots.

	      <para>Reservations of any sort are useful in many
		situations, such as planning and testing the
		suitability of disk space allocation in a new system,
		or ensuring that enough space is available on file
		systems for audio logs or system recovery procedures
		and files.</para></entry>
	  </row>

	  <row>
	    <entry id="zfs-term-refreservation">Reference
	      Reservation</entry>

	    <entry>The <literal>refreservation</literal> property
	      makes it possible to guaranteed a minimum amount of
	      space for the use of a specific dataset
	      <emphasis>excluding</emphasis> its descendants.  This
	      means that if a 10&nbsp;GB reservation is set on
	      <filename>storage/home/bob</filename>, if another
	      dataset tries to use all of the free space, at least
	      10&nbsp;GB of space is reserved for this dataset.  In
	      contrast to a regular <link
		linkend="zfs-term-reservation">reservation</link>,
	      space used by snapshots and decendant datasets is not
	      counted against the reservation.  As an example, if a
	      snapshot was taken of
	      <filename>storage/home/bob</filename>, enough disk space
	      would have to exist outside of the
	      <literal>refreservation</literal> amount for the
	      operation to succeed because descendants of the main
	      data set are not counted by the
	      <literal>refreservation</literal> amount and so do not
	      encroach on the space set.</entry>
	  </row>

	  <row>
	    <entry id="zfs-term-resilver">Resilver</entry>

	    <entry>When a disk fails and must be replaced, the new
	      disk must be filled with the data that was lost.  This
	      process of calculating and writing the missing data
	      (using the parity information distributed across the
	      remaining drives) to the new drive is called
	      <emphasis>resilvering</emphasis>.</entry>
	  </row>
	</tbody>
      </tgroup>
    </informaltable>
  </sect1>
</chapter>
