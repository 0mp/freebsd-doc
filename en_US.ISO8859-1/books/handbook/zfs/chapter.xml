<?xml version="1.0" encoding="iso-8859-1"?>
<!--
     The FreeBSD Documentation Project
     $FreeBSD$
-->

<chapter xmlns="http://docbook.org/ns/docbook"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
  xml:id="zfs">

  <info>
    <title>The Z File System (<acronym>ZFS</acronym>)</title>

    <authorgroup>
      <author>
	<personname>
	  <firstname>Tom</firstname>
	  <surname>Rhodes</surname>
	</personname>
	<contrib>Written by </contrib>
      </author>
      <author>
	<personname>
	  <firstname>Allan</firstname>
	  <surname>Jude</surname>
	</personname>
	<contrib>Written by </contrib>
      </author>
      <author>
	<personname>
	  <firstname>Benedict</firstname>
	  <surname>Reuschling</surname>
	</personname>
	<contrib>Written by </contrib>
      </author>
      <author>
	<personname>
	  <firstname>Warren</firstname>
	  <surname>Block</surname>
	</personname>
	<contrib>Written by </contrib>
      </author>
    </authorgroup>
  </info>

  <para>The <emphasis>Z File System</emphasis>, or
    <acronym>ZFS</acronym>, is an advanced file system designed to
    overcome many of the major problems found in previous
    designs.</para>

  <para>Originally developed at &sun;, ongoing <acronym>ZFS</acronym>
    development has moved to the
    <link xlink:href="http://open-zfs.org">OpenZFS Project</link>.
    <xref linkend="zfs-history"/> describes the development history in
    more detail.</para>

  <para><acronym>ZFS</acronym> has three major design goals:</para>

  <itemizedlist>
    <listitem>
      <para>Data integrity: All data includes a
	<link linkend="zfs-term-checksum">checksum</link> of the data.
	When data is written, the checksum is calculated and written
	along with it.  When that data is later read back, the
	checksum is calculated again.  If the checksums do not match,
	a data error has been detected.  <acronym>ZFS</acronym> will
	attempt to automatically correct errors when data redundancy
	is available.</para>
    </listitem>

    <listitem>
      <para>Pooled storage: physical storage devices are added to a
	pool, and storage space is allocated from that shared pool.
	Space is available to all file systems, and can be increased
	by adding new storage devices to the pool.</para>
    </listitem>

    <listitem>
      <para>Performance: multiple caching mechanisms provide increased
	performance.  <link linkend="zfs-term-arc">ARC</link> is an
	advanced memory-based read cache.  A second level of
	disk-based read cache can be added with
	<link linkend="zfs-term-l2arc">L2ARC</link>, and disk-based
	synchronous write cache is available with
	<link linkend="zfs-term-zil">ZIL</link>.</para>
    </listitem>
  </itemizedlist>

  <para>A complete list of <acronym>ZFS</acronym> features and
    terminology is shown in <xref linkend="zfs-term"/>.</para>

  <sect1 xml:id="zfs-differences">
    <title>What Makes <acronym>ZFS</acronym> Different</title>

    <para><acronym>ZFS</acronym> is significantly different from any
      previous file system because it is more than just a file system.
      Combining the traditionally separate roles of volume manager and
      file system provides <acronym>ZFS</acronym> with unique
      advantages.  The file system is now aware of the underlying
      structure of the disks.  Traditional file systems could only be
      created on a single disk at a time.  If there were two disks
      then two separate file systems would have to be created.  In a
      traditional hardware <acronym>RAID</acronym> configuration, this
      problem was worked around by presenting the operating system
      with a single logical disk made up of the space provided by a
      number of disks, on top of which the operating system placed its
      file system.  Even in the case of software
      <acronym>RAID</acronym> solutions like <acronym>GEOM</acronym>,
      the <acronym>UFS</acronym> file system living on top of the
      <acronym>RAID</acronym> transform believed that it was dealing
      with a single device.  <acronym>ZFS</acronym>'s combination of
      the volume manager and the file system solves this and allows
      the creation of many file systems all sharing a pool of
      available storage.  One of the biggest advantages to
      <acronym>ZFS</acronym>'s awareness of the physical layout of the
      disks is that <acronym>ZFS</acronym> can grow the existing file
      systems automatically when additional disks are added to the
      pool.  This new space is then made available to all of the file
      systems.  <acronym>ZFS</acronym> also has a number of different
      properties that can be applied to each file system, creating
      many advantages to creating a number of different filesystems
      and datasets rather than a single monolithic filesystem.</para>
  </sect1>

  <sect1 xml:id="zfs-quickstart">
    <title><acronym>ZFS</acronym> Quick Start Guide</title>

    <para>There is a start up mechanism that allows &os; to mount
      <acronym>ZFS</acronym> pools during system initialization.  To
      enable it, add this line to
      <filename>/etc/rc.conf</filename>:</para>

    <programlisting>zfs_enable="YES"</programlisting>

    <para>Then start the service:</para>

    <screen>&prompt.root; <userinput>service zfs start</userinput></screen>

    <para>The examples in this section assume three
      <acronym>SCSI</acronym> disks with the device names
      <filename><replaceable>da0</replaceable></filename>,
      <filename><replaceable>da1</replaceable></filename>, and
      <filename><replaceable>da2</replaceable></filename>.  Users
      of <acronym>SATA</acronym> hardware should instead use
      <filename><replaceable>ada</replaceable></filename> device
      names.</para>

    <sect2>
      <title>Single Disk Pool</title>

      <para>To create a simple, non-redundant <acronym>ZFS</acronym>
	pool using a single disk device, use
	<command>zpool</command>:</para>

      <screen>&prompt.root; <userinput>zpool create <replaceable>example</replaceable> <replaceable>/dev/da0</replaceable></userinput></screen>

      <para>To view the new pool, review the output of
	<command>df</command>:</para>

      <screen>&prompt.root; <userinput>df</userinput>
Filesystem  1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a   2026030  235230  1628718    13%    /
devfs               1       1        0   100%    /dev
/dev/ad0s1d  54098308 1032846 48737598     2%    /usr
example      17547136       0 17547136     0%    /example</screen>

      <para>This output shows that the <literal>example</literal> pool
	has been created and <emphasis>mounted</emphasis>.  It is now
	accessible as a file system.  Files may be created on it and
	users can browse it, as seen in the following example:</para>

      <screen>&prompt.root; <userinput>cd /example</userinput>
&prompt.root; <userinput>ls</userinput>
&prompt.root; <userinput>touch testfile</userinput>
&prompt.root; <userinput>ls -al</userinput>
total 4
drwxr-xr-x   2 root  wheel    3 Aug 29 23:15 .
drwxr-xr-x  21 root  wheel  512 Aug 29 23:12 ..
-rw-r--r--   1 root  wheel    0 Aug 29 23:15 testfile</screen>

      <para>However, this pool is not taking advantage of any
	<acronym>ZFS</acronym> features.  To create a dataset on this
	pool with compression enabled:</para>

      <screen>&prompt.root; <userinput>zfs create example/compressed</userinput>
&prompt.root; <userinput>zfs set compression=gzip example/compressed</userinput></screen>

      <para>The <literal>example/compressed</literal> dataset is now a
	<acronym>ZFS</acronym> compressed file system.  Try copying
	some large files to <filename
	  class="directory">/example/compressed</filename>.</para>

      <para>Compression can be disabled with:</para>

      <screen>&prompt.root; <userinput>zfs set compression=off example/compressed</userinput></screen>

      <para>To unmount a file system, use
	<command>zfs umount</command> and then verify by using
	<command>df</command>:</para>

      <screen>&prompt.root; <userinput>zfs umount example/compressed</userinput>
&prompt.root; <userinput>df</userinput>
Filesystem  1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a   2026030  235232  1628716    13%    /
devfs               1       1        0   100%    /dev
/dev/ad0s1d  54098308 1032864 48737580     2%    /usr
example      17547008       0 17547008     0%    /example</screen>

      <para>To re-mount the file system to make it accessible again,
	use <command>zfs mount</command> and verify with
	<command>df</command>:</para>

      <screen>&prompt.root; <userinput>zfs mount example/compressed</userinput>
&prompt.root; <userinput>df</userinput>
Filesystem         1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a          2026030  235234  1628714    13%    /
devfs                      1       1        0   100%    /dev
/dev/ad0s1d         54098308 1032864 48737580     2%    /usr
example             17547008       0 17547008     0%    /example
example/compressed  17547008       0 17547008     0%    /example/compressed</screen>

      <para>The pool and file system may also be observed by viewing
	the output from <command>mount</command>:</para>

      <screen>&prompt.root; <userinput>mount</userinput>
/dev/ad0s1a on / (ufs, local)
devfs on /dev (devfs, local)
/dev/ad0s1d on /usr (ufs, local, soft-updates)
example on /example (zfs, local)
example/data on /example/data (zfs, local)
example/compressed on /example/compressed (zfs, local)</screen>

      <para><acronym>ZFS</acronym> datasets, after creation, may be
	used like any file systems.  However, many other features are
	available which can be set on a per-dataset basis.  In the
	following example, a new file system, <literal>data</literal>
	is created.  Important files will be stored here, the file
	system is set to keep two copies of each data block:</para>

      <screen>&prompt.root; <userinput>zfs create example/data</userinput>
&prompt.root; <userinput>zfs set copies=2 example/data</userinput></screen>

      <para>It is now possible to see the data and space utilization
	by issuing <command>df</command>:</para>

      <screen>&prompt.root; <userinput>df</userinput>
Filesystem         1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a          2026030  235234  1628714    13%    /
devfs                      1       1        0   100%    /dev
/dev/ad0s1d         54098308 1032864 48737580     2%    /usr
example             17547008       0 17547008     0%    /example
example/compressed  17547008       0 17547008     0%    /example/compressed
example/data        17547008       0 17547008     0%    /example/data</screen>

      <para>Notice that each file system on the pool has the same
	amount of available space.  This is the reason for using
	<command>df</command> in these examples, to show that the file
	systems use only the amount of space they need and all draw
	from the same pool.  The <acronym>ZFS</acronym> file system
	does away with concepts such as volumes and partitions, and
	allows for several file systems to occupy the same
	pool.</para>

      <para>To destroy the file systems and then destroy the pool as
	they are no longer needed:</para>

      <screen>&prompt.root; <userinput>zfs destroy example/compressed</userinput>
&prompt.root; <userinput>zfs destroy example/data</userinput>
&prompt.root; <userinput>zpool destroy example</userinput></screen>
    </sect2>

    <sect2>
      <title><acronym>ZFS</acronym> RAID-Z</title>

      <para>Disks fail.  One
	method of avoiding data loss from disk failure is to
	implement <acronym>RAID</acronym>.  <acronym>ZFS</acronym>
	supports this feature in its pool design.
	<acronym>RAID-Z</acronym> pools require three or more disks
	but yield more usable space than mirrored pools.</para>

      <para>To create a <acronym>RAID-Z</acronym> pool, use this
	command, specifying the disks to add to the
	pool:</para>

      <screen>&prompt.root; <userinput>zpool create storage raidz da0 da1 da2</userinput></screen>

      <note>
	<para>&sun; recommends that the number of devices used in a
	  <acronym>RAID</acronym>-Z configuration is between three and
	  nine.  For environments requiring a single pool consisting
	  of 10 disks or more, consider breaking it up into smaller
	  <acronym>RAID-Z</acronym> groups.  If only two disks are
	  available and redundancy is a requirement, consider using a
	  <acronym>ZFS</acronym> mirror.  Refer to &man.zpool.8; for
	  more details.</para>
      </note>

      <para>This command creates the <literal>storage</literal> zpool.
	This may be verified using &man.mount.8; and &man.df.1;.  This
	command makes a new file system in the pool called
	<literal>home</literal>:</para>

      <screen>&prompt.root; <userinput>zfs create storage/home</userinput></screen>

      <para>Now compression and keeping extra copies of directories
	and files can be enabled with these commands:</para>

      <screen>&prompt.root; <userinput>zfs set copies=2 storage/home</userinput>
&prompt.root; <userinput>zfs set compression=gzip storage/home</userinput></screen>

      <para>To make this the new home directory for users, copy the
	user data to this directory, and create the appropriate
	symbolic links:</para>

      <screen>&prompt.root; <userinput>cp -rp /home/* /storage/home</userinput>
&prompt.root; <userinput>rm -rf /home /usr/home</userinput>
&prompt.root; <userinput>ln -s /storage/home /home</userinput>
&prompt.root; <userinput>ln -s /storage/home /usr/home</userinput></screen>

      <para>Users now have their data stored on the freshly
	created <filename class="directory">/storage/home</filename>.
	Test by adding a new user and logging in as that user.</para>

      <para>Try creating a snapshot which can be rolled back
	later:</para>

      <screen>&prompt.root; <userinput>zfs snapshot storage/home@08-30-08</userinput></screen>

      <para>Note that the snapshot option will only capture a real
	file system, not a home directory or a file.  The
	<literal>@</literal> character is a delimiter used between the
	file system name or the volume name.  When a user's home
	directory is accidentally deleted, restore it with:</para>

      <screen>&prompt.root; <userinput>zfs rollback storage/home@08-30-08</userinput></screen>

      <para>To list all available snapshots, run
	<command>ls</command> in the file system's
	<filename class="directory">.zfs/snapshot</filename>
	directory.  For example, to see the previously taken
	snapshot:</para>

      <screen>&prompt.root; <userinput>ls /storage/home/.zfs/snapshot</userinput></screen>

      <para>It is possible to write a script to perform regular
	snapshots on user data.  However, over time, snapshots can
	consume a great deal of disk space.  The previous snapshot can
	be removed using the following command:</para>

      <screen>&prompt.root; <userinput>zfs destroy storage/home@08-30-08</userinput></screen>

      <para>After testing,
	<filename class="directory">/storage/home</filename> can be
	made the real <filename class="directory">/home</filename>
	using this command:</para>

      <screen>&prompt.root; <userinput>zfs set mountpoint=/home storage/home</userinput></screen>

      <para>Run <command>df</command> and <command>mount</command> to
	confirm that the system now treats the file system as the real
	<filename class="directory">/home</filename>:</para>

      <screen>&prompt.root; <userinput>mount</userinput>
/dev/ad0s1a on / (ufs, local)
devfs on /dev (devfs, local)
/dev/ad0s1d on /usr (ufs, local, soft-updates)
storage on /storage (zfs, local)
storage/home on /home (zfs, local)
&prompt.root; <userinput>df</userinput>
Filesystem   1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a    2026030  235240  1628708    13%    /
devfs                1       1        0   100%    /dev
/dev/ad0s1d   54098308 1032826 48737618     2%    /usr
storage       26320512       0 26320512     0%    /storage
storage/home  26320512       0 26320512     0%    /home</screen>

      <para>This completes the <acronym>RAID-Z</acronym>
	configuration.  Daily status updates about the file systems
	created can be generated as part of the nightly
	&man.periodic.8; runs:</para>

      <screen>&prompt.root; <userinput>echo 'daily_status_zfs_enable="YES"' &gt;&gt; /etc/periodic.conf</userinput></screen>
    </sect2>

    <sect2>
      <title>Recovering <acronym>RAID</acronym>-Z</title>

      <para>Every software <acronym>RAID</acronym> has a method of
	monitoring its <literal>state</literal>.  The status of
	<acronym>RAID-Z</acronym> devices may be viewed with this
	command:</para>

      <screen>&prompt.root; <userinput>zpool status -x</userinput></screen>

      <para>If all pools are
	<link linkend="zfs-term-online">Online</link> and everything
	is normal, the message indicates that:</para>

      <screen>all pools are healthy</screen>

      <para>If there is an issue, perhaps a disk is in the
	<link linkend="zfs-term-offline">Offline</link> state, the
	pool state will look similar to:</para>

      <screen>  pool: storage
 state: DEGRADED
status: One or more devices has been taken offline by the administrator.
	Sufficient replicas exist for the pool to continue functioning in a
	degraded state.
action: Online the device using 'zpool online' or replace the device with
	'zpool replace'.
 scrub: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	storage     DEGRADED     0     0     0
	  raidz1    DEGRADED     0     0     0
	    da0     ONLINE       0     0     0
	    da1     OFFLINE      0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</screen>

      <para>This indicates that the device was previously taken
	offline by the administrator with this command:</para>

      <screen>&prompt.root; <userinput>zpool offline storage da1</userinput></screen>

      <para>Now the system can be powered down to replace
	<filename>da1</filename>.  When the system is back online,
	the failed disk can replaced in the pool:</para>

      <screen>&prompt.root; <userinput>zpool replace storage da1</userinput></screen>

      <para>From here, the status may be checked again, this time
	without <option>-x</option> so that all pools are
	shown:</para>

      <screen>&prompt.root; <userinput>zpool status storage</userinput>
 pool: storage
 state: ONLINE
 scrub: resilver completed with 0 errors on Sat Aug 30 19:44:11 2008
config:

	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    da0     ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</screen>

      <para>In this example, everything is normal.</para>
    </sect2>

    <sect2>
      <title>Data Verification</title>

      <para><acronym>ZFS</acronym> uses checksums to verify the
	integrity of stored data.  These are enabled automatically
	upon creation of file systems and may be disabled using the
	following command:</para>

      <screen>&prompt.root; <userinput>zfs set checksum=off storage/home</userinput></screen>

      <warning>
	<para>Doing so is <emphasis>not</emphasis> recommended!
	  Checksums take very little storage space and provide data
	  integrity.  Many <acronym>ZFS</acronym> features will not
	  work properly with checksums disabled.  There is also no
	  noticeable performance gain from disabling these
	  checksums.</para>
      </warning>

      <para>Checksum verification is known as
	<quote>scrubbing</quote>.  Verify the data integrity of the
	<literal>storage</literal> pool, with this command:</para>

      <screen>&prompt.root; <userinput>zpool scrub storage</userinput></screen>

      <para>The duration of a scrub depends on the amount of data
	stored.  Large amounts of data can take a considerable amount
	of time to verify.  It is also very <acronym>I/O</acronym>
	intensive, so much so that only one scrub> may be run at any
	given time.  After the scrub has completed, the status is
	updated and may be viewed with a status request:</para>

      <screen>&prompt.root; <userinput>zpool status storage</userinput>
 pool: storage
 state: ONLINE
 scrub: scrub completed with 0 errors on Sat Jan 26 19:57:37 2013
config:

	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    da0     ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</screen>

      <para>The completion time is displayed and helps to ensure data
	integrity over a long period of time.</para>
	<!-- WB: what does that mean? -->

      <para>Refer to &man.zfs.8; and &man.zpool.8; for other
	<acronym>ZFS</acronym> options.</para>
    </sect2>
  </sect1>

  <sect1 xml:id="zfs-zpool">
    <title><command>zpool</command> Administration</title>

    <para>The administration of <acronym>ZFS</acronym> is divided
      between two main utilities.  The <command>zpool</command>
      utility which controls the operation of the pool and deals with
      adding, removing, replacing and managing disks, and the
      <link linkend="zfs-zfs"><command>zfs</command></link> utility,
      which deals with creating, destroying and managing datasets
      (both <link linkend="zfs-term-filesystem">filesystems</link> and
      <link linkend="zfs-term-volume">volumes</link>).</para>

    <sect2 xml:id="zfs-zpool-create">
      <title>Creating &amp; Destroying Storage Pools</title>

      <para>Creating a <acronym>ZFS</acronym> Storage Pool
	(<acronym>zpool</acronym>) involves making a number of
	decisions that are relatively permanent because the structure
	of the pool cannot be changed after the pool has been created.
	The most important decision is what types of vdevs to group
	the physical disks into.  See the list of
	<link linkend="zfs-term-vdev">vdev types</link> for details
	about the possible options.  After the pool has been created,
	most vdev types do not allow additional disks to be added to
	the vdev.  The exceptions are mirrors, which allow additional
	disks to be added to the vdev, and stripes, which can be
	upgraded to mirrors by attaching an additional disk to the
	vdev.  Although additional vdevs can be added to a pool, the
	layout of the pool cannot be changed once the pool has been
	created, instead the data must be backed up and the pool
	recreated.</para>

      <para>A <acronym>ZFS</acronym> pool that is no longer needed can
	be destroyed so that the disks making up the pool can be
	reused in another pool or for other purposes.  Destroying a
	pool involves unmounting all of the datasets in that pool.  If
	the datasets are in use, the unmount operation will fail and
	the pool will not be destroyed.  The destruction of the pool
	can be forced with <option>-f</option>, but this can cause
	undefined behavior in applications which had open files on
	those datasets.</para>
    </sect2>

    <sect2 xml:id="zfs-zpool-attach">
      <title>Adding and Removing Devices</title>

      <para>Adding disks to a zpool can be broken down into two
	separate cases: attaching a disk to an existing vdev with
	<command>zpool attach</command>, or adding vdevs to the pool
	with <command>zpool add</command>.  Only some <link
	linkend="zfs-term-vdev">vdev types</link> allow disks to be
	added to the vdev after creation.</para>

      <para>When adding disks to the existing vdev is not an option,
	as in the case of RAID-Z, the other option is to add a vdev to
	the pool.  It is possible, but discouraged, to mix vdev types.
	<acronym>ZFS</acronym> stripes data across each of the vdevs.
	For example, if there are two mirror vdevs, then this is
	effectively a <acronym>RAID</acronym> 10, striping the writes
	across the two sets of mirrors.  Because of the way that space
	is allocated in <acronym>ZFS</acronym> to attempt to have each
	vdev reach 100% full at the same time, there is a performance
	penalty if the vdevs have different amounts of free
	space.</para>

      <para>Currently, vdevs cannot be removed from a zpool, and disks
	can only be removed from a mirror if there is enough remaining
	redundancy.</para>
    </sect2>

    <sect2 xml:id="zfs-zpool-replace">
      <title>Replacing a Functioning Device</title>

      <para>There are a number of situations in which it may be
	desirable to replace a disk with a different disk.  This
	process requires connecting the new disk at the same time as
	the disk to be replaced.  <command>zpool replace</command>
	will copy all of the data from the old disk to the new one.
	After this operation completes, the old disk is disconnected
	from the vdev.  If the new disk is larger than the old disk,
	it may be possible to grow the zpool, using the new space.
	See <link linkend="zfs-zpool-online">Growing a
	  Pool</link>.</para>
    </sect2>

    <sect2 xml:id="zfs-zpool-resilver">
      <title>Dealing with Failed Devices</title>

      <para>When a disk in a <acronym>ZFS</acronym> pool fails, the
	vdev that the disk belongs to will enter the
	<link linkend="zfs-term-degraded">Degraded</link> state.  In
	this state, all of the data stored on the vdev is still
	available, but performance may be impacted because missing
	data will need to be calculated from the available redundancy.
	To restore the vdev to a fully functional state, the failed
	physical device must be replaced, and <acronym>ZFS</acronym>
	must be instructed to begin the
	<link linkend="zfs-term-resilver">resilver</link> operation,
	where data that was on the failed device will be recalculated
	from available redundancy and written to the replacement
	device.  After the process has completed, the vdev will return
	to <link linkend="zfs-term-online">Online</link> status.  If
	the vdev does not have any redundancy, or if multiple devices
	have failed and there is not enough redundancy to
	compensate, the pool will enter the
	<link linkend="zfs-term-faulted">Faulted</link> state.  If a
	sufficient number of devices cannot be reconnected to the pool
	then the pool will be inoperative, and data must be
	restored from backups.</para>
    </sect2>

    <sect2 xml:id="zfs-zpool-selfheal">
      <title>ZFS Self-Healing</title>

      <para><acronym>ZFS</acronym> utilizes the checkums stored with
	each data block to provide a feature called self-healing.
	This feature will automatically repair data whose checksum
	does not match the one recorded on another device that is part
	of the storage pool.  For example, a mirror with two disks
	where one drive is starting to malfunction and cannot properly
	store the data any more.  This is even worse when the data has
	not been accessed for a long time in long term archive storage
	for example.  Traditional file systems need to run algorithms
	that check and repair the data like the &man.fsck.8; program.
	These commands take time and in severe cases, an administrator
	has to manually decide which repair operation has to be
	performed.  When <acronym>ZFS</acronym> detects that a data
	block is being read whose checksum does not match, it will try
	to read the data from the mirror disk.  If that disk can
	provide the correct data, it will not only give that data to
	the application requesting it, but also correct the wrong data
	on the disk that had the bad checksum.  This happens without
	any interaction of a system administrator during normal pool
	operation.</para>

      <para>The following example will demonstrate this self-healing
	behavior in <acronym>ZFS</acronym>.  First, a mirrored pool of
	two disks <filename>/dev/ada0</filename> and
	<filename>/dev/ada1</filename> is created.</para>

      <screen>&prompt.root; <userinput>zpool create <replaceable>healer</replaceable> mirror <replaceable>/dev/ada0</replaceable> <replaceable>/dev/ada1</replaceable></userinput>
&prompt.root; <userinput>zpool status <replaceable>healer</replaceable></userinput>
  pool: healer
 state: ONLINE
  scan: none requested
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0     0

errors: No known data errors
&prompt.root; <userinput>zpool list</userinput>
NAME     SIZE  ALLOC   FREE    CAP  DEDUP  HEALTH  ALTROOT
healer   960M  92.5K   960M     0%  1.00x  ONLINE  -</screen>

      <para>Now, some important data that we want to protect from data
	errors using the self-healing feature is copied to the pool.
	A checksum of the pool is then created to compare it against
	the pool later on.</para>

      <screen>&prompt.root; <userinput>cp /some/important/data /healer</userinput>
&prompt.root; <userinput>zfs list</userinput>
NAME     SIZE  ALLOC   FREE    CAP  DEDUP  HEALTH  ALTROOT
healer   960M  67.7M   892M     7%  1.00x  ONLINE  -
&prompt.root; <userinput>sha1 /healer > checksum.txt</userinput>
&prompt.root; <userinput>cat checksum.txt</userinput>
SHA1 (/healer) = 2753eff56d77d9a536ece6694bf0a82740344d1f</screen>

      <para>Next, data corruption is simulated by writing random data
	to the beginning of one of the disks that make up the mirror.
	To prevent <acronym>ZFS</acronym> from healing the data as
	soon as it detects it, we export the pool first and import it
	again afterwards.</para>

      <warning>
	<para>This is a dangerous operation that can destroy vital
	  data.  It is shown here for demonstrational purposes only
	  and should not be attempted during normal operation of a
	  <acronym>ZFS</acronym> storage pool.  Nor should this
	  <command>dd</command> example be run on a disk with a
	  different filesystem on it.  Do not use any other disk
	  device names other than the ones that are part of the
	  <acronym>ZFS</acronym> pool.  Make sure that proper backups
	  of the pool are created before running the command!</para>
      </warning>

      <screen>&prompt.root; <userinput>zpool export <replaceable>healer</replaceable></userinput>
&prompt.root; <userinput>dd if=/dev/random of=/dev/ada1 bs=1m count=200</userinput>
200+0 records in
200+0 records out
209715200 bytes transferred in 62.992162 secs (3329227 bytes/sec)
&prompt.root; <userinput>zpool import healer</userinput></screen>

      <para>The <acronym>ZFS</acronym> pool status shows that one
	device has experienced an error.  It is important to know that
	applications reading data from the pool did not receive any
	data with a wrong checksum.  <acronym>ZFS</acronym> did
	provide the application with the data from the
	<filename>ada0</filename> device that has the correct
	checksums.  The device with the wrong checksum can be found
	easily as the <literal>CKSUM</literal> column contains a value
	greater than zero.</para>

      <screen>&prompt.root; <userinput>zpool status <replaceable>healer</replaceable></userinput>
    pool: healer
   state: ONLINE
  status: One or more devices has experienced an unrecoverable error.  An
          attempt was made to correct the error.  Applications are unaffected.
  action: Determine if the device needs to be replaced, and clear the errors
          using 'zpool clear' or replace the device with 'zpool replace'.
     see: http://www.sun.com/msg/ZFS-8000-9P
    scan: none requested
  config:

      NAME        STATE     READ WRITE CKSUM
      healer      ONLINE       0     0     0
        mirror-0  ONLINE       0     0     0
         ada0     ONLINE       0     0     0
         ada1     ONLINE       0     0     1

errors: No known data errors</screen>

      <para><acronym>ZFS</acronym> has detected the error and took
	care of it by using the redundancy present in the unaffected
	<filename>ada0</filename> mirror disk.  A checksum comparison
	with the original one should reveal whether the pool is
	consistent again.</para>

      <screen>&prompt.root; <userinput>sha1 /healer >> checksum.txt</userinput>
&prompt.root; <userinput>cat checksum.txt</userinput>
SHA1 (/healer) = 2753eff56d77d9a536ece6694bf0a82740344d1f
SHA1 (/healer) = 2753eff56d77d9a536ece6694bf0a82740344d1f</screen>

      <para>The two checksums that were generated before and after the
	intentional tampering with the pool data still match.  This
	shows how <acronym>ZFS</acronym> is capable of detecting and
	correcting any errors automatically when the checksums do not
	match any more.  Note that this is only possible when there is
	enough redundancy present in the pool.  A pool consisting of a
	single device has no self-healing capabilities.  That is also
	the reason why checksums are so important in
	<acronym>ZFS</acronym> and should not be disabled for any
	reason.  No &man.fsck.8; or similar filesystem consistency
	check program is required to detect and correct this and the
	pool was available the whole time.  A scrub operation is now
	required to remove the falsely written data from
	<filename>ada1</filename>.</para>

      <screen>&prompt.root; <userinput>zpool scrub <replaceable>healer</replaceable></userinput>
&prompt.root; <userinput>zpool status <replaceable>healer</replaceable></userinput>
  pool: healer
 state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
            attempt was made to correct the error.  Applications are unaffected.
action: Determine if the device needs to be replaced, and clear the errors
            using 'zpool clear' or replace the device with 'zpool replace'.
   see: http://www.sun.com/msg/ZFS-8000-9P
  scan: scrub in progress since Mon Dec 10 12:23:30 2012
        10.4M scanned out of 67.0M at 267K/s, 0h3m to go
        9.63M repaired, 15.56% done
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0   627  (repairing)

errors: No known data errors</screen>

      <para>The scrub operation is reading the data from
	<filename>ada0</filename> and corrects all data that has a
	wrong checksum on <filename>ada1</filename>.  This is
	indicated by the <literal>(repairing)</literal> output from
	<command>zpool status</command>.  After the
	operation is complete, the pool status has changed to the
	following:</para>

      <screen>&prompt.root; <userinput>zpool status <replaceable>healer</replaceable></userinput>
  pool: healer
 state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
        attempt was made to correct the error.  Applications are unaffected.
action: Determine if the device needs to be replaced, and clear the errors
             using 'zpool clear' or replace the device with 'zpool replace'.
   see: http://www.sun.com/msg/ZFS-8000-9P
  scan: scrub repaired 66.5M in 0h2m with 0 errors on Mon Dec 10 12:26:25 2012
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0 2.72K

errors: No known data errors</screen>

      <para>After the scrub operation has completed and all the data
	has been synchronized from <filename>ada0</filename> to
	<filename>ada1</filename>, the error messages can be cleared
	from the pool status by running <command>zpool
	  clear</command>.</para>

      <screen>&prompt.root; <userinput>zpool clear <replaceable>healer</replaceable></userinput>
&prompt.root; <userinput>zpool status <replaceable>healer</replaceable></userinput>
  pool: healer
 state: ONLINE
  scan: scrub repaired 66.5M in 0h2m with 0 errors on Mon Dec 10 12:26:25 2012
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0     0

errors: No known data errors</screen>

      <para>Our pool is now back to a fully working state and all the
	errors have been cleared.</para>
    </sect2>

    <sect2 xml:id="zfs-zpool-online">
      <title>Growing a Pool</title>

      <para>The usable size of a redundant <acronym>ZFS</acronym> pool
	is limited by the size of the smallest device in the vdev.  If
	each device in the vdev is replaced sequentially, after the
	smallest device has completed the
	<link linkend="zfs-zpool-replace">replace</link> or
	<link linkend="zfs-term-resilver">resilver</link> operation,
	the pool can grow based on the size of the new smallest
	device.  This expansion can be triggered by using
	<command>zpool online</command> with <option>-e</option>
	on each device.  After expansion of all devices,
	the additional space will become available to the pool.</para>
    </sect2>

    <sect2 xml:id="zfs-zpool-import">
      <title>Importing &amp; Exporting Pools</title>

      <para>Pools can be exported in preparation for moving them to
	another system.  All datasets are unmounted, and each device
	is marked as exported but still locked so it cannot be used
	by other disk subsystems.  This allows pools to be imported on
	other machines, other operating systems that support
	<acronym>ZFS</acronym>, and even different hardware
	architectures (with some caveats, see &man.zpool.8;).  When a
	dataset has open files, <option>-f</option> can be used to
	force the export of a pool.  <option>-f</option> causes the
	datasets to be forcibly unmounted, which can cause undefined
	behavior in the applications which had open files on those
	datasets.</para>

      <para>Importing a pool automatically mounts the datasets.  This
	may not be the desired behavior, and can be prevented with
	<option>-N</option>.  <option>-o</option> sets temporary
	properties for this import only.  <option>altroot=</option>
	allows importing a zpool with a base mount point instead of
	the root of the file system.  If the pool was last used on a
	different system and was not properly exported, an import
	might have to be forced with <option>-f</option>.
	<option>-a</option> imports all pools that do not appear to be
	in use by another system.</para>
    </sect2>

    <sect2 xml:id="zfs-zpool-upgrade">
      <title>Upgrading a Storage Pool</title>

      <para>After upgrading &os;, or if a pool has been imported from
	a system using an older version of <acronym>ZFS</acronym>, the
	pool can be manually upgraded to the latest version of
	<acronym>ZFS</acronym>.  Consider whether the pool may ever
	need to be imported on an older system before upgrading.  The
	upgrade process is unreversible and cannot be undone.</para>

      <para>The newer features of <acronym>ZFS</acronym> will not be
	available until <command>zpool upgrade</command> has
	completed.  <option>-v</option> can be used to see what new
	features will be provided by upgrading, as well as which
	features are already supported by the existing version.</para>
    </sect2>

    <sect2 xml:id="zfs-zpool-status">
      <title>Checking the Status of a Pool</title>

      <para></para>
    </sect2>

    <sect2 xml:id="zfs-zpool-history">
      <title>Displaying Recorded Pool History</title>

      <para><acronym>ZFS</acronym> records all the commands that were
	issued to administer the pool.  These include the creation of
	datasets, changing properties, or when a disk has been
	replaced in the pool.  This history is useful for reviewing
	how a pool was created and which user did a specific action
	and when.  History is not kept in a log file, but is part of
	the pool itself.  Because of that, history cannot be altered
	after the fact unless the pool is destroyed.  The command to
	review this history is aptly named
	<command>zpool history</command>:</para>

      <screen>&prompt.root; <userinput>zpool history</userinput>
History for 'tank':
2013-02-26.23:02:35 zpool create tank mirror /dev/ada0 /dev/ada1
2013-02-27.18:50:58 zfs set atime=off tank
2013-02-27.18:51:09 zfs set checksum=fletcher4 tank
2013-02-27.18:51:18 zfs create tank/backup</screen>

      <para>The output shows <command>zpool</command> and
	<command>zfs</command> commands that were executed on the pool
	along with a timestamp.  Only commands that alter
	the pool in some way are recorded.  Commands like
	<command>zfs list</command> are not included.  When
	no pool name is given to
	<command>zpool history</command>, the history of all
	pools is displayed.</para>

      <para><command>zpool history</command> can show even more
	information when the options <option>-i</option> or
	<option>-l</option> are provided.  The option
	<option>-i</option> displays user initiated events as well
	as internally logged <acronym>ZFS</acronym> events.</para>

      <screen>&prompt.root; <userinput>zpool history -i</userinput>
History for 'tank':
2013-02-26.23:02:35 [internal pool create txg:5] pool spa 28; zfs spa 28; zpl 5;uts  9.1-RELEASE 901000 amd64
2013-02-27.18:50:53 [internal property set txg:50] atime=0 dataset = 21
2013-02-27.18:50:58 zfs set atime=off tank
2013-02-27.18:51:04 [internal property set txg:53] checksum=7 dataset = 21
2013-02-27.18:51:09 zfs set checksum=fletcher4 tank
2013-02-27.18:51:13 [internal create txg:55] dataset = 39
2013-02-27.18:51:18 zfs create tank/backup</screen>

      <para>More details can be shown by adding <option>-l</option>.
	History records are shown in a long format,
	including information like the name of the user who issued the
	command and the hostname on which the change was made.</para>

      <screen>&prompt.root; <userinput>zpool history -l</userinput>
History for 'tank':
2013-02-26.23:02:35 zpool create tank mirror /dev/ada0 /dev/ada1 [user 0 (root) on :global]
2013-02-27.18:50:58 zfs set atime=off tank [user 0 (root) on myzfsbox:global]
2013-02-27.18:51:09 zfs set checksum=fletcher4 tank [user 0 (root) on myzfsbox:global]
2013-02-27.18:51:18 zfs create tank/backup [user 0 (root) on myzfsbox:global]</screen>

      <para>This output clearly shows that the <systemitem
	  class="username">root</systemitem> user created the mirrored
	pool (consisting of <filename>/dev/ada0</filename> and
	<filename>/dev/ada1</filename>).  In addition to that, the
	hostname (<literal>myzfsbox</literal>) is also shown in the
	commands following the pool's creation.  The hostname display
	becomes important when the pool is exported from the current
	and imported on another system.  The commands that are issued
	on the other system can clearly be distinguished by the
	hostname that is recorded for each command.</para>

      <para>Both options to <command>zpool history</command> can be
	combined to give the most detailed information possible for
	any given pool.  Pool history provides valuable information
	when tracking down what actions were performed or when more
	detailed output is needed for debugging.</para>
    </sect2>

    <sect2 xml:id="zfs-zpool-iostat">
      <title>Performance Monitoring</title>

      <para>A built-in monitoring system can display
	statistics about I/O on the pool in real-time.  It
	shows the amount of free and used space on the pool, how many
	read and write operations are being performed per second, and
	how much I/O bandwidth is currently being utilized.
	By default, all pools in the system
	are monitored and displayed.  A pool name can be provided
	to limit monitoring to just that pool.  A
	basic example:</para>

      <screen>&prompt.root; <userinput>zpool iostat</userinput>
               capacity     operations    bandwidth
pool        alloc   free   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
data         288G  1.53T      2     11  11.3K  57.1K</screen>

      <para>To continuously monitor I/O activity on the pool, a
	number can be specified as the last parameter, indicating
	the frequency in seconds to wait between updates.
	The next statistic line is printed after each interval.  Press
	<keycombo action="simul">
	  <keycap>Ctrl</keycap>
	  <keycap>C</keycap>
	</keycombo> to stop this continuous monitoring.
	Alternatively, give a second number on the command line after
	the interval to specify the total number of statistics to
	display.</para>

      <para>Even more detailed pool I/O statistics can be displayed
	with <option>-v</option>.  Each device in
	the pool is shown with a statistics line.
	This is useful in seeing how many read and write
	operations are being performed on each device, and can help
	determine if any individual device is slowing down the
	pool.  This example shows a mirrored pool
	consisting of two devices:</para>

      <screen>&prompt.root; <userinput>zpool iostat -v </userinput>
                            capacity     operations    bandwidth
pool                     alloc   free   read  write   read  write
-----------------------  -----  -----  -----  -----  -----  -----
data                      288G  1.53T      2     12  9.23K  61.5K
  mirror                  288G  1.53T      2     12  9.23K  61.5K
    ada1                     -      -      0      4  5.61K  61.7K
    ada2                     -      -      1      4  5.04K  61.7K
-----------------------  -----  -----  -----  -----  -----  -----</screen>
    </sect2>

    <sect2 xml:id="zfs-zpool-split">
      <title>Splitting a Storage Pool</title>

      <para>A pool consisting of one or more mirror vdevs can be
	split into a second pool.  The last member of each mirror
	(unless otherwise specified) is detached and used to create a
	new pool containing the same data.  It is recommended that the
	operation first be attempted with the <option>-n</option>
	parameter.  The details of the proposed operation are
	displayed without actually performing it.  This helps ensure
	the operation will happen as expected.</para>
    </sect2>
  </sect1>

  <sect1 xml:id="zfs-zfs">
    <title><command>zfs</command> Administration</title>

    <para>The <command>zfs</command> utility is responsible for
      creating, destroying, and managing all <acronym>ZFS</acronym>
      datasets that exist within a pool.  The pool is managed using
      the <link linkend="zfs-zpool"><command>zpool</command></link>
      command.</para>

    <sect2 xml:id="zfs-zfs-create">
      <title>Creating &amp; Destroying Datasets</title>

      <para>Unlike traditional disks and volume managers, space
	in <acronym>ZFS</acronym> is not preallocated.  With
	traditional file systems, once all of the space was
	partitioned and assigned, there was no way to add an
	additional file system without adding a new disk.  With
	<acronym>ZFS</acronym>, new file systems can be created at any
	time.  Each <link
	  linkend="zfs-term-dataset"><emphasis>dataset</emphasis></link>
	has properties including features like compression,
	deduplication, caching and quoteas, as well as other useful
	properties like readonly, case sensitivity, network file
	sharing, and a mount point.  Each separate dataset can be
	administered, <link linkend="zfs-zfs-allow">delegated</link>,
	<link linkend="zfs-zfs-send">replicated</link>,
	<link linkend="zfs-zfs-snapshot">snapshoted</link>,
	<link linkend="zfs-zfs-jail">jailed</link>, and destroyed as a
	unit.  There are many advantages to creating a separate
	dataset for each different type or set of files.  The only
	drawbacks to having an extremely large number of datasets is
	that some commands like <command>zfs list</command> will be
	slower, and the mounting of hundreds or even thousands of
	datasets can slow the &os; boot process.</para>

      <para>Destroying a dataset is much quicker than deleting all
	of the files that reside on the dataset, as it does not
	invole scanning all of the files and updating all of the
	corresponding metadata.  In modern versions of
	<acronym>ZFS</acronym>, <command>zfs destroy</command>
	is asynchronous, and the free space may take several
	minutes to appear in the pool.  The <literal>freeing</literal>
	property, accessible with <command>zpool get freeing
	  <replaceable>poolname</replaceable></command> indicates how
	many datasets are having their blocks freed in the background.
	If there are child datasets, like
	<link linkend="zfs-term-snapshot">snapshots</link> or other
	datasets, then the parent cannot be destroyed.  To destroy a
	dataset and all of its children, use <option>-r</option>
	to recursively destroy the dataset and all of its
	children.  <option>-n -v</option> can be used
	to list
	datasets and snapshots that would be destroyed and, in the
	case of snapshots, how much space would be reclaimed by
	the actual destruction.</para>
    </sect2>

    <sect2 xml:id="zfs-zfs-volume">
      <title>Creating and Destroying Volumes</title>

      <para>A volume is a special type of <acronym>ZFS</acronym>
	dataset.  Rather than being mounted as a file system, it is
	exposed as a block device under
	<filename>/dev/zvol/<replaceable>poolname</replaceable>/<replaceable>dataset</replaceable></filename>.
	This allows the volume to be used for other file systems, to
	back the disks of a virtual machine, or to be exported using
	protocols like <acronym>iSCSI</acronym> or
	<acronym>HAST</acronym>.</para>

      <para>A volume can be formatted with any file system.  To the
	user, it will appear as if they are working with a regular
	disk using that specific filesystem and not
	<acronym>ZFS</acronym>.  Putting ordinary file systems on
	<acronym>ZFS</acronym> volumes provides features those file
	systems would not normally have.  For example, using the
	compression property on a 250&nbsp;MB volume allows creation
	of a compressed <acronym>FAT</acronym> filesystem.</para>

      <screen>&prompt.root; <userinput>zfs create -V 250m -o compression=on tank/fat32</userinput>
&prompt.root; <userinput>zfs list tank</userinput>
NAME USED AVAIL REFER MOUNTPOINT
tank 258M  670M   31K /tank
&prompt.root; <userinput>newfs_msdos -F32 /dev/zvol/tank/fat32</userinput>
&prompt.root; <userinput>mount -t msdosfs /dev/zvol/tank/fat32 /mnt</userinput>
&prompt.root; <userinput>df -h /mnt | grep fat32</userinput>
Filesystem           Size Used Avail Capacity Mounted on
/dev/zvol/tank/fat32 249M  24k  249M     0%   /mnt
&prompt.root; <userinput>mount | grep fat32</userinput>
/dev/zvol/tank/fat32 on /mnt (msdosfs, local)</screen>

      <para>Destroying a volume is much the same as destroying a
	regular file system dataset.  The operation is nearly
	instantaneous, but it may take several minutes for the free
	space to be reclaimed in the background.</para>
    </sect2>

    <sect2 xml:id="zfs-zfs-rename">
      <title>Renaming a Dataset</title>

      <para>The name of a dataset can be changed with
	<command>zfs rename</command>.  <command>rename</command> can
	also be used to change the parent of a dataset.  Renaming a
	dataset to be under a different parent dataset will change the
	value of those properties that are inherited by the child
	dataset.  When a dataset is renamed, it is unmounted and then
	remounted in the new location (inherited from the parent
	dataset).  This behavior can be prevented with
	<option>-u</option>.  Due to the nature of snapshots, they
	cannot be renamed outside of the parent dataset.  To rename a
	recursive snapshot, specify <option>-r</option>, and all
	snapshots with the same specified snapshot will be
	renamed.</para>
    </sect2>

    <sect2 xml:id="zfs-zfs-set">
      <title>Setting Dataset Properties</title>

      <para>Each <acronym>ZFS</acronym> dataset has a number of
	properties that control its behavior.  Most properties are
	automatically inherited from the parent dataset, but can be
	overridden locally.  Set a property on a dataset with
	<command>zfs set
	  <replaceable>property</replaceable>=<replaceable>value</replaceable>
	  <replaceable>dataset</replaceable></command>.  Most
	properties have a limited set of valid values,
	<command>zfs get</command> will display each possible property
	and its valid values.  Most properties can be reverted to
	their inherited values using
	<command>zfs inherit</command>.</para>

      <para>It is possible to set user-defined properties.  They
	become part of the dataset configuration and can be used to
	provide additional information about the dataset or its
	contents.  To distinguish these custom properties from the
	ones supplied as part of <acronym>ZFS</acronym>, a colon
	(<literal>:</literal>) is used to create a custom namespace
	for the property.</para>

      <screen>&prompt.root; <userinput>zfs set <replaceable>custom</replaceable>:<replaceable>costcenter</replaceable>=<replaceable>1234</replaceable> <replaceable>tank</replaceable></userinput>
&prompt.root; <userinput>zfs get <replaceable>custom</replaceable>:<replaceable>costcenter</replaceable> <replaceable>tank</replaceable></userinput>
NAME PROPERTY           VALUE SOURCE
tank custom:costcenter  1234  local</screen>

      <para>To remove a custom property, use
	<command>zfs inherit</command> with <option>-r</option>.  If
	the custom property is not defined in any of the parent
	datasets, it will be removed completely (although the changes
	are still recorded in the pool's history).</para>

      <screen>&prompt.root; <userinput>zfs inherit -r <replaceable>custom</replaceable>:<replaceable>costcenter</replaceable> <replaceable>tank</replaceable></userinput>
&prompt.root; <userinput>zfs get <replaceable>custom</replaceable>:<replaceable>costcenter</replaceable> <replaceable>tank</replaceable></userinput>
NAME    PROPERTY           VALUE              SOURCE
tank    custom:costcenter  -                  -
&prompt.root; <userinput>zfs get all <replaceable>tank</replaceable> | grep <replaceable>custom</replaceable>:<replaceable>costcenter</replaceable></userinput>
&prompt.root;</screen>
    </sect2>

    <sect2 xml:id="zfs-zfs-snapshot">
      <title>Managing Snapshots</title>

      <para><link linkend="zfs-term-snapshot">Snapshots</link> are one
	of the most powerful features of <acronym>ZFS</acronym>.  A
	snapshot provides a point-in-time copy of the dataset.  The
	parent dataset can be easily rolled back to that snapshot
	state.  Create a snapshot with <command>zfs snapshot
	  <replaceable>dataset</replaceable>@<replaceable>snapshotname</replaceable></command>.
	Adding <option>-r</option> creates a snapshot recursively,
	with the same name on all child datasets.</para>

      <para>Snapshots are mounted in a hidden directory
	under the parent dataset: <filename
	  class="directory">.zfs/snapshots/<replaceable>snapshotname</replaceable></filename>.
	Individual files can easily be restored to a previous state by
	copying them from the snapshot back to the parent dataset.  It
	is also possible to revert the entire dataset back to the
	point-in-time of the snapshot using
	<command>zfs rollback</command>.</para>

      <para>Snapshots consume space based on how much the parent file
	system has changed since the time of the snapshot.  The
	<literal>written</literal> property of a snapshot tracks how
	much space is being used by the snapshot.</para>

      <para>Snapshots are destroyed and the space reclaimed with
	<command>zfs destroy
	  <replaceable>dataset</replaceable>@<replaceable>snapshot</replaceable></command>.
	Adding <option>-r</option> recursively removes all
	snapshots with the same name under the parent dataset.  Adding
	<option>-n -v</option> to the command
	displays a list of the snapshots that would be deleted and
	an estimate of how much space would be reclaimed without
	performing the actual destroy operation.</para>
    </sect2>

    <sect2 xml:id="zfs-zfs-clones">
      <title>Managing Clones</title>

      <para>A clone is a copy of a snapshot that is treated more like
	a regular dataset.  Unlike a snapshot, a clone is not read
	only, is mounted, and can have its own properties.  Once a
	clone has been created, the snapshot it was created from
	cannot be destroyed.  The child/parent relationship between
	the clone and the snapshot can be reversed using
	<command>zfs promote</command>.  After a clone has been
	promoted, the snapshot becomes a child of the clone, rather
	than of the original parent dataset.  This will change how the
	space is accounted, but not actually change the amount of
	space consumed.</para>
    </sect2>

    <sect2 xml:id="zfs-zfs-send">
      <title>ZFS Replication</title>

      <para>Keeping data on a single pool in one location exposes
	it to risks like theft, natural and human disasters.  Keeping
	regular backups of the entire pool is vital when data needs to
	be restored.  <acronym>ZFS</acronym> provides a built-in
	serialization feature that can send a stream representation of
	the data to standard output.  Using this technique, it is
	possible to not only store the data on another pool connected
	to the local system, but also to send it over a network to
	another system that runs ZFS.  To achieve this replication,
	<acronym>ZFS</acronym> uses filesystem snapshots (see the
	section on <link
	  linkend="zfs-zfs-snapshot">ZFS snapshots</link>) to send
	them from one location to another.  The commands for this
	operation are <command>zfs send</command> and
	<command>zfs receive</command>, respectively.</para>

      <para>The following examples will demonstrate the functionality
	of <acronym>ZFS</acronym> replication using these two
	pools:</para>

      <screen>&prompt.root; <command>zpool list</command>
NAME    SIZE  ALLOC   FREE    CAP  DEDUP  HEALTH  ALTROOT
backup  960M    77K   896M     0%  1.00x  ONLINE  -
mypool  984M  43.7M   940M     4%  1.00x  ONLINE  -</screen>

      <para>The pool named <replaceable>mypool</replaceable> is the
	primary pool where data is written to and read from on a
	regular basis.  A second pool,
	<replaceable>backup</replaceable> is used as a standby in case
	the primary pool becomes unavailable.  Note that this
	fail-over is not done automatically by <acronym>ZFS</acronym>,
	but rather must be done by a system administrator in the event
	that it is needed.  Replication requires a snapshot to provide
	a consistent version of the file system to be transmitted.
	Once a snapshot of <replaceable>mypool</replaceable> has been
	created it can be copied to the
	<replaceable>backup</replaceable> pool.
	<acronym>ZFS</acronym> only replicates snapshots, changes
	since the most recent snapshot will not be replicated.</para>

      <screen>&prompt.root; <command>zfs snapshot <replaceable>mypool</replaceable>@<replaceable>backup1</replaceable></command>
&prompt.root; <command>zfs list -t snapshot</command>
NAME                    USED  AVAIL  REFER  MOUNTPOINT
mypool@backup1             0      -  43.6M  -</screen>

      <para>Now that a snapshot exists, <command>zfs send</command>
	can be used to create a stream representing the contents of
	the snapshot, which can be stored as a file, or received by
	another pool.  The stream will be written to standard
	output, which will need to be redirected to a file or pipe
	otherwise <acronym>ZFS</acronym> will produce an error:</para>

      <screen>&prompt.root; <command>zfs send <replaceable>mypool</replaceable>@<replaceable>backup1</replaceable></command>
Error: Stream can not be written to a terminal.
You must redirect standard output.</screen>

      <para>To backup a dataset with <command>zfs send</command>,
	redirect to a file located on the mounted backup pool.  First
	ensure that the pool has enough free space to accommodate the
	size of the snapshot you are sending, which means all of the
	data contained in the snapshot, not only the changes in that
	snapshot.</para>

      <screen>&prompt.root; <command>zfs send <replaceable>mypool</replaceable>@<replaceable>backup1</replaceable> > <replaceable>/backup/backup1</replaceable></command>
&prompt.root; <command>zpool list</command>
NAME    SIZE  ALLOC   FREE    CAP  DEDUP  HEALTH  ALTROOT
backup  960M  63.7M   896M     6%  1.00x  ONLINE  -
mypool  984M  43.7M   940M     4%  1.00x  ONLINE  -</screen>

      <para>The <command>zfs send</command> transferred all the data
	in the snapshot called <replaceable>backup1</replaceable> to
	the pool named <replaceable>backup</replaceable>.  Creating
	and sending these snapshots could be done automatically with a
	&man.cron.8; job.</para>

      <para>Instead of storing the backups as archive files,
	<acronym>ZFS</acronym> can receive them as a live file system,
	allowing the backed up data to be accessed directly.
	To get to the actual data contained in those streams, the
	reverse operation of <command>zfs send</command> must be used
	to transform the streams back into files and directories.  The
	command is <command>zfs receive</command>.  The example below
	combines <command>zfs send</command> and
	<command>zfs receive</command> using a pipe to copy the data
	from one pool to another.  This way, the data can be used
	directly on the receiving pool after the transfer is complete.
	A dataset can only be replicated to an empty dataset.</para>

      <screen>&prompt.root; <command>zfs snapshot <replaceable>mypool</replaceable>@<replaceable>replica1</replaceable></command>
&prompt.root; <command>zfs send -v <replaceable>mypool</replaceable>@<replaceable>replica1</replaceable> | zfs receive <replaceable>backup/mypool</replaceable></command>
send from @ to mypool@replica1 estimated size is 50.1M
total estimated size is 50.1M
TIME        SENT   SNAPSHOT

&prompt.root; <command>zpool list</command>
NAME    SIZE  ALLOC   FREE    CAP  DEDUP  HEALTH  ALTROOT
backup  960M  63.7M   896M     6%  1.00x  ONLINE  -
mypool  984M  43.7M   940M     4%  1.00x  ONLINE  -</screen>

      <sect3 xml:id="zfs-send-incremental">
	<title>ZFS Incremental Backups</title>

	<para>Another feature of <command>zfs send</command> is that
	  it can determine the difference between two snapshots to
	  only send what has changed between the two.  This results in
	  saving disk space and time for the transfer to another pool.
	  For example:</para>

	<screen>&prompt.root; <userinput>zfs snapshot <replaceable>mypool</replaceable>@<replaceable>backup2</replaceable></userinput>
&prompt.root; <userinput>zfs list -t snapshot</userinput>
NAME                    USED  AVAIL  REFER  MOUNTPOINT
mypool@backup1         5.72M      -  43.6M  -
mypool@backup2             0      -  44.1M  -
&prompt.root; <userinput>zpool list</userinput>
NAME    SIZE  ALLOC   FREE    CAP  DEDUP  HEALTH  ALTROOT
backup  960M  61.7M   898M     6%  1.00x  ONLINE  -
mypool  960M  50.2M   910M     5%  1.00x  ONLINE  -</screen>

	<para>A second snapshot called
	  <replaceable>backup2</replaceable> was created.  This second
	  snapshot contains only the changes on the ZFS filesystem
	  between now and the last snapshot,
	  <replaceable>backup1</replaceable>.  Using the
	  <literal>-i</literal> flag to <command>zfs send</command>
	  and providing both snapshots, an incremental snapshot can be
	  transferred, containing only the data that has
	  changed.</para>

	<screen>&prompt.root; <userinput>zfs send -i <replaceable>mypool</replaceable>@<replaceable>backup1</replaceable> <replaceable>mypool</replaceable>@<replaceable>backup2</replaceable> > <replaceable>/backup/incremental</replaceable></userinput>
&prompt.root; <userinput>zpool list</userinput>
NAME    SIZE  ALLOC   FREE    CAP  DEDUP  HEALTH  ALTROOT
backup  960M  80.8M   879M     8%  1.00x  ONLINE  -
mypool  960M  50.2M   910M     5%  1.00x  ONLINE  -
&prompt.root; <userinput>ls -lh /backup</userinput>
total 82247
drwxr-xr-x     1 root   wheel      61M Dec  3 11:36 backup1
drwxr-xr-x     1 root   wheel      18M Dec  3 11:36 incremental</screen>

	<para>The incremental stream was successfully transferred and
	  the file on disk is smaller than any of the two snapshots
	  <replaceable>backup1</replaceable> or
	  <replaceable>backup2</replaceable>.  This shows that it only
	  contains the differences, which is much faster to transfer
	  and saves disk space by not copying the complete pool each
	  time.  This is useful when having to rely on slow networks
	  or when costs per transferred byte have to be
	  considered.</para>
      </sect3>

      <sect3 xml:id="zfs-send-recv">
	<title>Receiving ZFS Data Streams</title>

	<para>Up until now, only the data streams in binary form were
	  sent to other pools.  To get to the actual data contained in
	  those streams, the reverse operation of <command>zfs
	    send</command> has to be used to transform the streams
	  back into files and directories.  The command is called
	  <command>zfs receive</command> and has also a short version:
	  <command>zfs recv</command>.  The example below combines
	  <command>zfs send</command> and <command>zfs
	    receive</command> using a pipe to copy the data from one
	  pool to another.  This way, the data can be used directly on
	  the receiving pool after the transfer is complete.</para>

	<screen>&prompt.root; <userinput>zfs send <replaceable>mypool</replaceable>@<replaceable>backup1</replaceable> | zfs receive <replaceable>backup/backup1</replaceable></userinput>
&prompt.root; <userinput>ls -lh /backup</userinput>
total 431
drwxr-xr-x     4219 root   wheel      4.1k Dec  3 11:34 backup1</screen>

	<para>The directory <replaceable>backup1</replaceable> does
	  contain all the data, which were part of the snapshot of the
	  same name.  Since this originally was a complete filesystem
	  snapshot, the listing of all ZFS filesystems for this pool
	  is also updated and shows the
	  <replaceable>backup1</replaceable> entry.</para>

	<screen>&prompt.root; <userinput>zfs list</userinput>
NAME                    USED  AVAIL  REFER  MOUNTPOINT
backup                 43.7M   884M    32K  /backup
backup/backup1         43.5M   884M  43.5M  /backup/backup1
mypool                 50.0M   878M  44.1M  /mypool</screen>

	<para>A new filesystem, <replaceable>backup1</replaceable> is
	  available and has the same size as the snapshot it was
	  created from.  It is up to the user to decide whether the
	  streams should be transformed back into filesystems directly
	  to have a cold-standby for emergencies or to just keep the
	  streams and transform them later when required.  Sending and
	  receiving can be automated so that regular backups are
	  created on a second pool for backup purposes.</para>
      </sect3>

      <sect3 xml:id="zfs-send-ssh">
	<title>Sending Encrypted Backups over SSH</title>

	<para>Although sending streams to another system over the
	  network is a good way to keep a remote backup, it does come
	  with a drawback.  All the data sent over the network link is
	  not encrypted, allowing anyone to intercept and transform
	  the streams back into data without the knowledge of the
	  sending user.  This is an unacceptable situation, especially
	  when sending the streams over the internet to a remote host
	  with multiple hops in between where such malicious data
	  collection can occur.  Fortunately, there is a solution
	  available to the problem that does not require the
	  encryption of the data on the pool itself.  To make sure the
	  network connection between both systems is securely
	  encrypted, <application>SSH</application> can be used.
	  Since ZFS only requires the stream to be redirected from
	  standard output, it is relatively easy to pipe it through
	  SSH.</para>

	<para>A few settings and security precautions have to be made
	  before this can be done.  Since this chapter is about ZFS
	  and not about configuring SSH, it only lists the things
	  required to perform the encrypted <command>zfs
	  send</command> operation.  The following settings should
	  be made:</para>

	<itemizedlist>
	  <listitem>
	    <para>Passwordless SSH access between sending and
	      receiving host using SSH keys</para>
	  </listitem>

	  <listitem>
	    <para>The <systemitem class="username">root</systemitem>
	      user needs to be able to log into the receiving system
	      because only that user can send streams from the pool.
	      <application>SSH</application> should be configured so
	      that <systemitem class="username">root</systemitem> can
	      only execute <command>zfs recv</command> and nothing
	      else to prevent users that might have hijacked this
	      account from doing any harm on the system.</para>
	  </listitem>
	</itemizedlist>

	<para>After these security measures have been put into place
	  and <systemitem class="username">root</systemitem> can
	  connect via passwordless <application>SSH</application> to
	  the receiving system, the encrypted stream can be sent using
	  the following commands:</para>

	<screen>&prompt.root; <userinput>zfs snapshot -r <replaceable>mypool/home</replaceable>@<replaceable>monday</replaceable></userinput>
&prompt.root; <userinput>zfs send -R <replaceable>mypool/home</replaceable>@<replaceable>monday</replaceable> | ssh <replaceable>backuphost</replaceable> zfs recv -dvu <replaceable>backuppool</replaceable></userinput></screen>

	<para>The first command creates a recursive snapshot (option
	  <literal>-r</literal>) called
	  <replaceable>monday</replaceable> of the filesystem named
	  <replaceable>home</replaceable> that resides on the pool
	  <replaceable>mypool</replaceable>.  The second command uses
	  the <literal>-R</literal> option to <command>zfs
	    send</command>, which makes sure that all datasets and
	  filesystems along with their children are included in the
	  transmission of the data stream.  This also includes
	  snaphots, clones and settings on individual filesystems as
	  well.  The output is piped directly to SSH that uses a short
	  name for the receiving host called
	  <replaceable>backuphost</replaceable>.  A fully qualified
	  domain name or IP address can also be used here.  The SSH
	  command to execute is <command>zfs recv</command> to a pool
	  called <replaceable>backuppool</replaceable>.  Using the
	  <literal>-d</literal> option with <command>zfs
	    recv</command> will remove the original name of the pool
	  on the receiving side and just takes the name of the
	  snapshot instead.  The <literal>-u</literal> option makes
	  sure that the filesystem is not mounted on the receiving
	  side.  More information about the transfer&mdash;like the
	  time that has passed&mdash;is displayed when the
	  <literal>-v</literal> option is provided.</para>
      </sect3>
    </sect2>

    <sect2 xml:id="zfs-zfs-quota">
      <title>Dataset, User and Group Quotas</title>

      <para><link linkend="zfs-term-quota">Dataset quotas</link> are
	used to restrict the amount of space that can be consumed
	by a particular dataset.
	<link linkend="zfs-term-refquota">Reference Quotas</link> work
	in very much the same way, but only count the space
	used by the dataset itself, excluding snapshots and child
	datasets.  Similarly,
	<link linkend="zfs-term-userquota">user</link> and
	<link linkend="zfs-term-groupquota">group</link> quotas can be
	used to prevent users or groups from using all of the
	space in the pool or dataset.</para>

      <para>To enforce a dataset quota of 10&nbsp;GB for
	<filename>storage/home/bob</filename>, use the
	following:</para>

      <screen>&prompt.root; <userinput>zfs set quota=10G storage/home/bob</userinput></screen>

      <para>To enforce a reference quota of 10&nbsp;GB for
	<filename>storage/home/bob</filename>, use the
	following:</para>

      <screen>&prompt.root; <userinput>zfs set refquota=10G storage/home/bob</userinput></screen>

      <para>The general format is
	<literal>userquota@<replaceable>user</replaceable>=<replaceable>size</replaceable></literal>,
	and the user's name must be in one of the following
	formats:</para>

      <itemizedlist>
	<listitem>
	  <para><acronym>POSIX</acronym> compatible name such as
	    <replaceable>joe</replaceable>.</para>
	</listitem>

	<listitem>
	  <para><acronym>POSIX</acronym> numeric ID such as
	    <replaceable>789</replaceable>.</para>
	</listitem>

	<listitem>
	  <para><acronym>SID</acronym> name
	    such as
	    <replaceable>joe.bloggs@example.com</replaceable>.</para>
	</listitem>

	<listitem>
	  <para><acronym>SID</acronym>
	    numeric ID such as
	    <replaceable>S-1-123-456-789</replaceable>.</para>
	</listitem>
      </itemizedlist>

      <para>For example, to enforce a user quota of 50&nbsp;GB for the
	user named <replaceable>joe</replaceable>:</para>

      <screen>&prompt.root; <userinput>zfs set userquota@joe=50G</userinput></screen>

      <para>To remove any quota:</para>

      <screen>&prompt.root; <userinput>zfs set userquota@joe=none</userinput></screen>

      <note>
	<para>User quota properties are not displayed by
	  <command>zfs get all</command>.
	  Non-<systemitem class="username">root</systemitem> users can
	  only see their own quotas unless they have been granted the
	  <literal>userquota</literal> privilege.  Users with this
	  privilege are able to view and set everyone's quota.</para>
      </note>

      <para>The general format for setting a group quota is:
	<literal>groupquota@<replaceable>group</replaceable>=<replaceable>size</replaceable></literal>.</para>

      <para>To set the quota for the group
	<replaceable>firstgroup</replaceable> to 50&nbsp;GB,
	use:</para>

      <screen>&prompt.root; <userinput>zfs set groupquota@firstgroup=50G</userinput></screen>

      <para>To remove the quota for the group
	<replaceable>firstgroup</replaceable>, or to make sure that
	one is not set, instead use:</para>

      <screen>&prompt.root; <userinput>zfs set groupquota@firstgroup=none</userinput></screen>

      <para>As with the user quota property,
	non-<systemitem class="username">root</systemitem> users can
	only see the quotas associated with the groups that they
	belong to.  However,
	<systemitem class="username">root</systemitem> or a user with
	the <literal>groupquota</literal> privilege can view and set
	all quotas for all groups.</para>

      <para>To display the amount of space used by each user on
	a filesystem or snapshot, along with any specified
	quotas, use <command>zfs userspace</command>.  For group
	information, use <command>zfs groupspace</command>.  For more
	information about supported options or how to display only
	specific options, refer to &man.zfs.1;.</para>

      <para>Users with sufficient privileges and
	<systemitem class="username">root</systemitem> can list the
	quota for <filename>storage/home/bob</filename> using:</para>

      <screen>&prompt.root; <userinput>zfs get quota storage/home/bob</userinput></screen>
    </sect2>

    <sect2 xml:id="zfs-zfs-reservation">
      <title>Reservations</title>

      <para><link linkend="zfs-term-reservation">Reservations</link>
	guarantee a minimum amount of space will always be available
	on a dataset.  The reserved space will not be available to any
	other dataset.  This feature can be especially useful to
	ensure that free space is available
	for an important dataset or log files.</para>

      <para>The general format of the <literal>reservation</literal>
	property is
	<literal>reservation=<replaceable>size</replaceable></literal>,
	so to set a reservation of 10&nbsp;GB on
	<filename>storage/home/bob</filename>, use:</para>

      <screen>&prompt.root; <userinput>zfs set reservation=10G storage/home/bob</userinput></screen>

      <para>To clear any reservation:</para>

      <screen>&prompt.root; <userinput>zfs set reservation=none storage/home/bob</userinput></screen>

      <para>The same principle can be applied to the
	<literal>refreservation</literal> property for setting a
	<link linkend="zfs-term-refreservation">Reference
	  Reservation</link>, with the general format
	<literal>refreservation=<replaceable>size</replaceable></literal>.</para>

      <para>This command shows any reservations or refreservations
	that exist on <filename>storage/home/bob</filename>:</para>

      <screen>&prompt.root; <userinput>zfs get reservation storage/home/bob</userinput>
&prompt.root; <userinput>zfs get refreservation storage/home/bob</userinput></screen>
    </sect2>

    <sect2 xml:id="zfs-zfs-compression">
      <title>Compression</title>

      <para></para>
    </sect2>

    <sect2 xml:id="zfs-zfs-deduplication">
      <title>Deduplication</title>

      <para>When enabled,
	<link linkend="zfs-term-deduplication">Deduplication</link>
	uses the checksum of each block to detect duplicate blocks.
	When a new block is a duplicate of an existing block,
	<acronym>ZFS</acronym> writes an additional reference to the
	existing data instead of the whole duplicate block.
	Tremendous space savings are possible if the data contains
	many duplicated files or repeated information.  Be warned:
	deduplication requires an extremely large amount of memory,
	and most of the space savings can be had without the extra
	cost by enabling compression instead.</para>

      <para>To activate deduplication, set the
	<literal>dedup</literal> property on the target pool:</para>

      <screen>&prompt.root; <userinput>zfs set dedup=on <replaceable>pool</replaceable></userinput></screen>

      <para>Only new data being written to the pool will be
	deduplicated.  Data that has already been written to the pool
	will not be deduplicated merely by activating this option.  As
	such, a pool with a freshly activated deduplication property
	will look something like this example:</para>

      <screen>&prompt.root; <userinput>zpool list</userinput>
NAME  SIZE ALLOC  FREE CAP DEDUP HEALTH ALTROOT
pool 2.84G 2.19M 2.83G  0% 1.00x ONLINE -</screen>

      <para>The <literal>DEDUP</literal> column shows the actual rate
	of deduplication for the pool.  A value of
	<literal>1.00x</literal> shows that data has not been
	deduplicated yet.  In the next example, the ports tree is
	copied three times into different directories on the
	deduplicated pool created above.</para>

      <screen>&prompt.root; <userinput>zpool list</userinput>
for d in dir1 dir2 dir3; do
for> mkdir $d &amp;&amp; cp -R /usr/ports $d &amp;
for> done</screen>

      <para>Redundant data is detected and deduplicated:</para>

      <screen>&prompt.root; <userinput>zpool list</userinput>
NAME SIZE  ALLOC FREE CAP DEDUP HEALTH ALTROOT
pool 2.84G 20.9M 2.82G 0% 3.00x ONLINE -</screen>

      <para>The <literal>DEDUP</literal> column now shows a factor of
	<literal>3.00x</literal>.  The multiple copies of the ports
	tree data was detected and deduplicated, taking only a third
	of the space.  The potential for space savings can be
	enormous, but comes at the cost of having enough memory to
	keep track of the deduplicated blocks.</para>

      <para>Deduplication is not always beneficial, especially when
	there is not much redundant data on a pool.
	<acronym>ZFS</acronym> can show potential space savings by
	simulating deduplication on an existing pool:</para>

      <screen>&prompt.root; <userinput>zdb -S <replaceable>pool</replaceable></userinput>
Simulated DDT histogram:

bucket              allocated                       referenced
______   ______________________________   ______________________________
refcnt   blocks   LSIZE   PSIZE   DSIZE   blocks   LSIZE   PSIZE   DSIZE
------   ------   -----   -----   -----   ------   -----   -----   -----
     1    2.58M    289G    264G    264G    2.58M    289G    264G    264G
     2     206K   12.6G   10.4G   10.4G     430K   26.4G   21.6G   21.6G
     4    37.6K    692M    276M    276M     170K   3.04G   1.26G   1.26G
     8    2.18K   45.2M   19.4M   19.4M    20.0K    425M    176M    176M
    16      174   2.83M   1.20M   1.20M    3.33K   48.4M   20.4M   20.4M
    32       40   2.17M    222K    222K    1.70K   97.2M   9.91M   9.91M
    64        9     56K   10.5K   10.5K      865   4.96M    948K    948K
   128        2   9.50K      2K      2K      419   2.11M    438K    438K
   256        5   61.5K     12K     12K    1.90K   23.0M   4.47M   4.47M
    1K        2      1K      1K      1K    2.98K   1.49M   1.49M   1.49M
 Total    2.82M    303G    275G    275G    3.20M    319G    287G    287G

dedup = 1.05, compress = 1.11, copies = 1.00, dedup * compress / copies = 1.16</screen>

      <para>After <command>zdb -S</command> finishes analyzing the
	pool, it shows the space reduction ratio that would be
	achieved by activating deduplication.  In this case,
	<literal>1.16</literal> is a very poor ratio that is mostly
	influenced by compression.  Activating deduplication on this
	pool would not save any significant amount of space.  Using
	the formula <emphasis>dedup * compress / copies =
	deduplication ratio</emphasis>, system administrators can plan
	the storage allocation more towards having multiple copies of
	data or by having a decent compression rate in order to
	utilize the space savings that deduplication provides.  As a
	rule of thumb, compression should be used before deduplication
	due to the much lower memory requirements.</para>
    </sect2>

    <sect2 xml:id="zfs-zfs-jail">
      <title>ZFS and Jails</title>

      <para><command>zfs jail</command> and the corresponding
	<literal>jailed</literal> property are used to delegate a
	<acronym>ZFS</acronym> dataset to a
	<link linkend="jails">Jail</link>.
	<command>zfs jail <replaceable>jailid</replaceable></command>
	attaches a dataset to the specified jail, and
	<command>zfs unjail</command> detaches it.  For the dataset to
	be administered from within a jail, the
	<literal>jailed</literal> property must be set.  Once a
	dataset is jailed, it can no longer be mounted on the host
	because the jail administrator may have set unacceptable mount
	points.</para>
    </sect2>
  </sect1>

  <sect1 xml:id="zfs-zfs-allow">
    <title>Delegated Administration</title>

    <para>A comprehensive permission delegation system allows
      unprivileged users to perform <acronym>ZFS</acronym>
      administration functions.  For example, if each user's home
      directory is a dataset, users can be given permission to create
      and destroy snapshots of their home directories.  A backup user
      can be given permission to use <acronym>ZFS</acronym>
      replication features.  A usage statistics script can be allowed
      to run with access only to the space utilization data for all
      users.  It is even possible to delegate the ability to delegate
      permissions.  Permission delegation is possible for each
      subcommand and most <acronym>ZFS</acronym> properties.</para>

    <sect2 xml:id="zfs-zfs-allow-create">
      <title>Delegating Dataset Creation</title>

      <para><command>zfs allow
	  <replaceable>someuser</replaceable> create
	  <replaceable>mydataset</replaceable></command> gives the
	specified user permission to create child datasets under the
	selected parent dataset.  There is a caveat: creating a new
	dataset involves mounting it.  That requires setting the
	<literal>vfs.usermount</literal> &man.sysctl.8; to
	<literal>1</literal> to allow non-root users to mount a
	filesystem.  There is another restriction aimed at preventing
	abuse: non-root users must own the mountpoint where the file
	system is being mounted.</para>
    </sect2>

    <sect2 xml:id="zfs-zfs-allow-allow">
      <title>Delegating Permission Delegation</title>

      <para><command>zfs allow
	  <replaceable>someuser</replaceable> allow
	  <replaceable>mydataset</replaceable></command> gives the
	specified user the ability to assign any permission they have
	on the target dataset (or its children) to other users.  If a
	user has the <literal>snapshot</literal> permission and the
	<literal>allow</literal> permission, that user can then grant
	the <literal>snapshot</literal> permission to some other
	users.</para>
    </sect2>
  </sect1>

  <sect1 xml:id="zfs-advanced">
    <title>ZFS Advanced Topics</title>

    <sect2 xml:id="zfs-advanced-tuning">
      <title>ZFS Tuning</title>

      <para></para>
    </sect2>

    <sect2 xml:id="zfs-advanced-booting">
      <title>Booting Root on ZFS</title>

      <para></para>
    </sect2>

    <sect2 xml:id="zfs-advanced-beadm">
      <title>ZFS Boot Environments</title>

      <para></para>
    </sect2>

    <sect2 xml:id="zfs-advanced-troubleshoot">
      <title>Troubleshooting</title>

      <para></para>
    </sect2>

    <sect2 xml:id="zfs-advanced-i386">
      <title>ZFS on i386</title>

      <para>Some of the features provided by <acronym>ZFS</acronym>
	are memory intensive, and may require tuning for maximum
	efficiency on systems with limited
	<acronym>RAM</acronym>.</para>

      <sect3>
	<title>Memory</title>

	<para>As a bare minimum, the total system memory should be at
	  least one gigabyte.  The amount of recommended
	  <acronym>RAM</acronym> depends upon the size of the pool and
	  which <acronym>ZFS</acronym> features are used.  A general
	  rule of thumb is 1&nbsp;GB of RAM for every 1&nbsp;TB of
	  storage.  If the deduplication feature is used, a general
	  rule of thumb is 5&nbsp;GB of RAM per TB of storage to be
	  deduplicated.  While some users successfully use
	  <acronym>ZFS</acronym> with less <acronym>RAM</acronym>,
	  systems under heavy load may panic due to memory exhaustion.
	  Further tuning may be required for systems with less than
	  the recommended RAM requirements.</para>
      </sect3>

      <sect3>
	<title>Kernel Configuration</title>

	<para>Due to the address space limitations of the
	  &i386; platform, <acronym>ZFS</acronym> users on the
	  &i386; architecture should add this option to a
	  custom kernel configuration file, rebuild the kernel, and
	  reboot:</para>

	<programlisting>options        KVA_PAGES=512</programlisting>

	<para>This expands the kernel address space, allowing
	  the <varname>vm.kvm_size</varname> tunable to be pushed
	  beyond the currently imposed limit of 1&nbsp;GB, or the
	  limit of 2&nbsp;GB for <acronym>PAE</acronym>.  To find the
	  most suitable value for this option, divide the desired
	  address space in megabytes by four.  In this example, it
	  is <literal>512</literal> for 2&nbsp;GB.</para>
      </sect3>

      <sect3>
	<title>Loader Tunables</title>

	<para>The <filename>kmem</filename> address space can be
	  increased on all &os; architectures.  On a test system with
	  1&nbsp;GB of physical memory, success was achieved with
	  these options added to
	  <filename>/boot/loader.conf</filename>, and the system
	  restarted:</para>

	<programlisting>vm.kmem_size="330M"
vm.kmem_size_max="330M"
vfs.zfs.arc_max="40M"
vfs.zfs.vdev.cache.size="5M"</programlisting>

	<para>For a more detailed list of recommendations for
	  <acronym>ZFS</acronym>-related tuning, see <link
	    xlink:href="http://wiki.freebsd.org/ZFSTuningGuide"></link>.</para>
      </sect3>
    </sect2>
  </sect1>

  <sect1 xml:id="zfs-links">
    <title>Additional Resources</title>

    <itemizedlist>
      <listitem>
	<para><link xlink:href="https://wiki.freebsd.org/ZFS">FreeBSD
	    Wiki - ZFS</link></para>
      </listitem>

      <listitem>
	<para><link
	    xlink:href="https://wiki.freebsd.org/ZFSTuningGuide">FreeBSD
	    Wiki - ZFS Tuning</link></para>
      </listitem>

      <listitem>
	<para><link
	    xlink:href="http://wiki.illumos.org/display/illumos/ZFS">Illumos
	    Wiki - ZFS</link></para>
      </listitem>

      <listitem>
	<para><link
	    xlink:href="http://docs.oracle.com/cd/E19253-01/819-5461/index.html">Oracle
	    Solaris ZFS Administration Guide</link></para>
      </listitem>

      <listitem>
	<para><link
	    xlink:href="http://www.solarisinternals.com/wiki/index.php/ZFS_Evil_Tuning_Guide">ZFS
	    Evil Tuning Guide</link></para>
      </listitem>

      <listitem>
	<para><link
	    xlink:href="http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide">ZFS
	    Best Practices Guide</link></para>
      </listitem>
    </itemizedlist>

    <sect2 xml:id="zfs-history">
      <title>History of <acronym>ZFS</acronym></title>

      <para></para>
    </sect2>
  </sect1>

  <sect1 xml:id="zfs-term">
    <title><acronym>ZFS</acronym> Features and Terminology</title>

    <para><acronym>ZFS</acronym> is a fundamentally different file
      system because it is more than just a file system.
      <acronym>ZFS</acronym> combines the roles of file system and
      volume manager, enabling additional storage devices to be added
      to a live system and having the new space available on all of
      the existing file systems in that pool immediately.  By
      combining the traditionally separate roles,
      <acronym>ZFS</acronym> is able to overcome previous limitations
      that prevented <acronym>RAID</acronym> groups being able to
      grow.  Each top level device in a zpool is called a vdev, which
      can be a simple disk or a <acronym>RAID</acronym> transformation
      such as a mirror or <acronym>RAID-Z</acronym> array.
      <acronym>ZFS</acronym> file systems (called datasets), each have
      access to the combined free space of the entire pool.  As blocks
      are allocated from the pool, the space available to each file
      system decreases.  This approach avoids the common pitfall with
      extensive partitioning where free space becomes fragmentated
      across the partitions.</para>

    <informaltable pgwide="1">
      <tgroup cols="2">
	<tbody valign="top">
	  <row>
	    <entry xml:id="zfs-term-zpool">zpool</entry>

	    <entry>A storage pool is the most basic building block of
	      <acronym>ZFS</acronym>.  A pool is made up of one or
	      more vdevs, the underlying devices that store the data.
	      A pool is then used to create one or more file systems
	      (datasets) or block devices (volumes).  These datasets
	      and volumes share the pool of remaining free space.
	      Each pool is uniquely identified by a name and a
	      <acronym>GUID</acronym>.  The zpool also controls the
	      version number and therefore the features available for
	      use with <acronym>ZFS</acronym>.

	      <note>
		<para>&os;&nbsp;9.0 and 9.1 include support for
		  <acronym>ZFS</acronym> version 28.  Future versions
		  use <acronym>ZFS</acronym> version 5000 with feature
		  flags.  This allows greater cross-compatibility with
		  other implementations of
		  <acronym>ZFS</acronym>.</para>
	      </note></entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-vdev">vdev&nbsp;Types</entry>

	    <entry>A zpool is made up of one or more vdevs, which
	      themselves can be a single disk or a group of disks, in
	      the case of a <acronym>RAID</acronym> transform.  When
	      multiple vdevs are used, <acronym>ZFS</acronym> spreads
	      data across the vdevs to increase performance and
	      maximize usable space.

	      <itemizedlist>
		<listitem>
		  <para xml:id="zfs-term-vdev-disk">
		    <emphasis>Disk</emphasis> - The most basic type
		    of vdev is a standard block device.  This can be
		    an entire disk (such as
		    <filename><replaceable>/dev/ada0</replaceable></filename>
		    or
		    <filename><replaceable>/dev/da0</replaceable></filename>)
		    or a partition
		    (<filename><replaceable>/dev/ada0p3</replaceable></filename>).
		    Contrary to the Solaris documentation, on &os;
		    there is no performance penalty for using a
		    partition rather than an entire disk.</para>
		</listitem>

		<listitem>
		  <para xml:id="zfs-term-vdev-file">
		    <emphasis>File</emphasis> - In addition to disks,
		    <acronym>ZFS</acronym> pools can be backed by
		    regular files, this is especially useful for
		    testing and experimentation.  Use the full path to
		    the file as the device path in the zpool create
		    command.  All vdevs must be atleast 128&nbsp;MB in
		    size.</para>
		</listitem>

		<listitem>
		  <para xml:id="zfs-term-vdev-mirror">
		    <emphasis>Mirror</emphasis> - When creating a
		    mirror, specify the <literal>mirror</literal>
		    keyword followed by the list of member devices
		    for the mirror.  A mirror consists of two or
		    more devices, all data will be written to all
		    member devices.  A mirror vdev will only hold as
		    much data as its smallest member.  A mirror vdev
		    can withstand the failure of all but one of its
		    members without losing any data.</para>

		  <note>
		    <para>regular single disk vdev can be upgraded to
		      a mirror vdev at any time using the
		      <command>zpool</command> <link
			linkend="zfs-zpool-attach">attach</link>
		      command.</para>
		  </note>
		</listitem>

		<listitem>
		  <para xml:id="zfs-term-vdev-raidz">
		    <emphasis><acronym>RAID-Z</acronym></emphasis> -
		    <acronym>ZFS</acronym> implements
		    <acronym>RAID-Z</acronym>, a variation on standard
		    <acronym>RAID-5</acronym> that offers better
		    distribution of parity and eliminates the
		    "<acronym>RAID-5</acronym> write hole" in which
		    the data and parity information become
		    inconsistent after an unexpected restart.
		    <acronym>ZFS</acronym> supports 3 levels of
		    <acronym>RAID-Z</acronym> which provide varying
		    levels of redundancy in exchange for decreasing
		    levels of usable storage.  The types are named
		    <acronym>RAID-Z1</acronym> through
		    <acronym>RAID-Z3</acronym> based on the number of
		    parity devices in the array and the number of
		    disks which can fail while the pool remains
		    operational.</para>

		  <para>In a <acronym>RAID-Z1</acronym> configuration
		    with four disks, each 1&nbsp;TB, usable storage is
		    3&nbsp;TB and the pool will still be able to
		    operate in degraded mode with one faulted disk.
		    If an additional disk goes offline before the
		    faulted disk is replaced and resilvered, all data
		    in the pool can be lost.</para>

		  <para>In a <acronym>RAID-Z3</acronym> configuration
		    with eight disks of 1&nbsp;TB, the volume will
		    provide 5&nbsp;TB of usable space and still be
		    able to operate with three faulted disks.  &sun;
		    recommends no more than nine disks in a single
		    vdev.  If the configuration has more disks, it is
		    recommended to divide them into separate vdevs and
		    the pool data will be striped across them.</para>

		  <para>A configuration of two
		    <acronym>RAID-Z2</acronym> vdevs consisting of 8
		    disks each would create something similar to a
		    <acronym>RAID-60</acronym> array.  A
		    <acronym>RAID-Z</acronym> group's storage capacity
		    is approximately the size of the smallest disk
		    multiplied by the number of non-parity disks.
		    Four 1&nbsp;TB disks in <acronym>RAID-Z1</acronym>
		    has an effective size of approximately 3&nbsp;TB,
		    and an array of eight 1&nbsp;TB disks in
		    <acronym>RAID-Z3</acronym> will yield 5&nbsp;TB of
		    usable space.</para>
		</listitem>

		<listitem>
		  <para xml:id="zfs-term-vdev-spare">
		    <emphasis>Spare</emphasis> -
		    <acronym>ZFS</acronym> has a special pseudo-vdev
		    type for keeping track of available hot spares.
		    Note that installed hot spares are not deployed
		    automatically; they must manually be configured to
		    replace the failed device using
		    <command>zfs replace</command>.</para>
		</listitem>

		<listitem>
		  <para xml:id="zfs-term-vdev-log">
		    <emphasis>Log</emphasis> - <acronym>ZFS</acronym>
		    Log Devices, also known as <acronym>ZFS</acronym>
		    Intent Log (<link
		      linkend="zfs-term-zil"><acronym>ZIL</acronym></link>)
		    move the intent log from the regular pool devices
		    to a dedicated device, typically an
		    <acronym>SSD</acronym>.  Having a dedicated log
		    device can significantly improve the performance
		    of applications with a high volume of synchronous
		    writes, especially databases.  Log devices can be
		    mirrored, but <acronym>RAID-Z</acronym> is not
		    supported.  If multiple log devices are used,
		    writes will be load balanced across them.</para>
		</listitem>

		<listitem>
		  <para xml:id="zfs-term-vdev-cache">
		    <emphasis>Cache</emphasis> - Adding a cache vdev
		    to a zpool will add the storage of the cache to
		    the <link
		      linkend="zfs-term-l2arc"><acronym>L2ARC</acronym></link>.
		    Cache devices cannot be mirrored.  Since a cache
		    device only stores additional copies of existing
		    data, there is no risk of data loss.</para>
		</listitem>
	      </itemizedlist></entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-arc">Adaptive Replacement
	      Cache (<acronym>ARC</acronym>)</entry>

	    <entry><acronym>ZFS</acronym> uses an Adaptive Replacement
	      Cache (<acronym>ARC</acronym>), rather than a more
	      traditional Least Recently Used (<acronym>LRU</acronym>)
	      cache.  An <acronym>LRU</acronym> cache is a simple list
	      of items in the cache sorted by when each object was
	      most recently used; new items are added to the top of
	      the list and once the cache is full items from the
	      bottom of the list are evicted to make room for more
	      active objects.  An <acronym>ARC</acronym> consists of
	      four lists; the Most Recently Used
	      (<acronym>MRU</acronym>) and Most Frequently Used
	      (<acronym>MFU</acronym>) objects, plus a ghost list for
	      each.  These ghost lists track recently evicted objects
	      to prevent them from being added back to the cache.
	      This increases the cache hit ratio by avoiding objects
	      that have a history of only being used occasionally.
	      Another advantage of using both an
	      <acronym>MRU</acronym> and <acronym>MFU</acronym> is
	      that scanning an entire filesystem would normally evict
	      all data from an <acronym>MRU</acronym> or
	      <acronym>LRU</acronym> cache in favor of this freshly
	      accessed content.  In the case of
	      <acronym>ZFS</acronym>, since there is also an
	      <acronym>MFU</acronym> that only tracks the most
	      frequently used objects, the cache of the most commonly
	      accessed blocks remains.</entry>
	  </row>

	  <row>
	    <entry
	      xml:id="zfs-term-l2arc"><acronym>L2ARC</acronym></entry>

	    <entry><acronym>L2ARC</acronym> is the second level
	      of the <acronym>ZFS</acronym> caching system.  The
	      primary <acronym>ARC</acronym> is stored in
	      <acronym>RAM</acronym>.  Since the amount of
	      available <acronym>RAM</acronym> is often limited,
	      <acronym>ZFS</acronym> can also use
	      <link linkend="zfs-term-vdev-cache">cache</link>
	      vdevs.  Solid State Disks (<acronym>SSD</acronym>s) are
	      often used as these cache devices due to their higher
	      speed and lower latency compared to traditional spinning
	      disks.  <acronym>L2ARC</acronym> is entirely
	      optional, but having one will significantly increase
	      read speeds for files that are cached on the
	      <acronym>SSD</acronym> instead of having to be read from
	      the regular spinning disks.  The
	      <acronym>L2ARC</acronym> can also speed up <link
		linkend="zfs-term-deduplication">deduplication</link>
	      since a <acronym>DDT</acronym> that does not fit in
	      <acronym>RAM</acronym> but does fit in the
	      <acronym>L2ARC</acronym> will be much faster than if the
	      <acronym>DDT</acronym> had to be read from disk.  The
	      rate at which data is added to the cache devices is
	      limited to prevent prematurely wearing out the
	      <acronym>SSD</acronym> with too many writes.  Until the
	      cache is full (the first block has been evicted to make
	      room), writing to the <acronym>L2ARC</acronym> is
	      limited to the sum of the write limit and the boost
	      limit, then after that limited to the write limit.  A
	      pair of sysctl values control these rate limits;
	      <literal>vfs.zfs.l2arc_write_max</literal> controls how
	      many bytes are written to the cache per second, while
	      <literal>vfs.zfs.l2arc_write_boost</literal> adds to
	      this limit during the "Turbo Warmup Phase" (Write
	      Boost).</entry>
	  </row>

	  <row>
	    <entry
	      xml:id="zfs-term-zil"><acronym>ZIL</acronym></entry>

	    <entry><acronym>ZIL</acronym> accelerates synchronous
	      transactions by using storage devices (such as
	      <acronym>SSD</acronym>s) that are faster than those used
	      for the main storage pool.  When data is being written
	      and the application requests a synchronous write (a
	      guarantee that the data has been safely stored to disk
	      rather than only cached to be written later), the data
	      is written to the faster <acronym>ZIL</acronym> storage,
	      then later flushed out to the regular disks, greatly
	      reducing the latency and increasing performance.
	      Only workloads that are synchronous such as databases
	      will benefit from a <acronym>ZIL</acronym>.  Regular
	      asynchronous writes such as copying files will not use
	      the <acronym>ZIL</acronym> at all.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-cow">Copy-On-Write</entry>

	    <entry>Unlike a traditional file system, when data is
	      overwritten on <acronym>ZFS</acronym>, the new data is
	      written to a different block rather than overwriting the
	      old data in place.  Only when this write is complete is
	      the metadata then updated to point to the new location.
	      In the event of a shorn write (a system crash or power
	      loss in the middle of writing a file), the entire
	      original contents of the file are still available and
	      the incomplete write is discarded.  This also means that
	      <acronym>ZFS</acronym> does not require a &man.fsck.8;
	      after an unexpected shutdown.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-dataset">Dataset</entry>

	    <entry><emphasis>Dataset</emphasis> is the generic term
	      for a <acronym>ZFS</acronym> file system, volume,
	      snapshot or clone.  Each dataset has a unique name in
	      the format: <literal>poolname/path@snapshot</literal>.
	      The root of the pool is technically a dataset as well.
	      Child datasets are named hierarchically like
	      directories.  For example,
	      <literal>mypool/home</literal>, the home dataset, is a
	      child of <literal>mypool</literal> and inherits
	      properties from it.  This can be expanded further by
	      creating <literal>mypool/home/user</literal>.  This
	      grandchild dataset will inherity properties from the
	      parent and grandparent.  Properties on a child can be
	      set to override the defaults inherited from the parents
	      and grandparents.  Administration of datasets and their
	      children can be
	      <link linkend="zfs-zfs-allow">delegated</link>.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-filesystem">Filesystem</entry>

	    <entry>A <acronym>ZFS</acronym> dataset is most often used
	      as a file system.  Like most other file systems, a
	      <acronym>ZFS</acronym> file system is mounted somewhere
	      in the systems directory heirarchy and contains files
	      and directories of its own with permissions, flags, and
	      other metadata.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-volume">Volume</entry>

	    <entry>In additional to regular file system datasets,
	      <acronym>ZFS</acronym> can also create volumes, which
	      are block devices.  Volumes have many of the same
	      features, including copy-on-write, snapshots, clones and
	      checksumming.  Volumes can be useful for running other
	      file system formats on top of <acronym>ZFS</acronym>,
	      such as <acronym>UFS</acronym> or in the case of
	      Virtualization or exporting <acronym>iSCSI</acronym>
	      extents.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-snapshot">Snapshot</entry>

	    <entry>The
	      <link linkend="zfs-term-cow">copy-on-write</link>
	      (<acronym>COW</acronym>) design of
	      <acronym>ZFS</acronym> allows for nearly instantaneous
	      consistent snapshots with arbitrary names.  After taking
	      a snapshot of a dataset (or a recursive snapshot of a
	      parent dataset that will include all child datasets),
	      new data is written to new blocks (as described above),
	      however the old blocks are not reclaimed as free space.
	      There are then two versions of the file system, the
	      snapshot (what the file system looked like before) and
	      the live file system; however no additional space is
	      used.  As new data is written to the live file system,
	      new blocks are allocated to store this data.  The
	      apparent size of the snapshot will grow as the blocks
	      are no longer used in the live file system, but only in
	      the snapshot.  These snapshots can be mounted (read
	      only) to allow for the recovery of previous versions of
	      files.  It is also possible to
	      <link linkend="zfs-zfs-snapshot">rollback</link> a live
	      file system to a specific snapshot, undoing any changes
	      that took place after the snapshot was taken.  Each
	      block in the zpool has a reference counter which
	      indicates how many snapshots, clones, datasets or
	      volumes make use of that block.  As files and snapshots
	      are deleted, the reference count is decremented; once a
	      block is no longer referenced, it is reclaimed as free
	      space.  Snapshots can also be marked with a
	      <link linkend="zfs-zfs-snapshot">hold</link>, once a
	      snapshot is held, any attempt to destroy it will return
	      an <literal>EBUSY</literal> error.  Each snapshot can
	      have multiple holds, each with a unique name.  The
	      <link linkend="zfs-zfs-snapshot">release</link> command
	      removes the hold so the snapshot can then be deleted.
	      Snapshots can be taken on volumes, however they can only
	      be cloned or rolled back, not mounted
	      independently.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-clone">Clone</entry>

	    <entry>Snapshots can also be cloned; a clone is a writable
	      version of a snapshot, allowing the file system to be
	      forked as a new dataset.  As with a snapshot, a clone
	      initially consumes no additional space, only as new data
	      is written to a clone and new blocks are allocated does
	      the apparent size of the clone grow.  As blocks are
	      overwritten in the cloned file system or volume, the
	      reference count on the previous block is decremented.
	      The snapshot upon which a clone is based cannot be
	      deleted because the clone depends on it (the
	      snapshot is the parent, and the clone is the child).
	      Clones can be <literal>promoted</literal>, reversing
	      this dependency, making the clone the parent and the
	      previous parent the child.  This operation requires no
	      additional space, but it will change the way the
	      used space is accounted.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-checksum">Checksum</entry>

	    <entry>Every block that is allocated is also checksummed
	      (the algorithm used is a per dataset property, see
	      <command>zfs set</command>).  The checksum of each block
	      is transparently validated as it
	      is read, allowing <acronym>ZFS</acronym> to detect
	      silent corruption.  If the data that is read does not
	      match the expected checksum, <acronym>ZFS</acronym> will
	      attempt to recover the data from any available
	      redundancy, like mirrors or <acronym>RAID-Z</acronym>).
	      Validation of all checksums can be triggered with the
	      <link
		linkend="zfs-term-scrub"><command>scrub</command></link>
	      command.  Available checksum algorithms include:

	      <itemizedlist>
		<listitem>
		  <para>fletcher2</para>
		</listitem>

		<listitem>
		  <para>fletcher4</para>
		</listitem>

		<listitem>
		  <para>sha256</para>
		</listitem>
	      </itemizedlist>

	      The fletcher algorithms are faster, but sha256 is a
	      strong cryptographic hash and has a much lower chance of
	      collisions at the cost of some performance.  Checksums
	      can be disabled, but it is not advised.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-compression">Compression</entry>

	    <entry>Each dataset in <acronym>ZFS</acronym> has a
	      compression property, which defaults to off.  This
	      property can be set to one of a number of compression
	      algorithms, which will cause all new data that is
	      written to the dataset to be compressed.  In addition to
	      the reduction in disk usage, this can also increase read
	      and write throughput, as only the smaller compressed
	      version of the file needs to be read or written.

	      <note>
		<para><acronym>LZ4</acronym> compression is only
		  available after &os;&nbsp;9.2.</para>
	      </note></entry>
	  </row>

	  <row>
	    <entry
	      xml:id="zfs-term-deduplication">Deduplication</entry>

	    <entry>Checksums make it possible to detect duplicate
	      blocks of data as they are written.  If deduplication is
	      enabled, instead of writing the block a second time, the
	      reference count of the existing block is increased,
	      saving storage space.  To do this,
	      <acronym>ZFS</acronym> keeps a deduplication table
	      (<acronym>DDT</acronym>) in memory, containing the list
	      of unique checksums, the location of that block and a
	      reference count.  When new data is written, the checksum
	      is calculated and compared to the list.  If a match is
	      found, the data is considered to be a duplicate.  When
	      deduplication is enabled, the checksum algorithm is
	      changed to <acronym>SHA256</acronym> to provide a secure
	      cryptographic hash.  <acronym>ZFS</acronym>
	      deduplication is tunable; if dedup is on, then a
	      matching checksum is assumed to mean that the data is
	      identical.  If dedup is set to verify, then the data in
	      the two blocks will be checked byte-for-byte to ensure
	      it is actually identical.  If the data is not identical,
	      the hash collision will be noted and the two blocks will
	      be stored separately.  Because <acronym>DDT</acronym>
	      must store the hash of each unique block, it consumes a
	      very large amount of memory (a general rule of thumb is
	      5-6&nbsp;GB of ram per 1&nbsp;TB of deduplicated data).
	      In situations where it is not practical to have enough
	      <acronym>RAM</acronym> to keep the entire
	      <acronym>DDT</acronym> in memory, performance will
	      suffer greatly as the <acronym>DDT</acronym> must be
	      read from disk before each new block is written.
	      Deduplication can use <acronym>L2ARC</acronym> to store
	      the <acronym>DDT</acronym>, providing a middle ground
	      between fast system memory and slower disks.  Consider
	      using compression instead, which often provides nearly
	      as much space savings without the additional memory
	      requirement.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-scrub">Scrub</entry>

	    <entry>Instead of a consistency check like &man.fsck.8;,
	      <acronym>ZFS</acronym> has <command>scrub</command>.
	      <command>scrub</command> reads all data blocks stored on
	      the pool and verifies their checksums against the known
	      good checksums stored in the metadata.  A periodic
	      check of all the data stored on the pool ensures the
	      recovery of any corrupted blocks before they are needed.
	      A scrub is not required after an unclean shutdown, but
	      it is recommended that a <command>scrub</command> is run
	      at least once each quarter.  Checksums of each block are
	      tested as they are read in normal use, but a scrub
	      operation makes sure even infrequently used blocks are
	      checked for silent corruption.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-quota">Dataset Quota</entry>

	    <entry><acronym>ZFS</acronym> provides very fast and
	      accurate dataset, user, and group space accounting in
	      addition to quotas and space reservations.  This gives
	      the administrator fine grained control over how space is
	      allocated and allows critical file systems to reserve
	      space to ensure other file systems do not use up all of
	      the free space.

	      <para><acronym>ZFS</acronym> supports different types of
		quotas: the dataset quota, the <link
		  linkend="zfs-term-refquota">reference
		  quota (<acronym>refquota</acronym>)</link>, the
		<link linkend="zfs-term-userquota">user
		  quota</link>, and the
		<link linkend="zfs-term-groupquota">group
		  quota</link>.</para>

	      <para>Quotas limit the amount of space that a dataset
		and all of its descendants (snapshots of the dataset,
		child datasets and the snapshots of those datasets)
		can consume.</para>

	      <note>
		<para>Quotas cannot be set on volumes, as the
		  <literal>volsize</literal> property acts as an
		  implicit quota.</para>
	      </note></entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-refquota">Reference
	      Quota</entry>

	    <entry>A reference quota limits the amount of space a
	      dataset can consume by enforcing a hard limit.  However,
	      this hard limit includes only space that the dataset
	      references and does not include space used by
	      descendants, such as file systems or snapshots.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-userquota">User
	      Quota</entry>

	    <entry>User quotas are useful to limit the amount of space
	      that can be used by the specified user.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-groupquota">Group
	      Quota</entry>

	    <entry>The group quota limits the amount of space that a
	      specified group can consume.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-reservation">Dataset
	      Reservation</entry>

	    <entry>The <literal>reservation</literal> property makes
	      it possible to guarantee a minimum amount of space for a
	      specific dataset and its descendants.  This means that
	      if a 10&nbsp;GB reservation is set on
	      <filename>storage/home/bob</filename>, and another
	      dataset tries to use all of the free space, at least
	      10&nbsp;GB of space is reserved for this dataset.  If a
	      snapshot is taken of
	      <filename class="directory">storage/home/bob</filename>,
	      the space used by that snapshot is counted against the
	      reservation.  The <link
		linkend="zfs-term-refreservation">refreservation</link>
	      property works in a similar way, except it
	      <emphasis>excludes</emphasis> descendants like
	      snapshots.

	      <para>Reservations of any sort are useful in many
		situations, such as planning and testing the
		suitability of disk space allocation in a new system,
		or ensuring that enough space is available on file
		systems for audio logs or system recovery procedures
		and files.</para></entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-refreservation">Reference
	      Reservation</entry>

	    <entry>The <literal>refreservation</literal> property
	      makes it possible to guarantee a minimum amount of
	      space for the use of a specific dataset
	      <emphasis>excluding</emphasis> its descendants.  This
	      means that if a 10&nbsp;GB reservation is set on
	      <filename>storage/home/bob</filename>, and another
	      dataset tries to use all of the free space, at least
	      10&nbsp;GB of space is reserved for this dataset.  In
	      contrast to a regular
	      <link linkend="zfs-term-reservation">reservation</link>,
	      space used by snapshots and decendant datasets is not
	      counted against the reservation.  As an example, if a
	      snapshot was taken of
	      <filename>storage/home/bob</filename>, enough disk space
	      would have to exist outside of the
	      <literal>refreservation</literal> amount for the
	      operation to succeed because descendants of the main
	      data set are not counted by the
	      <literal>refreservation</literal> amount and so do not
	      encroach on the space set.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-resilver">Resilver</entry>

	    <entry>When a disk fails and must be replaced, the new
	      disk must be filled with the data that was lost.  The
	      process of using the parity information distributed
	      across the remaining drives to calculate and write the
	      missing data to the new drive is called
	      <emphasis>resilvering</emphasis>.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-online">Online</entry>

	    <entry>A pool or vdev in the
	      <literal>Online</literal> state has all of its member
	      devices connected and fully operational.  Individual
	      devices in the <literal>Online</literal> state are
	      functioning normally.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-offline">Offline</entry>

	    <entry>Individual devices can be put in an
	      <literal>Offline</literal> state by the administrator if
	      there is sufficient redundancy to avoid putting the pool
	      or vdev into a
	      <link linkend="zfs-term-faulted">Faulted</link> state.
	      An administrator may choose to offline a disk in
	      preparation for replacing it, or to make it easier to
	      identify.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-degraded">Degraded</entry>

	    <entry>A pool or vdev in the
	      <literal>Degraded</literal> state has one or more disks
	      that have been disconnected or have failed.  The pool is
	      still usable, but if additional devices fail, the
	      pool could become unrecoverable.  Reconnecting the
	      missing devices or replacing the failed disks will
	      return the pool to an
	      <link linkend="zfs-term-online">Online</link> state
	      after the reconnected or new device has completed the
	      <link linkend="zfs-term-resilver">Resilver</link>
	      process.</entry>
	  </row>

	  <row>
	    <entry xml:id="zfs-term-faulted">Faulted</entry>

	    <entry>A pool or vdev in the
	      <literal>Faulted</literal> state is no longer
	      operational and the data residing on it can no longer be
	      accessed.  A pool or vdev enters the
	      <literal>Faulted</literal> state when the number of
	      missing or failed devices exceeds the level of
	      redundancy in the vdev.  If missing devices can be
	      reconnected the pool will return to a
	      <link linkend="zfs-term-online">Online</link> state.  If
	      there is insufficient redundancy to compensate for the
	      number of failed disks, then the contents of the pool
	      are lost and must be restored from backups.</entry>
	  </row>
	</tbody>
      </tgroup>
    </informaltable>
  </sect1>
</chapter>
